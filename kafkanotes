What is Apache Kafka?
    Real time event streaming is rapidly growing across many business types, currently in all markets everything is driven by events. Simple way to think about is event streaming enables companies to analyze and respond to that event in real time, vast number of business events can create huge amount of data which can make real time decisions difficult, companies must be reliable that can lead to quick decisions and enhance customer experience, all of these and more can done through event streaming 
    There are many event streaming platforms available in market, and most popular platform for event streaming is apache kafka 

What is Apache Kafka?
    It is an open source, free of cost distributed event streaming platform 
    - Event streaming means capturing the data in real time from event sources like databases, sensors, file system, external appl in the stream of event form, storing processing in real time and directing to other applications or system as when needed
    - It ensures continuous flow of data so right information is reached at right place 
    - Most common usecases of event streaming are to process finanical, stock market related transaction or to capture social media activities or to gather data from IOT devices like traffic cameras for penalities calculation 

Architecture
    - It is a high level architecture of apache kafka, it is a distributed system that means it runs as a cluster of one or more servers 
    - On lefthand side we have producers which push the data to kafka topics, while on right hand side we have consumers or subscribers which reads the data from kafka topic and feed to other applications. 
    - Please note producer and consumer are independent of each other, they are highly decoupled so they may run on different machines and read or push the data in different rate 

Limitations
   - Despite of many features, it is more of publish and subscribe platform, it does not have entire data processing and operation tools
   - Also when it comes to data storing capabilities, u need to use on cloud or on premises platform

So what comes to rescue, there is another product called Confluent platform

What is Confluent?
   - It is full scale event streaming platform built on apache kafka with additional features
   - It expands the benefits of kafka with enterprise grade features with removing burder of management and monitoring 
   - It simplifies connecting data sources to kafka, building streaming appl, securing, managing kafka infrastructure
   - It let you focus on business value of data rather than underlying mechanics
   - Please note confluent platform is licensed product that means there is cost associated with it 

Confluent Architecture
    - It uses apache kafka as base and added development, monitoring, scalability, security features as toppings on it 
    - Including key capabilities like publish and subscribe, storing stream of events, processing stream of events, it does add schema registry, REST proxy, built in Kafka connectors, ksql db type of features 

Confluent 				Apache
1. more features than      1. comes with limited concept
apache kafka
2. licensed product        2. free of cost
cost to business

Connect Kafka broker using Spring boot appl
    - Here we create Kakfa producer to publish String and JSON schema messages to kafka topic using spring boot appl

1. start confluent server, we are using confluent server as same as apache server

C:\Softwares\confluent-windows-5.0.1\bin\windows> confluent.bat

2. Create topic called spring_boot_kafka_topic_v1
C:>kafka-topics --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic spring_boot_kafka_topic_v1

3. To check whether topic is created or not
C:>kafka-topics --describe --zookeeper localhost:2181 --topic spring_boot_kafka_topic_v1

Now we use this topic in spring boot appl to push the message

4. Create SpringBoot-Kafka-Producer project 

In main class, add @EnableKafka annotation which comes from Spring Kafka dependency, so it specifies spring boot appl is going to connect to kafka based services

5. Add kafka specific properties to application.yml file 

- Spring boot have enough support in order to connect to confluent or apache kafka using the properties

server:
   port: 1111

spring:
   kafka:
     producer:
       bootstrap-server: "localhost:9092"  #kafka server
       key-serializer: "org.apache.kafka.common.serialization.StringSerializer"
       value-serializer: "org.apache.kafka.common.serialization.StringSerializer"

Next property is key-serializer and value-serializer. First we try to push the message as string, so in order to add string serializer we have added the dependency of spring kafka, so we have one class called "StringSerializer"(press ctrl shft T), so both kafka key and value both are strings
      So this is initial setup, so bootstrap server will connect to your confluent server and key serializer, value serializer will infer that we are going to push string values into ur kafka topic. So spring.kafka.producer will automatically enable that u r going to create spring kafka producer 

   To know where this value go and bind to, there is class called "KafkaProperties class(press ctrl shift T)", inside that we have Producer class where we see property like bootstrap server, keyserializer, value serializer. So all property given in application.yml is bind to Producer class property under KafkaProperties class. In top we can see prefix="spring.kafka", in this way spring boot identifies where the property should bind 


6. Create custom producer class called SpringBootKafkaProducer and from there we can send messages to topic 
   - Create this class as service using @Service
   - Next we create KafkaTemplate which is responsible to send messages to kafka topic. Now we autowire as in default created as soon as spring identifies spring.kafka properties in application.yml. 
     In KafkaTemplate we mention key value pair which is string for both 
   @Autowired
   public KafkaTemplate<String,String> kafkaTemplate;

  - We create custom method called sendMessage(), which takes String value that will send using send(), which takes first parameter as topic name, then value and key will automatically created when we push the value. send() will return a ListenableFuture and add a callback on it with 2 methods called onSuccess and onFailure
   So when u send the topic, in case failure comes what action to take and if success comes what action to take.

@Service
public class SpringBootKafkaProducer {
	
	@Autowired
	public KafkaTemplate<String,String> kafkaTemplate;
	
	public void sendMessage(String value) {
		ListenableFuture<SendResult<String,String>> future=kafkaTemplate.send("spring_boot_kafka_topic_v1",value);
		future.addCallback(new ListenableFutureCallback<Object>() {

			@Override
			public void onSuccess(Object result) {
				System.out.println("Messages successfully pushed on topic");
			}

			@Override
			public void onFailure(Throwable ex) {
				System.out.println("Messages failed to push on topic");
			}
			
		});
	}

}

7. Create Rest controller to expose our API 
      Once we call "/send/{message}" api, we will receive the message and pass it to SpringBootKafkaProducer, so we inject it using @Autowired. Now we pass the message so that it will push to kafka topic 

@RestController
public class SpringBootKafkaController {

	@Autowired
	SpringBootKafkaProducer springBootKafkaProducer;
	
	@GetMapping("/send/{message}")
	public void send(@PathVariable("message")String message) {
		springBootKafkaProducer.sendMessage(message);
	}
}

8. Start the application
9. Run http://localhost:1111/send/helloworld
       - u can see "Message pushed to topic" in console, so that message is successfully pushed into topic  

10. Now we try to send custom object called Person with certain properties and push person object message as json  to kafka topic 

11. Now we create another method in producer to send Person message

public void sendMessage(Person person) {
		ListenableFuture<SendResult<String,Person>> future=kafkaTemplate1.send("spring_boot_kafka_topic_v1",person);
		future.addCallback(new ListenableFutureCallback<Object>() {

			@Override
			public void onSuccess(Object result) {
				System.out.println("Messages successfully pushed on topic");
			}

			@Override
			public void onFailure(Throwable ex) {
				System.out.println("Messages failed to push on topic");
			}
			
		});
	}

12. Create another endpoint in controller 

@PostMapping("/send")
	public void send(@RequestBody Person person) {
		springBootKafkaProducer.sendMessage(person);
	}

13. In application.yml we change value serializer as JsonSerializer

valueSerializer: "org.springframework.kafka.support.serializer.JsonSerializer"

14. Start the application

15. In postman, localhost:1111/send with POST request give in body - raw - json
{
   "id":"100",
   "name":"Ram",
   "age":20
}

Now we can see message is successfully pushed into topic


Creating Kafka Consumer with Spring Boot
    - Now we read string and json object from kafka topic

1. Create SpringBoot-Kafka-Consumer project 

2. Now we create consumer along with producer properties in application.yml using spring.kafka.consumer property
   When you consume the value from topic we need to deserialize it, so we have a keydeserializer and  valuedeserializer, in our case we read string info so we use the class "StringDeserializer"
   Next property is group id, we need to group the consumers by specific id using "group-id" and we can give any custom name to your group 
   Next we create a custom property to declare the topic name, so 

topic:
   name: "provide the topic name from where we read messages"

server:
   port: 2222

spring:
   kafka:
     producer:
       bootstrap-server: "localhost:9092"  
       key-serializer: "org.apache.kafka.common.serialization.StringSerializer"
       value-serializer: "org.apache.kafka.common.serialization.StringSerializer"
       #value-serializer: "org.springframework.kafka.support.serializer.JsonSerializer"
     consumer:
       bootstrap-server: "localhost:9092"
       key-deserializer: "org.apache.kafka.common.serialization.StringDeserializer"
       value-deserializer: "org.apache.kafka.common.serialization.StringDeserializer"
       group-id: "spring-boot-kafka-consumer"

topic:
  name: "spring_boot_kafka_topic_v1"

3. Next we create Config class with @Configuration so it is configuration class
   - First we need to create ConsumerFactory which is bean, which returns DefaultKafkaConsumerFactory, now this consumer factory will read the property of type consumer 
    As we see all consumer properties are mapped to java class called "KafkaProperties" and that will be add as argument to it. As soon as passing as argument to method it will automatically autowired
   Now we pass kafkaProperties to DefaultKafkaConsumerFactory constructor and call buildConsumerProperties() which will extract all consumer related properties and create consumerfactory 

@Bean
public ConsumerFactory<String,String> consumerFactory(KafkaProperties kafkaProperties){
   return new DefaultKafkaConsumerFactory<>(kafkaProperties.buildConsumerProperties());
}

   Now we create another bean called ContainerFactory  which is required for listener purpose. Inside that we create object of ConcurrentKafkaListenerContainerFactory , with this factory we have setConsumerFactory()

@Bean
public KafkaListenerContainerFactory<ConcurrentMessageListenerContainer<String,String>> kafkaListenerContainerFactory(KafkaProperties kafkaProperties){
   ConcurrentKafkaListenerContainerFactory<String,String> factory=new ConcurrentKafkaListenerContainerFactory<>();
   factory.setConsumerFactory(consumerFactory(kafkaProperties));
   return factory;
}

4. Now we create listener part called SpringBootKafkaConsumer with @Service annotation
   - Now we add method listen() with string argument and we print the value whatever we are reading 
   - This method has to annotate with KafkaListener within it,we need to provide topic name under "topics" attribute and provide the value which we created in application.yml file 
   - Next we need to provide containerFactory which we provide in Config file 

@Service
public class SpringBootKafkaConsumer {
 
    @KafkaListener(topics="${topic.name}", containerFactory="kafkaListenerContainerFactory")
public void listen(String value){
    System.out.println("Message received: "+value);
}
}

5. Start both the application

6. Now run http://localhost:1111/send/helloworld
     - You can see the messages are received on consumer side 
     - you can see messages successfully pushed to topic on producer side 

So consumer is reading the message from topic when message is published 

7. Now we read json message, in Config class we will override the key and value deserializer from application.yml using another constructor of DefaultKafkaConsumerFactory.
   So now read all message in form of Json rather than string values

@Bean
	public ConsumerFactory<String,Person> consumerFactory1(KafkaProperties kafkaProperties){
	   return new DefaultKafkaConsumerFactory<>(kafkaProperties.buildConsumerProperties(),new StringDeserializer(),new JsonDeserializer<>(Person.class));
	}
	
	@Bean
	public KafkaListenerContainerFactory<ConcurrentMessageListenerContainer<String,Person>> kafkaListenerContainerFactory1(KafkaProperties kafkaProperties){
	   ConcurrentKafkaListenerContainerFactory<String,Person> factory=new ConcurrentKafkaListenerContainerFactory<>();
	   factory.setConsumerFactory(consumerFactory1(kafkaProperties));
	   return factory;
	}

8. In SpringBootKafkaConsumer, we add new method to listen to person json object

@KafkaListener(topics = "${topic.name}", containerFactory = "kafkaListenerContainerFactory1")
	public void listen1(Person value) {
		System.out.println("Message received: " + value);
	}

9. Start Consumer appl

10. In postman, localhost:1111/send with POST request give in body - raw - json
{
   "id":"100",
   "name":"Ram",
   "age":20
}

Now we can see json message is successfully received on consumer side, as well we can send string message also


Produce Avro messages on Kafka with Spring Boot
    - We use Schema registry provided by Confluent platform to deal with avro schemas 

1. start confluent server, we are using confluent server as same as apache server

2. Create topic called spring_boot_kafka_topic_v1
C:>kafka-topics --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic spring_boot_kafka_avro_topic_v1

3. Create SpringBoot-AvroProducer project

4. In pom.xml, we have spring-kafka that supports kafka 
   - We have spring-starter-web in order to expose the rest services
   - Rest of dependency is related to confluent and avro, since we are going to produce avro message on confluent platform 
    kafka-avro-serializer used when you are producing the avro message, you need to serialize them in order to push  into the topic 
    Kafka-schema-registry-client is needed since  we talk to schema registry service 
    Avro dependency which is specific to creating the avro schemas
    In build we added one plugin called avro-maven-plugin to generate the java classes out of avro schema and it refers source dir path from src/main/resources and output dir is src/main/java.
   Any avro schemas present under resources, it will compile that avro schema and generate java classes under src/main/java 

5. In src/main/resources, we create a folder avro.schemas and within it there is avro file(StockHistory.avsc)

namespace: represent the package where java class has to create 
fields: attributes of that java class when it generates

We use to publish messages on the kafka topic. So when we compile this project, this stockHistory file will compile together and generate the java class file 

SpringBoot-AvroProducer>mvn clean install

After build success, when we refresh the project we can see schema package is created with StockHistory.java is created. This class is created based on avro file and it will contain all the fields present in avsc file as properties in this class 

6. In application.yml, we add producer related properties 

- Here we use String as keyserializer and for valueserializer we are producing the avro messages, so for that purpose we specify AvroSerializer(press ctrl shft T)

- Next we need to add properties to mention schema registry server which is by default running in 8081 port

- Next we create a custom topic name where we produce the message

server:
  port: 9081

spring:
  kafka:
    bootstrap-servers: "localhost:9092"
    producer:
      keySerializer: "org.apache.kafka.common.serialization.StringSerializer"
      valueSerializer: "io.confluent.kafka.serializers.KafkaAvroSerializer"
      properties:
        schema:
          registry:
            url: "http://localhost:8081"

avro:
  topic:
    name: "spring_boot_kafka_avro_topic_v1"


7. Create SpringAvroProducer class and provide with @Service
   - Next we access topic name from yml file and store in topicName variable
        @Value("${avro.topic.name}")
        String topicName;

   - Next we autowire KafkaTemplate with key value as String and StockHistory avro schema
          @Autowired
    private KafkaTemplate<String, StockHistory> kafkaTemplate;

   - Next create method called send() with StockHistory as argument and we need to mention how to push the message
    Now we need to send message using kafkaTemplate.send() with topicname as first argument, key as id from stockHistory and value as stockHistory object. This method returns ListenableFuture, and then add callback on that future object and provide the methods for success and failure 

 public void send(StockHistory stockHistory){
       ListenableFuture<SendResult<String,StockHistory>> future=  kafkaTemplate.send(topicName,String.valueOf(stockHistory.getTradeId()),stockHistory);
       future.addCallback(new ListenableFutureCallback<SendResult<String, StockHistory>>() {
           @Override
           public void onFailure(Throwable ex) {
               System.out.println("Message failed to produce");
           }

           @Override
           public void onSuccess(SendResult<String, StockHistory> result) {
               System.out.println("Avro message successfully produced");
           }
       });

8. Create StockHistoryModel class with same properties as schema 

9. Create SpringAvroRestController with post mapping 
    - Once we get model field value as input and we get those object using @RequestBody. Now we will transfer all values from model to StockHistory schema 
    So first we create object of StockHistory class and call newBuilder() and we set all the values 
   - Now StockHistory avro is ready and now we push this avro message to topic using SpringAvroProducer send()

10. Start the application

11. Goto postman and try to send StockHistory 

http://localhost:9081/sendStockHistory with POST request - Body - raw - JSON

{
   "tradeQuantity":100,
   "tradeMarket":"NST",
   "stockName":"Tata",
   "tradeType":"SEL",
   "price":250.5,
   "amount":20000
}
 
Now it will "Avro message successfully produced" in producer console, means avro message is sent to topic 

Consume Avro message

1. In application.yml, we configure all consumer related properties 
    We provide group-id, keyDeserializer as StringDeserializer and valueDeserializer as KafkaAvroDeserializer 
    Next we give autoOffsetReset property which tells the consumer how to read the data from kafka topic (ie) when u publish data on kafka topic that means you are adding on to the offset, now that offset will increase as in when you add the data. Now when you build the consumer from where you want to read the data, whether you want to read from the start of the offset or from latest offset.
   There are 2 possible values,  
       1. "earliest" which means when you build this consumer for the first time, read the messages from kafka topic from first offset (ie) from 0 
       2. "latest" which means read the messages from kafka topic from latest offset, it can be anything 1 or 10
  - Next we need to specify one more property telling consumer that we going to read avro messages 

consumer:
      group-id: "spring-boot-avro-consumer-id"
      keyDeserializer: "org.apache.kafka.common.serialization.StringDeserializer"
      valueDeserializer: "io.confluent.kafka.serializers.KafkaAvroDeserializer"
      autoOffsetReset: "earliest"
      properties:
        schema:
          registry:
            url: "http://localhost:8081"
        specific:
          avro:
            reader: "true"
   
2. Next we create Config class with @Configuration so it is configuration class
   - First we need to create ConsumerFactory which is bean, which returns DefaultKafkaConsumerFactory, now this consumer factory will read the property of type consumer 
    As we see all consumer properties are mapped to java class called "KafkaProperties" and that will be add as argument to it. As soon as passing as argument to method it will automatically autowired
   Now we pass kafkaProperties to DefaultKafkaConsumerFactory constructor and call buildConsumerProperties() which will extract all consumer related properties and create consumerfactory 

  @Bean
    public ConsumerFactory<String, StockHistory> consumerFactory(KafkaProperties kafkaProperties) {
        return new DefaultKafkaConsumerFactory<>(kafkaProperties.buildConsumerProperties());
    }

Now we create another bean called ContainerFactory  which is required for listener purpose. Inside that we create object of ConcurrentKafkaListenerContainerFactory , with this factory we have setConsumerFactory()

 @Bean
    public KafkaListenerContainerFactory<ConcurrentMessageListenerContainer<String, StockHistory>> kafkaListenerContainerFactory(KafkaProperties kafkaProperties) {
        ConcurrentKafkaListenerContainerFactory<String, StockHistory> factory = new ConcurrentKafkaListenerContainerFactory<String, StockHistory>();
        factory.setConsumerFactory(consumerFactory(kafkaProperties));
        return factory;
    }

3. Next we create SpringBootAvroConsumer class with KafkaListener with topicname and containerFactory
    - we create read() which takes ConsumerRecord class as argument with String as key and StockHistory as value. ConsumerRecord is a class which holds kafka data in key value pairs and access key and value when you listen the message for any particular process 

@Service
public class SpringBootAvroConsumer {

    @KafkaListener(topics = "${avro.topic.name}", containerFactory = "kafkaListenerContainerFactory")
    public void read(ConsumerRecord<String, StockHistory> record){
        String key=record.key();
        StockHistory history=record.value();
        System.out.println("Avro message received for key : "+key+ " value : "+history.toString());
    }

}

4. Start the application

5. Goto postman and try to send StockHistory 

http://localhost:9081/sendStockHistory with POST request - Body - raw - JSON

{
   "tradeQuantity":100,
   "tradeMarket":"NST",
   "stockName":"Tata",
   "tradeType":"SEL",
   "price":250.5,
   "amount":20000
}

Now it will print the message and also consume the message and prints in console 


What is Avro?
   - Avro is a data serialization system, now what does serialization means, consider we are using Java and we created some objects but when we want to transfer data over the network or we want 

   - Avro gets used in Hadoop as well as Kafka etc
   - Avro is defined by schema and that schema is written in json. Schema is nothing its just like a simple contract which consists of some fields, say when u publishing data on kafka then u publishing json object with 4 fields then u defining that in schema or contract with its name, datatype, its properties like nullable or non nullable
    So when publishing ur data, it would get verified by that schema if ur data is correct or not, so that we wont publishing bad data in kafka which creates problem for subscribers or consumers   

Advantages
1. Data is fully typed
       When u r using the schema, the data is fully typed, you cant put any random value in ur objects 

2. Data is compressed automatically
       When u r using avro serialization and deserializaion so data gets compressed, when u put it in Kafka so it lessens ur resource usage

3. Documentation is embedded in schema
      When u r creating avro schema, there u can define the documentation as well, consider we have json with 2 fields name and age these are quite self-explanatory fields, but if there is any field which is not very self-explanatory then we need to add some liner documentation for it, you can put it in schema and user get an definite idea for that field

4. Schema can evolve over time 
       Its not like that if you have made a contract then  its gonna last for a lifetime and we cant change it, you can definitely evolve over time but with set of rules, you cant change anything you like in the schema, there are defined rules for evolving ur schema 

5. Provides support for maintaining compatability on schema evolution 

6. Avro schema helps in keeping ur data clean and robust 
       Avro supports platform like Kafka that has multiple producers and consumers which get evolved over time and every schema helps in keeping ur data clean and robust 

Disadvantage
1. Data is not readable, it will need some tool or deserialization to read it
    If you have seen the event through some kafka client  that you put in kafka, those are very readable events, we can read it in string or json. But when u serialize it using kafka that dosent remains to be readable, you have to deserialize to read it so we need some tool

2. Performance is good unless u r going above 1 million records per second
     
Schema Evolution
    - It is all about dealing with changes in ur message record over time
    - Consider that we are collecting click stream and ur original schema for each click. So we have producers that are sending this data to kafka and we have consumers that are reading this data from kafka and doing some analysis 
    So we can deliver by implementing custom serializer and a deserializer. So everything works fine but later we want to upgrade the schema, so the problem starts here. If we change the schema we need to create new producers because we want to send some new fields. 
    Is it necessary to change all of current producers, which mean we can create new producers to include new fields but we dont want system to break if we are not upgrading all of them. Is it necessary to change existing consumer, which mean we can create new consumers to work with newly added fields, but current consumers are doing good and nothing to do with the new attributes 
    So we dont want to change current producer and consumers because that is too much of work. So we want to support both old and new schema simultaneously. In standard case if we change ur schema we have to change everything (ie)producers, consumers, serializers and deserializers. So after making these changes we cant read old messages because we change the code and any attempt to read old messages using new code will raise an exception
    So we need to have a combination of old and new producers as well as mix of old and new consumers. Kafka should able to store both type of messages in same topic, consumsers should able to read both types of messages without any error. This is called schema evolution problem
     The industry solution to handling schema evolution is to include schema info with the data, so when someone writing the data, they write schema and data both, so when someone wants to read the data they first read schema and then read data based on the schema. If we follow this approach we can keep changing schema as frequently as required without changing our code because we always reading schema before reading data 
    There are prebuild serialization system to help us and simply the whole process of translating messages according to schema and embedding schema info in the message record. Avro is one of popular serialization system, kafka uses Avro to handle schema evolution problem 
    - It is a key feature of Avro in this case, schema can change over time for instance
    1. Adding a field    2. Removing a field 

1. Consider we collecting employee data and ur original schema looks like below 
      
{ "type": "record",
  "name": "Employee",
  "fields": [
     {"name": "emp_id", "type": "int"},
     {"name": "empName", "type": ["string", "null"]},
     {"name": "address", "type": ["string", "null"]},
   ]
}

So we have producer and sending the data to kafka and we have consumers which reading this data from Kafka and doing some analysis 

   - We had this system in place for few months, and later u decide to upgrade ur schema like below 
{ "type": "record",
  "name": "Employee",
  "fields": [
     {"name": "emp_id", "type": "int"},
     {"name": "empName", "type": ["string", "null"]},
     {"name": "address", "type": ["string", "null"]},
     {"name": "qualification", "type": ["string", "null"]},
     {"name": "city", "type": ["string", "null"]}
   ]
}

The problem starts here, if u r changing the schema, u need to create a new Producer because we want to send some new fields. But I dont want to change current producers and consumers because that will be too much of work, so we want to support both old and new schema simultaneously 
    In standard case if u change ur schema, u have to change everything ur producers, consumers, serializers and deserializers. After making these changes we cant read old messages because we change the code and any attempt to read old messages using new code will leads an exception 
    In summary we need to have combination of old and new producers as well as mix of old and new consumers. Kafka should able to store both types of messages on same topic and consumer should able to read both types of messages without any error, this is called scheme evolution 

How to handle Schema evolution?
      The industry solution to handle schema evolution is to include schema with data, so someone writing data they write schema and data both, so someone want to read the data, they first read the schema and read data based on the schema. If we follow this approach we can keep changing schema as frequently as required without change our code because we are reading schema before the data 
      There are prebuild and reusable serialization system to simplify the whole process of transmitting messages according to schema and embedded schema info in message record 
     Avro is one of them, it is most popular serialization system. Kafka uses Avro to handle schema evolution problem 

Avro offers 4 things
  1. Allows you to define a schema for your data, and schema defined through json  

{ "type": "record",
  "name": "Employee",
  "fields": [
     {"name": "emp_id", "type": "int"},
     {"name": "empName", "type": ["string", "null"]},
     {"name": "address", "type": ["string", "null"]},
     {"name": "medicalrecords", "type": ["string", "null"],default:"None"}
   ]
}

Here emp_id is mandatory, however empName, address can have null value, medicalrecords have a default value as none

  2. Generates codes for ur schema - create Employee.java 
  3. Provides API's to serialize your data according to the schema and embed schema info in the data
   So I can use Employee.java generated in step2 to create the data objects and use Avro Api like KafkaAvroSerializer to serialize and send to kafka broker 
  4. Provide API's to extract schema info and deserialize ur data based on the schema 
   Similarly on receiving end can use Avro Api like KafkaAvroDeserializer to deserialize those messages back to Employee object 


So far we learnt KafkaAvroSerializer takes care of all serialization work at producer end and KafkaAvroDeserializer will take care of deserialization work at consumer end, but how they will communicate with each other about schema. The deserializer should know the schema, without knowing the schema it cant deserialize the raw bytes back to object, thats where schema registry is useful
     KafkaAvroSerializer will store the schema details in schema registry and include the id of schema into message record. When kafkaAvroDeserializer receives the message it takes schema id from message and get schema details from registry. Once we have schema details and message bytes it is simple to deserialize 

1. C:>kafka-topics --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic avro_topic_v1
2. Create maven project AvroSchema-V1
3. Create employee.avsc inside schema folder
4. mvn clean install, so it creates Employee.java for avro file
5. Create AvroProducer.java
6. Create AvroConsumer.java
7. Run AvroConsumer
8. Run AvroProducer
     Now it will display the employee object from topic to consumer console

   But whole point of using Avro is to support schema evolution
Now we modify our schema and create a new version of prior schema and then we create new Producer to send some messages based on new schema 
   This example show us schema evolution, we will see old producer, new producer and old consumer working together in same system, finally we create new consumer which will read both type of messages without any exception 

1. In same project we update the schema, but schema name and type will not change, however we change the record structure. Now we added some extra fields like city,qualification into schema as below

{ "type": "record",
  "name": "Employee",
  "fields": [
     {"name": "emp_id", "type": "int"},
     {"name": "empName", "type": ["string", "null"]},
     {"name": "address", "type": ["string", "null"]},
     {"name": "medicalrecords", "type": ["string", "null"],default:"None"},
      {"name": "qualification", "type": ["string", "null"]},
     {"name": "city", "type": ["string", "null"]}
   ]
}

But we cannot free to evolve schema in random fashion, avro provides some rules for compatiability of schema 

3. Now we want to create new producer called AvroProducerV2 and send some messages in new format, but this time we create new Employee object based on new schema, then setting the value and finally sending to kafka broker  

4. Run AvroConsumer 
5. Run AvroProducerV2 
   Now we can see old consumer can read message from new producer
6. Run AvroProducer 
    Now also old consumer can read message from old producer also 

So using Avro and schema registry we can quickly build a system where producers and consumers can read all type of messages


Avro Schema Compatability 

1. Backward compatiable change
       - Data written by an older schema can be read by a newer schema, eg: v1 message can be read by v2

- Create customer.avsc with firstName and lastName fields
- Create Consumer1.java
- Create Producer1.java
- Run Consumer1 and then Producer1
      It will print the output
-Now we add a field called middleName 
-Now when we run Producer1, it will throw an exception called " Schema being registered is incompatible with an earlier schema; error code: 409"
   The first version of schema we created is not compatiable with new changes we made it 
- http://localhost:8081/config - by default it shows compatiability as "BACKWARD" (ie) every stream must adhere to backward compatibility 
    Which means data written by older schema can be read by newer schema, so if any message is already in system which has firstName, lastName and you are making a change into the schema and that new schema has middlename and if older message comes to this schema, they wont able to map it so its saying it is not backward compatibility with new schema
   So what confluent says is we can define a default value, so for middlename we can define as null, but middlename type is string so it can accept only string, so in type we can define ["null","string"], so it can either null or string for middlename, since first value is null we can define default value as null
   If we change the order like ["string","null"] and if we say default value is null, it shows error. So whatever default value we give, it have to match the first element in type 
   {
        "name": "middleName",
        "type": ["null","string"],
        "default": null
    }   
Now when we run there will be no error. So this new schema is backward compatibility if older producer produces any message, then it will consider default value for middleName 

2. Forward compatiable change
       - Data written by a new schema cant be read by a older schema, eg: v2 message can be read by v1

- Now in new schema, we are going to rename the lastName field and it is no longer exist in new schema, we want to call it as "surname"
- Now we change the compatibility in POSTMAN with PUT request - Body - Json - "http://localhost:8081/config"
{
  "compatibility":"FORWARD"
}
- Now if we run Producer1, then again it shows an error as "Schema is incompatibile with earlier schema"
   So if this new schema produce any message with surname and older schema is still expecting that the value would have lastname, so simply the message will fail. 
   So we can use "surName" instead of "lastName" by using "aliases" property and whenever you create a new property always define the default value 
 {
        "name": "lastName",
        "type": "string",
        "aliases": ["surName"]
    }
- Now when we run the producer it will show the output

3.{
  "compatibility":"FULL"
}
4. {
  "compatibility":"NONE"
}

5. What cant be migrated?
       - Changing the datatype of field
       - Modifying the values of an enum
       - Removing a field which does not have default value

6. What can be migrated?
       - Fields with default values specified can later be removed without affecting the previous schema
       - Fields can be renamed by supplying an alias

Rules
1. Make primary key required
2. Give default value to fields that could be removed in future
3. Be careful when using enums as they cant evolve over time
4. Dont rename fields, you can add aliasesinstead of other name
5. When evolving a schema, always give default value
6. when evolving a schema, never delete a required field


Schema Evolution
    You have different producer producing data for same topic and we have different consumers which are receiving the same data, so how schema can evolve on producers and consumers without affecting each other, so there should some policy to evolve schema without affective each other

Schema Compatibility policy
     - Defines the rule of how the schema can evolve
     - Subsequent version updates has to honor the schema's original version
     - Backward,Forward,Full,None

Backward Compatibility
     - New version of a schema would backward compatible with earlier version of that schema
     - Data written from earlier version of the version, can be read with a new version of the schema
     - Delete fields or add optional fields only
     - In backward, we have to create field with default values
     - Consider we have version1 with id and color
{
  "type" : "record",
  "name" : "book",
  "namespace" : "com.pack",
  "fields" : [
       {
          "name" : "id",
          "type" : "int"
       },
       {
          "name" : "color",
          "type" : "string",
          "default" : "blue"
       }
    ]
}

   - Consider we create another version with one more field called pages with default value
{
  "type" : "record",
  "name" : "book",
  "namespace" : "com.pack",
  "fields" : [
       {
          "name" : "id",
          "type" : "int"
       },
       {
          "name" : "color",
          "type" : "string",
          "default" : "blue"
       },
       {
          "name" : "pages",
          "type" : "int",
          "default" : -1
       }
    ]
}
    
- So we get data from v1 where pages field is not there, so on reader side it acts pages field with default value without breaking the schema  


2. Forward Compatibility
      - Existing schema is compatible with future version of the schema, that means data written from new version of the schema can still be read with old version of the schema 
      - Add fields or delete optional fields 
      - In forward compatibility we create fields without default value then also it will consider 

- Consider we have version1 with id and color
{
  "type" : "record",
  "name" : "book",
  "namespace" : "com.pack",
  "fields" : [
       {
          "name" : "id",
          "type" : "int"
       },
       {
          "name" : "color",
          "type" : "string",
          "default" : "blue"
       }
    ]
}

   - Consider we create another version with one more field called pages 
{
  "type" : "record",
  "name" : "book",
  "namespace" : "com.pack",
  "fields" : [
       {
          "name" : "id",
          "type" : "int"
       },
       {
          "name" : "color",
          "type" : "string",
          "default" : "blue"
       },
       {
          "name" : "pages",
          "type" : "int"
       }
    ]
}
- So on consumer side it will have pages field also but it will drop it, so whenever we forward compatibility we add new fields with default value 

3. Full compatibility
       - New version of schema provides both backward and forward compatibility 


Schema Composition
    - Schema can be shared and reused

{
   "name" : "uuid",
   "type" : "record",
   "namespace" : "com.pack",
   "fields" : [
       {
         "name" : "value",
         "type" : "string",
         "default" : ""
       }
    ]
}

{
   "name" : "account",
   "namespace" : "com.pack",
   "includeSchemas" : [
       {
           "name" : "uuid"
       }
    ],
    "type" : "record",
    "fields" : []
}

Convert complex Java objects to Avro schema
    Conversion of existing complex plain old java objects to Avro schema so that we can use such avro schema for further kafka based operations

1. Create JavaToAvro maven project with avro dependency

<dependency>
        <groupId>org.apache.avro</groupId>
        <artifactId>avro</artifactId>
        <version>1.11.0</version>
     </dependency>

2. Create pojo class

public class Employee {
     private Integer empId;
     private String name;
     private Double salary;
     private Address address;
}

public class Address {
    private String street;
    private String city;
    private String state;
    private BigDecimal zipcode;
}

3. Create main class
       - First we use ReflectData class which is responsible to convert class into avro schema. Here we call getSchema() which take class as argument and returns schema 
    Schema schema=ReflectData.get().getSchema(Employee.class);
    System.out.println(schema.toString(true));

Now when we run, it will print schema on the console. This is not complete schema because it is not handling default or null value behaviour because for all fields we can see only name and type, what happens if any fields comes null, in such case when we use this schema directly it will throw an exception in real time usage, for that purpose we need to handle null behaviour, so such things we have to handle it manually 

Now we can create this schema file inside resources folder

File f=new File("src/main/resources/employee.avsc");
FileWriter fw=new FileWriter(f);
fw.append(schema.toString(true));
fw.close();

Topic, partitions and offsets
    - In Kafka we have topics, topics represent a particular stream of data, so kafka topic is similar to table in a db without all constraints, so if u have many tables in db we have many topics in kafka
   - You can have as many topics as you want, a topic is identified by its unique name 
   - Topics are split into partitions, so when u create kafka topic, u need to specify how many partitions we want for our kafka topics, each partition is going to be stream of data and each partition will have data in it and being ordered 
   - Each message within partition will get an incremental id, which is position of message in partition and it is called as offset 
   - For example, if we take a kafka topic with 3 partition, if we look at partition 0, it will have the message with offset 0, then message with offset 1,2,3 etc all way to 11 and the next message will be written is going to be offset number 12 
     Partition 1 also part of kafka topic and this also have offsets going from 0 all the way to 7 and next message is written from offset 8
     Partition 2 has message offsets going from 0 all to 9 and next message should be written is number 10 

            Partition 0  0 1 2 3 4 5 6 7 8 9 10 11
Kafka Topic Partition 1  0 1 2 3 4 5 6 7 8
            Partition 2  0 1 2 3 4 5 6 7 8 9 10
   
So as we see partitions are independent, we will be writing to each partition independently at its own speed, and offsets in each partition are indepenednt  and again message has coordinates of a topic name, partition id and an offset 

What goes into Kafka?     
    Consider we have group of trucks in truck company and what we want to do is to have the truck position in kafka, since we need stream of truck position for dashborad or some alerting, so we create kafka topic called trucks_gps which contain the position of all trucks in real time
    Each truck is going to send kafka every 20secs, their position which will be included as part of message and each message will contain truck id as well as truch position like latitude, longitude, speed, ,weight of truck etc. So we create the topic with 10 partition and as many u can 
    From their consumer appl are going to be location dashboard for mobile appl or notification service 

1. Offset only have a meaning for a specific partition, so offset 3 in partititon 0 does not represent the same data or same message as offset 3 in partition 1
2. Also if we look at ordering of messages, the order will be guaranteed only within the partition, so across partition we have no ordering guaranteed, so we have ordering only at partition level 
3. Data in kafka by default is kept only for one week (ie) after one week the data is going to be erased from the partition, so this allows kafka to make sure it doesnt run out of disk and stream the latest data 
4. Kafka is immutable, once the data is written to partition it cannot be changed 

Kafka Producers
    - Kafka producers are going to write data to topics which are made of partitions 
    - Now the producers in kafka, they will automatically know to which broker and partition to write to, based on ur message. In case there is a kafka broker failure in ur cluster, the producers will automatically recover from it which makes kafka resilient 

How does producer know how to send the data to a topic?
     - We can use message keys, so along the message value we can choose to send message key, a key can be anything like string, number etc. If we dont send the key, then key is null, then the data is send as round robin fashion (ie) ur first message will sent to partition 0, second message to partition 1 etc
     - In case if we send key with ur message, then all the messages that share the same key will always go to same partition 
     - If u need ordering for a specific field, for example if you have our trucks and you want to get all gps position in order for that specific truck, then u need to have message key sets as unique identifier for ur truck. So in our truck gps example we need to choose message key as truck_id so that we have all the truck positions for that one specific truck in order as part of same partition 

Kakfa Messages
    - Kafka messages are created by producer, the key can be null and type of key is binary, so binary is 0's and 1's, as we said it can be string or numbers, and it will convert string or numbers to binary 
     So we have key which is binary field which can be null and then we have the value which is content of ur message, again this can be null. So key,value are most important part in message, but there are other things in message, for example ur message can be compressed, so the compression type can be indicated as part of message like none, gzip, snappy, lz4, zstd. We also have optional headers for your message, so headers are pairs of key value
    Once the message is sent into a kafka topic  then it will receive partition number and offset id, so the partition and the offset are going to be the part of the kafka message and then finally timestamp alongside the message will be added either by the user or system, then that message will be sent to kafka 

Producer Serializer
     When we start writing some messages in Kakfa, we are going to use some higher level objects, and to transform these objects into binaries, we will use producer serializer 
     So serializer will indicate how to transform these objects into bytes, they will be used for key abd value 
     For example we have value="Helloworld" as a string and key=123 thats an integer, in that case we beed to set key serializer to be IntegerSerializer and internally it will convert that integer into bytes, and these bytes will be part of the key which will be binary. 
     In case of value which is string we use StringSerializer as value serializer to convert that string into bytes will be part of value which is binary format

Consumers 
     - Consumers read data from a topic, identified by name. Consumers know from which broker to read from and which partitions to read from 
     - In case of broker failures, consumer knows to recover 
     - Data for the consumers is going to read in order within each partitions
     Consider consumer is consuming from topic A/partition 0, then it will first read message 0 then 1,2 etc till 11. If another consumer is reading from 2 partitions (ie) partition 1 and partition 2, is going to read both partitions in order, so within partition the data is going to be read in order but across partitions, we have no way of saying which one is going to be read first or second and this is why there is no Ordering across partitions

Consumer Deserializer
     - Consumers are going to be reading our messages from Kafka which are made of bytes, so deserializer is needed for the consumer to indicate how to transform these bytes back into some objects or data  


Avro Producer
1. Create project AvroProducer
2. Run producer appl
      When we run, it prints Customer0@0 (ie) paritition 0 and offset 0
      When we run once again, it prints Customer0@1 (ie) paritition 0 and offset 1
3. Run consumer in console 
>kafka-avro-console-consumer --bootstrap-server localhost:9092 --topic customer-avro_1 --from-beginning --property schema.registry.url=http://localhost:8081






{
    "type": "record",
    "name": "User",
    "namespace": "com.pack.schema1",
    "fields": [
    {
        "name": "userId",
        "type": "int"
    },
    {
        "name": "firstName",
        "type": "string"
    },
    {
        "name": "lastName",
        "type": "string"
    }
    ]
}

1. mvn clean install
2. Create topic
 C:\Spring\KafkaSchema>kafka-topics.bat --zookeeper localhost:2181 --replication-factor 1 --partitions 3 --topic avrotopic3 --create

3. Create Producer
4. create consumer
5. Run consumer1
6. Run Producer1
      Now it will print the values

7. But in backward compatiability, we can add fields with optional values, so we add new value middlename

{
    "type": "record",
    "name": "User",
    "namespace": "com.pack.schema1",
    "fields": [
    {
        "name": "userId",
        "type": "int"
    },
    {
        "name": "firstName",
        "type": "string"
    },
    {
        "name": "lastName",
        "type": "string"
    },
    {
        "name": "middleName",
        "type": ["null","string"],
        "default": null
    }
    ]
}

So consumer can consume old schema as well as new schema. So this new schema is backward compatibility if older producer produces any message, then it will consider default value for middleName 

{
        "name": "age",
        "type": "int"
    }

In forward compatibility we add new field without any default value

What is stream processing ?
   - Data Streams are 
         unbounded - no definite starting ot ending
         Often infinite and ever growing
         Sequence of data in small pockets
       The first and most important thing is to understand that the data streams are unbounded, infinite and ever growing sequence of data that is continuously generated and sent in small sizes in order of KB's
      - Common Examples are 
  1. sensors - sending data from transportation vehicles, industrial equipments, healthcare devices, wearables
  2. Log entries - generated by mobile apps, web appl, infrastructure components
  3. Click streams - generated by ecommerce appl, news and media websites, video streaming, online gaming
  4. Transactions - coming from stock market, credit cards, ATM machines, ecommerce orders, payments, logistics and food deliveries
  5. Data feeds - example social media activities, traffic activities, security and threat systems

    Everything can be seen as a sequence of smart data packets, and these are unbounded, infinite and ever growing, once started they never end

How to process them ?
   1. One approach is to collect and store them in a storage system, then we can do 2 things
    - Query the data to get answers to a specific question, this is called request response approach and usually done through SQL. This approach is all about asking one question at a time and getting the answer to the question as quick as possible
   2. Second approach is to create one big job to find answers to a bunch of queries and schedule the job to run at regular intervals. This approach is all about asking a bunch of questions at once and repeating the question every hour or every day. This is approach id called batch processing
    Stream processing sits between these 2 approaches. In this approach, we ask a question once, and the system should give you the most recent version of the answer all the time. So stream processing is continous process and the business reports are updated continuously based on the data available till time. As we receive more data, the reports are updated and refreshed with new information 
    But in background we use same operations like joining two streams, grouping ur data, computing aggregates, and other things that we do in db and batch processing system. Since stream processing is also data processing work but we do it in a continuous and ongoing process 

Can we use database and batch processing to perform stream processing? yes and no
    Technically it is possible to use database and batch processing to do a stream processing,so answer is yes. But dealing with data in realtime using those system makes ur solution more complex, so ans is no
    So we need a tool that is specifically designed for stream processing which is Kafka streams  

What is Kafka Streams?
     - Kafka streams is a java library for building appl and microservices where the input data is streamed in a kafka topic. So u cannot use kafka streams if ur data is not coming from kafka topic. The starting point of kafka stream is one or more kafka topics 
     - Kafka streams being a simple library, so we can create standard java and scala appl to perform real time stream processing. And you can deploy ur appl in any machines, VM, container or on a kubernetes cluster 
     - Kafka streams provide with parallel processing capability, fault tolerance, scalability  
     - It is a easy data processing and transformation library within kafka, it comes with kafka library not any external library 
     - It is a client library for processing and analyzing data that is arriving from kafka topics. It is just java library 
     - It enables to consume from kafka topics, perform some transformation work on data and send to other systems

What Kafka Streams offers?
1. Working with streams/tables and interoperating with them 
   You can mix and match ur solutions with streams and tables and you can even convert a stream to table and vice versa
2. Allows you to Group ur streams and continuously updating Aggregates
3. You can join streams, tables and a combination of both
4. You can create and manage fault tolerant, efficient local state stores
5. Creating windows of different types
6. Dealing with all the time domain complexities such as event time, processing time, latecomes, high watermark, exactly once processing etc
7. Allows u to serve other microservices using request/response interface over streams appl, this is called kafka streams interactive query
8. provide with set of tools for unit testing ur appl
9. Inherent fault tolerance and dynamic scalability
10. Deploy ur stream processing appl in containers and manage using Kubernetes 
    

Key features
1. Simple and lightweight client library
          It means it can be easily plugged in with any java appl and integrated with deployment tools to get going with stream processing 

2. No external dependencies on system other that Apache kafka

3. Supports fault tolerant local state which enables very fast efficient stateful operations

4. It processes one record at a time

5. It also guarantees that each record will be processed once and only once even there is a failure in the middle of processing either on stream client or kafka brokers 

Kafka Streams Architecture
      So the kafka streams is all about continuously reading a stream of data from one or more kafka topics. And then, u develop ur appl logic to process those streams in real time and take necessary actions
      So assume that you created a Kafka streams appl and deployed it on a single machine, and ur kafka stream appl is continuously consuming data from 2 topics T1 and T2 with each having 3 partitions 
      Now kafka streams will internally create 3 logical task because maximum number of partitions across the input topics T1 and T2 is 3. So the Kafka stream framework knows that we can create 3 consumer where each could be consuming from one partition in parallel. We dont need to write any code, the framework will detect it and create 3 logical tasks.
     Next step is to assign partitions to these tasks, kafka framework will allocate the partitions evenly (ie) one partition from each topic to each task. Finally every task will have 2 partitions to process.
     Now these tasks are ready to be assigned to application threads. If u configured the appl to run with 2 threads, kafka would assign one task to each thread. And the remaining third task will go to one of these threads because we do not have anyother thread.
     In this case, the task distribution is not even. The thread running two task might run slow. However all tasks would be running and ultimately all data are processed 
     We can make Kafka streams as multithreaded appl, by simply setting number of Maxthreads. So as of now we have one appl instance running on a single machine and sharing the workload in 2 parallel threads.
     Now we want to scale this appl, so we start another instance with a single thread on different machine, a new thread T3 will be created, and one task will automatically migrate to the new thread, this is called task reassignment. When task reassignment occurs, the task partitions and their corresponding local state stores also migrate from existing thread to newly added thread. As a result, kafka streams has effectively rebalanced the work load among instances of the appl
   In case of fault tolerance, if a task runs on a machine fails, kafka streams will automatically restart the tasks to one of remaining running instance, so failure is transparent to enduser and automatically takes care by framework itself 

Stream Processing Topology
    It is nothing but a graph of nodes connected by edges where nodes are represented by processors and edges are represented by streams 
    In simple words it is a graphical representation of stream processing flow in ur appl

1. Stream
     - It is most important abstractions provided by kafka streams, it represents an unbounded, continuous flow of updating data set.
       It is ordered replayable and fault tolerant sequence of immutable key and value pair of data because we know Kafka stores data in the form of key and value pair

2. Stream processing application
        - Any application or any program that uses of Kafka stream library to build processor topology is called Stream processing appl

3. Stream processor
        - It is a node in processor topology which represents a processing steps to transform data into streams by receiving input records from upstream processors and apply transformation operations and produce output to downstream processor
        - There are 2 special type of processors
1. Source processor - It is a type of stream processor where it does not have a upstream processor 
   Consider the source processor to be used as the start of ur processor topology flow, its job is to consume records from one or more kafka topics and produce stream to its downstream processors in the topology

2. Sink processor - It is also a special type of stream processor which dosent have any downstream processors, it sends any receive records from its upstream processor to specified kafka topic

Stream processing Topology
      Nodes represents stream processors while edges represents streams 
      At the top we have source processor which does not have any upstream processor so here its job is to source data from kafka topics, while at bottom we have sink processor with the job to sink data coming from its upstream processor to kafka topic

What Kafka stream library offers ?
    There are 2 ways to define processor topology
1. Kafka stream DSL(Domain Specific Language)
      - It is built on the top of streams processor api, it supports declarative functional programming style with stateless and stateful transformation
      - Stateless transformation methods includes map, mapValues, filter etc
      - Stateful transformation methods includes count, reduce, joins, windowing etc. 
      Java8 streams method and kafka streams method are more similar
      - Stream DSL provides some built-in abstractions to implement or define stream processing topology 
    1. StreamBuilder - used to define a topology 
    2. kstream - used for working with record stream
    3. KTable - used for working with change log streams
    4. GlobalKTable - used for working with global  change log streams

Streams DSL is a high level API that offers standard data transformation functions with two main abstractions 
   1. KStream is most essential abstraction offered by Kafka Streams API. It is like an insert-only table with two columns (ie) key and value. So KStream represents a key/value table where data is coming from kafka, and it is being continuously inserted 

   2. KTable is like insert/update table witk key and value columns. The ktable key acts as a primary key.
     So when a new record comes to ktable, kafka will check for the primary key existence. If the key already exists, the old record is updated with the new value. If the key does not exist, a new record is inserted.
     So at a high level, ktable is a primary key table where data is coming from kafka and it is updated or inserted depending upon the key. Whereas the KStream is a table where data is coming from kafka and getting inserted to kstream even if the key already exists  
    

2. Lower level processor api
        - Allows developers to define and connect custom processors and to interact with state stores
        - It can be used to implement both stateless and stateful operations,where stateful is achieved through the use of state stores 

Kafka Stream Configuration using Spring boot
        Consider we have one topic called "spring.boot.kafka.stream.input", it has messages of type string. So we build stream processing topology with the help of springboot framework, where this topology will read the data from input topic with help of input stream (ie) we are going to build a stream to read the data from topic and then we apply some transformation logic like filter some events and map some values into different form and the resultant output of that stream will put into another topic called "spring.boot.kafka.stream.output", so the result of my stream processing topology (ie) output stream will pushed into output topic 
 
1. Create topic spring.boot.kafka.stream.input and spring.boot.kafka.stream.output

2. Create SpringBootKafkaStream project with spring for apache kafka and spring for apache kafka streams dependency
   Kafka-streams dependency will help us to enable ot build stream processing logic in the appl

2. Enable kafka and kafka streams in main class using @EnableKafka and @EnableKafkaStreams

3. In application.properties, we write the properties required for kafka

spring.kafka.streams.bootstrap-servers=localhost:9092
spring.kafka.streams.application-id=spring-kafka-stream-id

First mandatory property is kafka server and next property is  unique id for every appl

4. Create configuration class with @Configuration
   Create bean called StreamsConfig which returns StreamsConfig object with properties related to Kafka stream 
  We know the properties given in application.yml file is mapped with KafkaProperties class. This class contains an inner class called Streams where all properties are mapped.
   Now from this properties object we can build Stream properties which will return HashMap and set that properties under StreamsConfig object 

@Configuration
public class Config {

    @Bean
    public StreamsConfig streamsConfig(KafkaProperties properties){
        return new StreamsConfig(properties.buildStreamsProperties());
    }
}

5. Next we build stream processing topology, for that we create EventStreamProcessor class with @Component 
   We create streamTopology method, in order to build topology we need an abstraction called StreamBuilder which is automatically created when we add spring.kafka.streams properties in application.yml, because spring boot follows autoconfiguration so as soon as it saw these properties, it will automatically create bean for StreamBuilder, so we just autowire it
     @Autowired
    private StreamsBuilder streamsBuilder;

    Next we use this streamsbuilder object to create stream on topic called "spring.boot.kafka.stream.input" and we need to consume this value with serializer or deserializers of type stream. We have seen the value present in input topic is string, key and value is of same type (ie) string, so we use string for both key and value, this method will return a Kstream 
   Serdes used by kafka streams for reading input bytes into expected object types
   KStream<String, String> kStream = streamsBuilder.stream("spring.boot.kafka.stream.input", Consumed.with(Serdes.String(), Serdes.String()));

   Now as we build the stream the flow of data from input topic starts coming so we have message of type string. Now first method we apply on stream is filter where we want to retain only those messages which starts with "Message_" then we keep those messages and discard rest of messages. Now on the retained messages we want to apply mapValues() where we transform the coming messages into uppercase and then we call peek() which is just logging(printing) ur transform messages. 
    Now we output all the result into another topic called "spring.boot.kafka.stream.output" using to() and produce with same sting key and value serializers

kStream.filter((key, value) -> value.startsWith("Message_")).mapValues((k, v) -> v.toUpperCase()).peek((k, v) -> System.out.println("Key : " + k + " Value : " + v)).to("spring.boot.kafka.stream.output", Produced.with(Serdes.String(), Serdes.String()));

As we know kafka stream is real time processing, it should be started as soon as ur appl is started, so in order to make real time we have 2 annotations so that it can be started as soon as ur appl starts 
  1. @Bean - beans are always executed as ur appl starts 
  2. @PostConstruct - which is most preferable, we are saying execute this method as soon as u r ready with all beans we are created  

So when appl starts, it ready with all beans, it will hit to the stream topology method, start executing like building the stream on input, filtering the values, mapping the values and finally putting to output topic  


@Component
public class EventStreamProcessor {

    @Autowired
    private StreamsBuilder streamsBuilder;

    @PostConstruct
    public void streamTopology() {
        KStream<String, String> kStream = streamsBuilder.stream("spring.boot.kafka.stream.input", Consumed.with(Serdes.String(), Serdes.String()));
        kStream.filter((key, value) -> value.startsWith("Message_")).mapValues((k, v) -> v.toUpperCase()).peek((k, v) -> System.out.println("Key : " + k + " Value : " + v)).to("spring.boot.kafka.stream.output", Produced.with(Serdes.String(), Serdes.String()));
    }
}

6. Start the appl, in the console we can see Kafkaconsumer, which means kafka stream is build on top of the consumer, so if we start kafka stream we are actually starting the consumer, so kafka stream can be actually used as kafka consumer along with its dsl methods  

7. Create kafka console producer to send the message
>kafka-console-producer.bat --broker-list localhost:9092 --topic spring.boot.kafka.stream.input
>Message_TestMessage1
>Message_TestMessage2
>TestMessage
>message_TestMessage3

Now if we check the spring boot console which will acts as consumer, we can see only 2 messages are filtered and then pushed to that topic 

8. Now we check in consumer whether the message is consumed or not
>kafka-console-consumer.bat --bootstrap-server localhost:9092 --from-beginning --topic spring.boot.kafka.stream.output


Defining Kafka Streams
     Kafka stream topology is defined using StreamsBuilder class, to define a stream first we create instance of StreamBuilder class, then use that builder to create Kstream which is Kafka stream abstraction 
     We can create KStream using builder.stream() which takes input topic and consumed configuration objects with key and value records 

StreamBuilder builder=new StreamBuilder();
KStream<String,String> firstStream=builder.stream(inputTopic, Consumed.with(Serdes.String(), Serdes.String());

With KStream we can tranform the data and kafka streams provides mapping operation for that like 
1. map() - takes key and value and produce new kay value 
2. mapValues() - takes ValueMapper which is functional interface defines a map method with single input
3. filter()


https://github.com/confluentinc/learn-kafka-courses/tree/main/kafka-streams

Kafka Stream basic operations

1. Create KafkaStreams maven project with 
<dependency>
    <groupId>org.apache.kafka</groupId>
    <artifactId>kafka-clients</artifactId>
    <version>3.2.1</version>
</dependency>
<dependency>
    <groupId>org.apache.kafka</groupId>
    <artifactId>kafka-streams</artifactId>
    <version>3.2.1</version>
</dependency>

2. Create ProducerExample.java with main method

public class ProducerExample1 {

	public static void main(String[] args) {
		Properties prop=new Properties();
		prop.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
		prop.put(StreamsConfig.APPLICATION_ID_CONFIG, "stream1");
		
		StreamsBuilder builder=new StreamsBuilder();
		
		KStream<String, String> stream=builder.stream("streaminput", Consumed.with(Serdes.String(), Serdes.String()));
        stream.peek((k,v)->System.out.println("key="+k+" value="+v))
             .filter((k,v)->v.contains("orderNumber-"))
             .mapValues(v->v.substring(v.indexOf("-") + 1))
             .filter((k,v)->Long.parseLong(v)>1000)
             .peek((k,v)->System.out.println("key="+k+" value="+v))
              .to("streamoutput",Produced.with(Serdes.String(), Serdes.String()));
        KafkaStreams st=new KafkaStreams(builder.build(),prop);
        st.start();
	}

}

3. Start the appl

4. Start producer
C:\>kafka-console-producer.bat --broker-list localhost:9092 --topic streaminput
>orderNumber-2000
>orderNumber-1000
>order

Here we give filter input starts with "orderNumber-" and after that it takes value after "-" using mapValues() and then again filter the value >1000

5. Start consumer
C:\SpringBoot\KafkaStreams>kafka-console-consumer.bat --bootstrap-server localhost:9092 --from-beginning --topic streamoutput
>2000

Here it will display only one value since it starts with "orderNumber-" and after that we take 2000 which is greater than 1000


KTable
    Streams are series or sequence of key value pairs which are independent of each other. In contrast to that, an update stream is also sequences of key value pairs
   Update stream is a update that is applied to previous value. So records come in with same key are actually updating the value of previous record with that same key 

Defining a KTable
   We can use StreamBuilder to create KTable also, after that we use builder.table() method which takes name of topic and configuration topic which tells kafka streams what the topic and how to read the events in that input topic 


StreamBuilder builder=new StreamBuilder();
KTable<String,String> firstTable=builder.table(inputTopic, Materialized.with(Serdes.String(), Serdes.String()); 

    So unlike a KStream, a KTable can only be subscribed to a single topic at once, and that is the topic that the table really represents. So table is representing latest value of each records. 
    So unlike a KStream, KTable needs to be storing all of those values somewhere so that it knows what was the latest value at any given time (ie) ktable is backed by state store. State store is copy of the events that are in the topic that the table is built from, you store on disc so that we can look up and track the latest value of each of the records in that topic 

   Next KTable which is different from kstream is that, ktable by default does not forward every change. So in case of kstream every event is its own event and every event has its own meeting, so we always forward new events. Whereas in KTable we care about what is the latest value for key. For ktable it dosent care what each event that coming in, all we care is what is latest value of that key  
   We might buffer the updates for that table in cache, and that cache only when it is flushed these updates gets forwarded. By default cache gets flushed every 30sec which determines how quickly we get updates from ktable, you can set it to 0 and see every update or we can set it longer and get updates when u need them 

KTable Operators
1. Mapping - map(), mapValues()
2. Filtering - filter()

GlobalKTable
    - There is special kind of ktable which is globalktable

StreamBuilder builder=new StreamBuilder();
GlobalKTable<String,String> firstTable=builder.globalTable(inputTopic, Materialized.with(Serdes.String(), Serdes.String()); 

The difference between ktable and globalktable is do with partitions. The topic lives on broker which contain anything that contains events which may be 0 or 100 but it is lots of data. So kafka divides these topics up into partitions.
    A partition is just logical subset of data in a topic and that is going to be partitioned by key (ie) all events in same key will end up in same partitions. Kafka streams will deal with only one partition at a time. A typical ktable will only see the subset of data for one partition of that topic at a time 
    GlobalKtable on other hand will actually does hold all the records across all partitions, so this useful when u want to get view of all the data of entire topic. Usually this is for data which is smaller in size and something that is static and not updated frequently like zipcode, country codes 

1. Create ProducerExample2.java 

public class ProducerExample2 {

	public static void main(String[] args) {
		Properties prop=new Properties();
		prop.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
		prop.put(StreamsConfig.APPLICATION_ID_CONFIG, "stream2");
		
		StreamsBuilder builder=new StreamsBuilder();
		
		//here we provide topic name, with materialized config object along with we need to provide name for KTable in order to be materialized
		//It will use caching and only admit the latest records for each key after the commit which is 30 sec or when cache is full at 10MB
		KTable<String, String> table=builder.table("streaminput", Materialized.<String,String, KeyValueStore<Bytes, byte[]>>as("ktable-store").withKeySerde(Serdes.String()).withValueSerde(Serdes.String()));
      
             table.filter((k,v)->v.contains("orderNumber-"))
             .mapValues(v->v.substring(v.indexOf("-") + 1))
             .filter((k,v)->Long.parseLong(v)>1000)
             .toStream()
             .peek((k,v)->System.out.println("key="+k+" value="+v))
              .to("streamoutput",Produced.with(Serdes.String(), Serdes.String()));
        KafkaStreams st=new KafkaStreams(builder.build(),prop);
        st.start();
	}

}

2. Start the appl

3. Start producer
C:\>kafka-console-producer.bat --broker-list localhost:9092 --topic streaminput
>orderNumber-2020

Here we give filter input starts with "orderNumber-" and after that it takes value after "-" using mapValues() and then again filter the value >1000 and then convert to kafka stream using toStream() and use peek() for printing the output 

5. Start consumer
C:\SpringBoot\KafkaStreams>kafka-console-consumer.bat --bootstrap-server localhost:9092 --from-beginning --topic streamoutput
>2020

Here it will display only one value since it starts with "orderNumber-" and after that we take 2000 which is greater than 1000
  Your appl should run for 40sec and u will get latest output

Word count example
1. Create ProducerExample3.java

public class ProducerExample3 {

	public static void main(String[] args) {
		Properties prop=new Properties();
		prop.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
		prop.put(StreamsConfig.APPLICATION_ID_CONFIG, "stream5");
		prop.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG,"earliest");
		prop.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());
		prop.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());
		
		StreamsBuilder builder=new StreamsBuilder();
		
		KStream<String, String> stream=builder.stream("streaminput2");

		KTable<String,Object> wordCount = stream.mapValues(t->t.toLowerCase())
				.flatMapValues(lt->Arrays.asList(lt.split(" ")))
				.selectKey((ig,word)->word)
				.groupByKey()
				.count().mapValues(value->Long.toString(value));
		wordCount.toStream().peek((k,v)->System.out.println("key="+k+" value="+v)).to("streamoutput2");	

        KafkaStreams st=new KafkaStreams(builder.build(),prop);
        st.start();
	}

}

2. Start confluent

3. C:\Softwares\kafka_2.12-2.6.0\config>kafka-topics.bat --zookeeper localhost:2181 --replication-factor 1 --partitions 3 --topic streaminput2 --create
Created topic "streaminput2".

4. C:\Softwares\kafka_2.12-2.6.0\config>kafka-topics.bat --zookeeper localhost:2181 --replication-factor 1 --partitions 3 --topic streamoutput2 --create
Created topic "streamoutput2".

5. C:\Softwares\confluent-windows-5.0.1\bin\windows>kafka-console-producer.bat --broker-list localhost:9092 --topic streaminput2
>hello world
>welcome world
>welocme message

6. C:\SpringBoot\KafkaStreams>kafka-console-consumer.bat --bootstrap-server localhost:9092 --topic streamoutput2 --from-beginning   
    - Will display the number of words 

Data Serialization 
    As kafka will read message in the form of bytes, we use Serdes for this. Serdes used by kafka streams for reading input bytes into expected object types. To create custom Serde we use

Serde<T> serde=Serdes.serdeForm(new CustomSerializer<T>, new CustomDeserializer<T>);

PreExisting Serders: String, Integer, Double, Long, Float, Bytes, ByteArray, ByteBuffer, UUID and Void
    Avro - SpecificAvroSerde, GenericAvroSerde
    ProtoBuf - KafkaProtobufSerde
    JSONSchema - KafkaJsonSchemaSerde

Joins in Kafka Streams
    Kafka streams offers join operations like two or more streams together into a third stream or table. Kafka streams joins require that the records being joined have the same key. So two events with unrelated keys are likely to be unrelated so it cant be joined
    Whereas a join might be something like customer purchases being matched with customer addresses and producing an output stream of the full info set  that u have about customer

3 types of Joins
1. Stream-Stream joins 
       We have two event streams which being joined into a new event stream and called as windowed join. The records that arrive are joined with other records of that same key within a defined window of time, and all this data is stored in local state store to keep track of what data has arrived during this time
      Now its possible to change the value type or produce the same value type, the join could be anything that you want as long as the keys are same. And the keys themselves are only available in read only mode so that they can used to compute the new value but u cant modify them 
3 types
   1. Inner join 

2. Stream-table joins
       Now stream-table join is not windowed like stream-stream join was, which means that anytime u get event from stream side, its going to be joined with whatever the latest value for the table is, and that will be producing the output record.
2 types
1. Kstream-ktable join - which is normal join based on whatever data there is for that key in that partition
2. Kstream-globalktable join 

3. Table-table join
       It is going to result in another table which represents the latest value for these keys 


    Joins are one of most common and desirable features of any data system, we may work with batch system or real time system, in both cases at some point you would need to join 2 data sets. In kafka stream framework ur data sets are abstracted into 2 categories
  1. KStream      2. Ktable and GlobalKTable

Preconditions and Limitations for joins in Kafka
1. Join are based on message key
   Joins in kafka are performed over record keys, so kstream or ktable must have a valid key. If we dont have key or if key is null then we cannot perform a join.
    There is only one exception that is globalktable where nonkey based joins are allowed with kstream to globalktable joins, and all other joins are key based

2. Both the topics in a join must be copartitioned
       The input topics of the join on left side and right side must have same number of partitions and data on both the topic must be copartitioned (ie) all appl that write to input topic must have same partitioning strategy so that records with same key are delivered to the same partition number
       If the inputs of a join is not copartitioned yet, u must ensure this manually by rewriting data into new topic that has the same number of partitions and uses same partitioner, this is a mandatory requirement for parallel processing in kafka 
      There is only exception to this rule which is kstream to globalktable, copartitioning is not compulsory in case of kstream to globalktable joins because all partitions of globalktable is made available to each instance (ie) each streams task has a full copy of globalktable hence co-partitioning is needed for joining globalktable 

Join op       Result     Join type           Feature
1. kstream-   Kstream    Inner,left,outer   windowed,
   kstream               swapping for right  key based

2. ktable-    KTable     Inner,left,outer,  non-window
   ktable                swapping from right  key base

3. KStream-   KStream    Inner,left,outer   non-window
   KTable                                   key based

4. KStream-   KStream    Inner,left         non-window
   GlobalKTabke                         key, non-key                                              based

As listed in above table, inner and left outer joins are supported everywhere, but kafka does not have method for implementing right outer joins, however we can easily perform right outer joins by swapping left and right side data sets, but this swapping is only allowed with kstream to kstream and ktable to ktable joins.
   In other cases such as kstream to ktable, kstream to globalktable, the kstream must be on left side of join so right outer join is not possible in these 2 cases because u cannot swap the orders
   Finally full outer joins are available for first 3 cases however it is not allowed in globalktable 

Spring Boot + Kafka Streams - https://www.youtube.com/watch?v=3N_Lah56pTI
   We are going to create producer service which will take student object to REST API and send that student to kafka topic with the help of Kafka producer api. Now we use kafka streams to analyze and process the real time stream. Here kafka streams will take student object from the topic and it will check whether the student status is pass or fail and if the status is pass it will send model object to Passtopic, if it is fail it will take model object to failTopic 

1. Create KafkaStudentProducer project with lombok, spring for kafka, spring web

2. Create model class Student.java
@Data
public class Student {
    private Integer id;
    private String name;
    private String course;
    private String status;
}

3. Create KafkaProducerConfig.java with @Configuration
   Next we create bean which is used to return KafkaTemplate with key as String and value as Student.
        @Bean
	public KafkaTemplate<String, Student> kafkaTemplate(){
		return new KafkaTemplate<>(producerFactory());
	}

This bean will take producerFactory where we set all our configuration and return to Kakfa

        @Bean
	public ProducerFactory<String,Student> producerFactory(){
		Map<String, Object> config=new HashMap<>();
		config.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
		config.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
		config.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, JsonSerializer.class);
		return new DefaultKafkaProducerFactory<>(config);	
	}

4. Create KafkaProducerService.java with @Service, we autowire KafkaTemplate and we create simple method called send() in which we send student object to Kafka topic using KafkaTemplate

@Service
@Log4j2
public class KafkaProducerService {

	@Autowired
	KafkaTemplate<String, Student> kafkaTemplate;
	
	public void send(Student student) {
		log.info("Student object is {}",student);
		kafkaTemplate.send("studentTopic",UUID.randomUUID().toString(),student);
	}
}

5. Create StudentController.java to send the message to kafka topic

@RestController
public class StudentController {

	@Autowired
	KafkaProducerService kafkaService;
	
	@PostMapping("/post")
	public void student(@RequestBody Student student) {
		kafkaService.send(student);
	}
}

6. Start the application
7. Start zookeeper, kafka server
8. Create topic "studentTopic"
9. In postman, run http://localhost:2000/post with POST request - Body - raw - json
{
   "id":100,
   "name":"Ram",
   "course":"Java",
   "status":"pass"
}

10. Run kafka console consumer to check whether data is consumed
>kafka-console-consumer.bat --bootstrap-server localhost:9092 --topic studentTopic --from-beginning

11. Create KafkaStudentStreamService project with lombok, spring for kafka streams, cloud stream 

Use Spring Cloud Stream when you are creating a system where one channel is used for input does some processing and sends it to one output channel. In other words it is more of an RPC system to replace say RESTful API calls.

If you plan to do an event sourcing system, use Spring-Kafka where you can publish and subscribe to the same stream. This is something that Spring Cloud Stream does not allow you do do easily as it disallows the following

A few things that Spring Cloud Stream helps you avoid doing are:
   setting up the serializers and deserializers

12. Create Student model class 

13. In application.yml, add the binder and binding configuration 

spring:
  cloud:
    stream:
      bindings:
        student-input-channel:
          destination: studentTopic
        student-pass-output-channel:
          destination: passTopic
        student-fail-output-channel:
          destination: failTopic
      kafka:
        streams:
          binder:
             brokers: localhost:9092

We added 3 binding student-input-channel, student-pass-output-channel, student-fail-output-channel and for each channel we have provided topic. But spring boot dosent recognize whether it is input or output channel, in order to identify we have to create binding interface

14. Create interface called StreamBindings.java with 3 methods and map the methods with input and output channels

public interface KafkaStreamBindings {
    @Input("student-input-channel")
	KStream<String, Student> inputStream();
    @Output("student-pass-output-channel")
	KStream<String, Student> passStream();
    @Output("student-fail-output-channel")
	KStream<String, Student> failStream();
}

15. Create StreamService.java with @EnableBinding and provide with KafkaStreamBindings.class where we have added all our bindings
    Now we create passStudent() method which takes KStream as an input and returns also KStream. Here we use method level annotation called @StreamListener with "student-input-channel" because we are listening to input channel. After getting the student info it has to send data to output channel "student-pass-output-channel" using @SendTo

@EnableBinding(KafkaStreamBindings.class)
@Service
public class StreamService {

	@StreamListener("student-input-channel")
	@SendTo("student-pass-output-channel")
	public KStream<String, Student> passStudent(KStream<String, Student> student) {
		return student.filter((k,v)-> v.getStatus().equalsIgnoreCase("pass"));
	}

	@StreamListener("student-input-channel")
	@SendTo("student-fail-output-channel")
	public KStream<String, Student> failStudent(KStream<String, Student> student) {
		return student.filter((k,v)-> v.getStatus().equalsIgnoreCase("fail"));
	}

}

Here KafkaStreamBindings is an interface and implementation of all methods are done by Spring cloud stream, so whatever data from "student-input-channel" it will take the data and convert into KStream type. So @StreamListener will call passStudent() and pass the data in the form of KStream using Spring cloud Stream and after that it will do some logic returns the data and send to specific channel and from that channel it goes to related topic 

16. Create passTopic, failTopic
17. Run consumer for passTopic
>kafka-console-consumer.bat --bootstrap-server localhost:9092 --topic passTopic --from-beginning
18. Run consumer for failTopic
>kafka-console-consumer.bat --bootstrap-server localhost:9092 --topic failTopic --from-beginning
17. Start both the appl


Kafka Streams with Spring Cloud

1. Create KafkaStream1Application with lombok, cloud stream, spring for apache kafka stream dependency
    - We create simple StreamListener which is Kafka message consumer who will listen to kafka topic, read all  incoming messages and logging

2. Kafka and spring are highly configurable so we configure all spring cloud stream properties in application.yml file like input output channel binding which defines list of sources and destinations and binder configuration defines source and destination technology. Spring cloud offers various binder technology like apache kafka, rabbitmq, amazon kinesis, goggle pub/sub, azure etc
     Spring cloud offers 2 types of Kafka binders like
1. Apache Kafka binder implements Kafka Client API's 
2. Apache Kafka Streams Binder is explicitly designeds for Kafka streams API. 
     Here we focus on Apache Kafka Streams Binder. 

spring:
   cloud:
      stream:
        bindings:
           input-channel-1:
              destination: users
        kafka:
           streams:
              binder:
                applicationId: hellostreams
                brokers: localhost:9092
                configuration:
                   default:
                      key:
                        serde: org.apache.kafka.common.serialization.Serdes$StringSerde
                      value:
                        serde: org.apache.kafka.common.serialization.Serdes$StringSerde

We define one channel called "input-channel-1" and destination is "users" where we connect with "users" kafka topic using "input-channel-1"
    Now we want to implement kafka stream binder and rest of configuration is specific to kafka streams, we are setting kafka broker hostname and port, so appl can connect to kafka cluster. We are expecting string key and string value so we set StringSerde for both 

3. Now we want to create kafka listener service that binds to kafka input topic and listens to all incoming messages 
    Creating kafka stream listener requires 2 things

1. Interface to define input output bindings
       We define method signature for input stream reader annotated with @Input, which reads from kafka topic and returns KStream. KStream is kafka message stream made up of string key and string value 
     @Input is normally deprecated, it is new change coming in Spring cloud 3 and above but it is still under preview, so in order to avoid deprecation message we change cloud version name as "Hoxton.SR9" or 2021.0.4. If we refresh the project, deprecation message will missing
    Goto spring.io - Projects - Spring cloud - Learn

public interface KafkaListenerBinding {
     
    @Input("input-channel-1")
    KStream<String,String> inputStream();
}
      
2. Service class to listen to input bindings 
        First annotate with @Service annotation, we want to log input message so we use @Log4j2 
        Now we want to bind this class to spring cloud stream infrastructure so we use @EnableBinding with binder interface name 
       Now this class will trigger cloud stream framework and connect to kafka input channel using the  kafka streams API and start consuming input messages as KStream
       Now we pass each message to KStream by creating a method with @StreamListener. Spring cloud framework will call this method and pass to kstream, now we wabt to log the message so we use forEach() on input stream 

@Service
@Log4j2
@EnableBinding(KafkaListenerBinding.class)
public class KafkaListenerService {
 
    @StreamListener("input-channel-1")
    public void process(KStream<String,String> input) {
      input.foreach((k,v) -> log.info(String.format("Key: %s, Value: %s",k,v)));
    }
}

So input channel has to pick by spring cloud framework and bind to "users" kafka topic, so we define binding interface and linked input channel using @Input annotation.
   However we define configuration and linked with interface but still not using spring cloud framework, so we create listenerservice class. @EnableBinding will trigger spring cloud framework, now spring cloud framework will implement binding interface and create kafka stream, we can listen to stream using listener method called process(), so listener method will receive input stream and sending to log for printing          
4. Start confluent services
>confluent local services start 

5. Create topic
>kafka-topics.bat --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic users --create

6. Run producer 
>kafka-console-producer.bat --broker-list localhost:9092 --topic users
>{"name":"Ram","age":20,"gender":"male"}

7. Start spring boot appl and check whether it can consume the message

Kafka support in Spring
     Kafka offers 2 types of API's
1. Kafka Client API's - used to create kafka producer and consumer appl
2. Kafka Streams API's - used to support stream processing requirements 

Similarly Spring implemented Kafka in 2 ways
1. Spring for Apache Kafka
          Spring started implementing Kafka support in Spring framework under the project name  "Spring for Apache Kafka", which became top level spring project to implement all the core features of Kakfa Client API

Goto Spring.io - Projects - View all projects - click Spring for Apache Kafka  
          We can use this framework to create kafka producer and consumer microservices 

2. Spring cloud
      1. Spring cloud streams
            - Kafka Binder
            - Kafka Streams Binder 

Spring realized the requirement for a specialized stream processing framework using Spring cloud streams which present under Spring cloud framework

Goto Spring.io - Projects - Spring Cloud - click Spring Cloud Stream on right side 

    Spring cloud stream is one of subproject inside spring cloud projects. So Spring cloud stream implements many technologies such as Kafka, RabbitMQ and many more. 
    We can see 2 Kafka Binders
1. Apache Kafka which is traditional implementation
        Spring Cloud stream started to support Kafka using Kafka binder. The initial kafka binders were based on the Kafka Consumer Producer API and they do not support Kafka stream API
2. Kafka Stream is most recent implementation 
        Spring started implementing Kafka Stream binder to use KStream, KTable features of Kafka Streams API 

A typical Kafka project will use both methods. Kafka Streams is specialized framework to implement a Stream processor pattern. In this pattern we will read data stream from kafka, process it and write it back to kafka.
   But Kafka stream microservice will consume data from kafka topic only, they do not support reading data from another source and thats the limitation

     Consider we have a requirement to receive data over a Restful API or pull data calling a SOAP API. Once data is received, we want to do some processing and then send it to a kafka topic, u cannot do it with kafka streams because kafka stream processing starts reading data from kafka topic, if  data is not in Kafka topic we cannot implement in Kafka Streams 
     Similarly, if you are reading data from kafka topic and want to pass it to another RESTful service  or to a SOAP service or save it to a database, you cannot do it using kafka streams. We will using Kafka Spring project to implement consumer api and then we will call a RESTful service to send the data or save it to database 
     So we use Spring for Kafka project in following scenarios
   1. You need a producer only or a consumer only and also want to interact with other non-kafka services
     We use Kafka Stream in scenarios like, where we have a requirement to read a data stream from Kafka and implement some stream processing technique such as aggregating, summarizing, windowing and send summarized result to another kafka topic. Kafka streams does not limit to output side, we can send results to kafka topic or we can send to other 3rd party system such as database or RESTful service 

Spring Cloud Stream
    Spring framework always aspired to simplify enterprise Java development. They offered spring boot framework to simplify creating Java based microservices. Similarly, the spring integration framework was designed to provide a solution for simplified application integration. However some modern enterprise appl started to implement distributed event-driven streaming architecture 
    Here appl run in a cloud platform and follow a microservice architecture, they continuously operate on stream of events or messages 
    Spring integration framework already offered a similar solution for enterprise integration patterns. In this pattern one appl publish message to a common message channel, other appl can read the message from the channel at a later time. In all these, the appl must agree on a shared channel and the format of the message, and the communication remains asynchronous.
   Spring realized that this framework could be extended and simplified further to support the modern demand for event driven streaming architecture. As a result, Spring came up with new framework called Spring Cloud Streams 
   Spring Cloud Stream is a new framework that combines the power of 2 spring framework (ie) spring boot and Spring integrations. So Spring cloud stream builds upon Spring boot to create standalone, production grade microservice appl and uses Spring integration to provide connectivity to message brokers . As a result, Spring Cloud streams used for building event driven stream processing microservices 

Architecture
    Spring cloud stream appl is message driven microservice appl which is used to consume message events, process it to take necessary business actions, and produce event for other microservices 
    So we have main appl code for processing message events received through one or more input channels. The core of ur appl implements all the business logic and takes the necessary business actions. Finally if needed we can produce message events to one or more output channels 
    So appl developer focussed on processing message events, implementing necessary business actions and producing new messages, rest all is taken care by the framework
    Spring cloud stream allows you to create and configure input output message channels using configuration which is called as spring-cloud-stream-bindings, and these bindings are picked up by the binder to implement input/output channel and communicate with external messaging system. So binder is 3rd party Spring component responsible for providing integration with the external messaging systems. We have different 3rd party binders like
     1. Apache Kafka
     2. Kafka Streams
     3. Rabbit MQ
     4. Amazon Kinesis
     5. Google Pub/Sub
     6. Azure Event Hubs

   The binder is responsible for the connectivity and routing of messages from the respective messaging systems, data type conversions etc. But main feature is that they will collect the incoming message events from messaging system, do the necessary datatype conversions, call the user code and pass each message to you for processing. Similarly they take ur outgoing message events, do the necessary datatype conversions and send the messages to underlying messaging system. 
    In general, Spring cloud stream is responsible for following things
1. Configure input/output channel bindings
       In application.yml is the place where we configure the channel bindings and binder. We created one input channel and named as "input-channel-1"
       The binders are responsible for communicating from messaging system, so we configure connection details like hostname and port. Similarly the binder also needs to perform data type conversion to configure it with message key and value types.

2. Configure the binder
       Define an interface for input and output channel. We specify @Input annotation to mark this channel as input channel and define expected datatype of message stream as KStream of String key and String value 

3. Implementing the business logic 
       We create service class and enable binding for this class using @EnableBinding. Now Spring knows that we want to communicate with messaging system for input output channels defined in the interface 
      Here we configure one input channel, so the framework will start reading incoming messages from the configured channel. So EnableBinding will trigger reading of messages and we want to process those messages. So we define process() and configure it to become a stream listener and tell that we want this method to listen to "input-channel-1"
      Now the framework will call this method and pass input stream and the framework will read the message from "input-channel-1" invoke this function and pass the message here in this function 

Kafka Streams
    - It is a client library for processing and analyzing data that is arriving from Kafka topics. So it is just java library and core functionality is avaiable as Stream DSL 
    Streams DSL is a high level API that offers standard data transformation functions with 2 main abstractions called KStream and KTable
    KStream is the most essential abstraction offered by the Kafka Streams API, it is like an insert-only table with two columns - key and value. So KStream represents a key/value table where data is coming from Kafka, and it being continuously inserted 
    KTable is an insert-update table with key and value columns. The KTable key acts as primary key, so whenever new record comes to KTable, Kafka will check for primary key existence. If key already exists, the old record is updated with new value. If the key does not exist, a new record is inserted. So at high level, KTable is primary key table where data is coming from Kafka, and it is being updated or inserted depending upon the key 
     Whereas KStream is a table where data is coming from Kafka and getting inserted to KStream even if the key already exists 

Refer KafkaListenerService.java
       We created process(), which receives a KStream. We know Spring cloud stream will connect to Kafka clusters, pull the messages from Kakfa, and call this process() method to pass the messages here as KStream. Now we can process each message as they arrive 
       Spring framework will make the data available to us as a KStream or as KTable. Once we have KStream or KTable, we will be using Kafka Streams API to process the data 

    So we will be using Spring cloud streams to create microservice appl and communicate with kafka cluster. Kafka Streams will help us to process the incoming data, apply transformation and solve the business problem.

Simple Restful Kafka Producer  - https://github.com/LearningJournal/Kafka-Streams-with-Spring-Cloud
      We want to create Spring boot microservice which exposes RESTful POST api. The microservice will receive JSON object and send the key value message to the given kafka topic 

1. Create KafkaRestProducer spring boot project with spring web, lombok, spring for apache kafka dependency

2. We expect our microservice to receive JSON object with 3 fields like topic, key and value. So we create data model for incoming object  

@Data
public class IncomingMessage {
     private String topic;
     private String key;
     private String value;
}

3. Create rest controller to accept JSON object over a POST api, construct a message and send it to Kafka service  
    The Kafka service will take care of sending the message to kafka topic.

4. Creaet KafkaProducerService class and annotated with @Service, create method with 3 input values and send message to kafka 
    However sending a message to Kafka is dobne by a Kafka producer object. Spring for kafka offers you a ready to use KafkaTemplate, u can create a KakfaTemplate and autowire it so Spring boot will inject the object and used to send message to kafka cluster 

@Service
@Log4j2
public class KafkaProducerService {
   
     @Autowired
     private KafkaTemplate<String,String> kafkaTemplate;

     public void sendMessage(String topic, String key, String value) {
     kafkaTemplate.send(topic,key,value);
     log.info(String.format("Producing Message- Key: %s, Value: %s to topic: %s, key, value, topic);
     }
}

5. KafkaTemplate should know kafka server host and port, so we configure info in application.yml file

spring:
   kafka:
      producer:
         client-id: rest-producer
         bootstrap-servers: localhost:9092
         key-serializer: org.apache.kafka.common.serialization.StringSerializer
         value-serializer: org.apache.kafka.common.serialization.StringSerializer

Now Spring boot will read those configuration and apply them to create KafkaTemplate 
 
6. Create Rest Controller class 
     We have a sendMessageToKafka() which takes @RequestBody which is of IncomingMessage type. Now we want to send message to Kafka so we inject KafkaProducerService and call sendMessage() with topic, key and value

@RestController
public class KafkaMessageController {

     @Autowired
     KafkaProducerService kafkaProducerService;
     
     @PostMapping("/post")
     public String sendMessageToKafka(@RequestBody IncomingMessage message){
         kafkaProducerService.sendMessage(message.getTopic(),message.getKey(),message.getValue());
         return String.format("Success - Message for key %s is sent to Kafka Topic: %s",message.getKey(),message.getTopic());
     }
}

The rest controller will take the incoming message and pass it over to the KafkaProducerService service. KafkaProducerService will create KafkaTemplate to send the messages to the Kafka server 

7. Start confluent server 
8. Create Kafka topics
>kafka-topics.bat --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic users --create

9. Start Kafka console consumer to see if messages are coming to kafka cluster
>kafka-console-consumer.bat --bootstrap-server localhost:9092 --topic users --from-beginning --property print.key=true --property key.separator=":"

10. Start the application 

11. In Postman with POST request we give "http://localhost:1111/post" with body - raw - json
{
   "topic":"users","key":"101","value":"Ram"
}

check in Kafka console consumer, we can see message 

Creating Retail POS Simulator
      Previously we create kafka producer and semd some messages to kafka topic, we sent message with string key and string value. In typical kafka project we will not see String message value, your message key is mostly string but message value could be complex Java object 
      Consider we have retial store and we have a sales terminal to create different invoices and these invoices are sent to kafka topic. On other side we will developing some stream processing appl to analyze the stream of invoices 
      So we create spring boot appl which continuously generate some invoices. Your invoice object will have some fields like invoicenumber, createdTime, storeId etc, the deliveryType of invoice could be home delivery or take-away. A home-delivery invoice will have an address but take-away does not have address field. next we have line items for invoice

1. Create KafkaInvoiceGenerator spring boot project with lombok, spring for apache kafka dependency

2. Keep json files related to invoice details inside resources folder

3. We define data model for invoice which has deliveryaddress,LineItem and PosInvoice

@Data
public class DeliveryAddress {
    private String addressLine;
    private String city;
    private String state;
    private String pinCode;
    private String ContactNumber;
}

However we want to use this class to read JSON formatted data from address.json file, so we use jackson databind library for reading JSON file

4. Add jackson-databind dependency in pom.xml

<dependency>
    <groupId>com.fasterxml.jackson.core</groupId>
    <artifactId>jackson-databind</artifactId>
    <version>2.13.4</version>
</dependency>

5. We need to add some Jackson annotations to class

@Data
@JsonInclude(JsonInclude.Include.NON_NULL)
public class DeliveryAddress {

    @JsonProperty("AddressLine")
    private String addressLine;
    @JsonProperty("City")
    private String city;
    @JsonProperty("State")
    private String state;
    @JsonProperty("PinCode")
    private String pinCode;
    @JsonProperty("ContactNumber")
    private String contactNumber;
}

So we are annotation each field with @JsonProperty so this annotation will map the field names from address.json file to this class field. We use another annotation @JsonInclude(JsonInclude.Include.NON_NULL) to ignore nulls if there are fields with null value 

6. Now we define LineItem model class

@Data
@JsonInclude(JsonInclude.Include.NON_NULL)
public class LineItem {

    @JsonProperty("ItemCode")
    private String itemCode;
    @JsonProperty("ItemDescription")
    private String itemDescription;
    @JsonProperty("ItemPrice")
    private Double itemPrice;
    @JsonProperty("ItemQty")
    private Integer itemQty;
    @JsonProperty("TotalValue")
    private Double totalValue;

}

7. Now we define PosInvoice model class

@Data
@JsonInclude(JsonInclude.Include.NON_NULL)
public class PosInvoice {

    @JsonProperty("InvoiceNumber")
    private String invoiceNumber;
    @JsonProperty("CreatedTime")
    private Long createdTime;
    @JsonProperty("StoreID")
    private String storeID;
    @JsonProperty("PosID")
    private String posID;
    @JsonProperty("CashierID")
    private String cashierID;
    @JsonProperty("CustomerType")
    private String customerType;
    @JsonProperty("CustomerCardNo")
    private String customerCardNo;
    @JsonProperty("TotalAmount")
    private Double totalAmount;
    @JsonProperty("NumberOfItems")
    private Integer numberOfItems;
    @JsonProperty("PaymentMethod")
    private String paymentMethod;
    @JsonProperty("TaxableAmount")
    private Double taxableAmount;
    @JsonProperty("CGST")
    private Double cGST;
    @JsonProperty("SGST")
    private Double sGST;
    @JsonProperty("CESS")
    private Double cESS;
    @JsonProperty("DeliveryType")
    private String deliveryType;
    @JsonProperty("DeliveryAddress")
    private DeliveryAddress deliveryAddress;
    @JsonProperty("InvoiceLineItems")
    private List<LineItem> invoiceLineItems = new ArrayList<LineItem>();
}

8. Now we create AddressGenerator service class to generate the address 
     We are annotating this class with @Service annotation which provide some business functionalities. So this class is simpl,e Java class that offers business functionality to generate a random address 
    Constructor is doing 3 things, first initialize a Java Random number generator, second initialize a Jackson Databind Object mapper and using Jackson object mapper we read address.json data file, we use readValue() to read data file and load it into an array called "addresses"
     If someone want to get the address we can call getNextAddress()
     We generate one random number between 0 to 100 and takes one address to return it 

@Service
public class AddressGenerator {
    
    private final Random random;

    private final DeliveryAddress[] addresses;

    private int getIndex() {
        return random.nextInt(100);
    }

    static AddressGenerator getInstance() {
        return ourInstance;
    }

    public AddressGenerator() {
        final String DATAFILE = "src/main/resources/data/address.json";
        final ObjectMapper mapper;
        random = new Random();
        mapper = new ObjectMapper();
        try {
            addresses = mapper.readValue(new File(DATAFILE), DeliveryAddress[].class);
        } catch (Exception e) {
            throw new RuntimeException(e);
        }
    }

    DeliveryAddress getNextAddress() {
        return addresses[getIndex()];
    }
}

9. ProductGenerator will generate line item and 
@Service
public class ProductGenerator {
    
    private final Random random;
    private final Random qty;
    private final LineItem[] products;

    public ProductGenerator() {
        String DATAFILE = "src/main/resources/data/products.json";
        ObjectMapper mapper = new ObjectMapper();
        random = new Random();
        qty = new Random();
        try {
            products = mapper.readValue(new File(DATAFILE), LineItem[].class);
        } catch (Exception e) {
            throw new RuntimeException(e);
        }
    }

    private int getIndex() {
        return random.nextInt(100);
    }

    private int getQuantity() {
        return qty.nextInt(2) + 1;
    }

    public LineItem getNextProduct() {
        LineItem lineItem = products[getIndex()];
        lineItem.setItemQty(getQuantity());
        lineItem.setTotalValue(lineItem.getItemPrice() * lineItem.getItemQty());
        return lineItem;
    }
}

The constructor is reading products.json file and loading it into LineItem array.
    We have getNextProduct() method which randomly take one item from array and set the quantity, we are setting quantity using a random number between 1 to 3


10. InvoiceGenerator will generate the invoice 

@Service
public class InvoiceGenerator {
   
    private final Random invoiceIndex;
    private final Random invoiceNumber;
    private final Random numberOfItems;
    private final PosInvoice[] invoices;

    @Autowired
    AddressGenerator addressGenerator;
    @Autowired
    ProductGenerator productGenerator;

    public InvoiceGenerator() {
        String DATAFILE = "src/main/resources/data/Invoice.json";
        ObjectMapper mapper;
        invoiceIndex = new Random();
        invoiceNumber = new Random();
        numberOfItems = new Random();
        mapper = new ObjectMapper();
        try {
            invoices = mapper.readValue(new File(DATAFILE), PosInvoice[].class);
        } catch (Exception e) {
            throw new RuntimeException(e);
        }
    }

    private int getIndex() {
        return invoiceIndex.nextInt(100);
    }

    private int getNewInvoiceNumber() {
        return invoiceNumber.nextInt(99999999) + 99999;
    }

    private int getNoOfItems() {
        return numberOfItems.nextInt(4) + 1;
    }

    public PosInvoice getNextInvoice() {
        PosInvoice invoice = invoices[getIndex()];
        invoice.setInvoiceNumber(Integer.toString(getNewInvoiceNumber()));
        invoice.setCreatedTime(System.currentTimeMillis());
        if ("HOME-DELIVERY".equalsIgnoreCase(invoice.getDeliveryType())) {
            DeliveryAddress deliveryAddress = AddressGenerator.getInstance().getNextAddress();
            invoice.setDeliveryAddress(deliveryAddress);
        }
        int itemCount = getNoOfItems();
        Double totalAmount = 0.0;
        List<LineItem> items = new ArrayList<>();
        ProductGenerator productGenerator = ProductGenerator.getInstance();
        for (int i = 0; i < itemCount; i++) {
            LineItem item = productGenerator.getNextProduct();
            totalAmount = totalAmount + item.getTotalValue();
            items.add(item);
        }
        invoice.setNumberOfItems(itemCount);
        invoice.setInvoiceLineItems(items);
        invoice.setTotalAmount(totalAmount);
        invoice.setTaxableAmount(totalAmount);
        invoice.setCGST(totalAmount * 0.025);
        invoice.setSGST(totalAmount * 0.025);
        invoice.setCESS(totalAmount * 0.00125);
        logger.debug(invoice);
        return invoice;
    }
}

So InvoiceGenerator uses AddressGenerator and ProductGenerator for constructing the invoice, so we autowired both the services. 
    Inside constructor we use Jackson object mapper to read invoice template and load it in an array 
    We have getNextInvoice(), first we take one random invoice from array and filling details like invoicenumber and createdTime. If invoice template is home-delivery invoice, we need to fill delivery address, so we use AddressGenerator to get one random address and assign it 
   Similarly we use ProductGenerator service to insert some lineitems into my invoice. Finally we fill out other details such as quantity,price, taxes and return the invoice 

11. Create KafkaProducerService class 
       First annotate with @Service and @Log4j2, then we will inject KafkaTemplate with key as String and value as PosInvoice 
       Next we create method that takes an invoice and  create a log entry and send message to kafka topic which takes topic name, storeid as key and invoice object as value 

@Service
@Log4j2
public class KafkaProducerService {
    
    @Value("${application.configs.topic.name}")
    private String topicName;

    @Autowired
    KafkaTemplate kafkaTemplate;

    public void sendMessage(PosInvoice invoice) {
      log.info(String.format("Producing Invoice No: %s", invoice.getInvoiceNumber()));
      kafkaTemplate.send(topicName,invoice.getStoreID(),invoice);
    }
} 

12. Configure kafka properties in application.yml

spring:
   kafka:
      producer:
         client-id: json-pos-simulator
         bootstrap-servers: localhost:9092
         key-serializer: org.apache.kafka.common.serialization.StringSerializer
         value-serializer: org.springframework.kafka.support.serializer.JsonSerializer 
         properties:
           spring.json.add.type.headers: false 

#The above configuration is for JsonSerializer, by default, the JsonSerializer includes the type information to the message header. We do not want the serializer to include the type info and cause a problem at the time of deserialization 

application:
   configs:
      invoice.count: 60
      topic.name: pos-topic
#invoice.count specify number of invoices, we will use it to create a loop counter 

13. Now we implement ApplicationRunner in main class and override run() to call other functionalities
    We injected KafkaProducerService and InvoiceGenerator service, then we use InvoiceGenerator to generate an invoice and use the Producer service to send it to kafka

@SpringBootApplication
public class KafkaInvoiceGeneratorApplication implements ApplicationRunner {

	public static void main(String[] args) {
		SpringApplication.run(KafkaInvoiceGeneratorApplication.class, args);
	}
	
	@Autowired
	KafkaProducerService producerService;
	
	@Autowired
	InvoiceGenerator invoiceGenerator;
	
	@Value("${application.configs.invoice.count}")
	private int INVOICE_COUNT;

	@Override
	public void run(ApplicationArguments args) throws Exception {
		for(int i=0;i<INVOICE_COUNT;i++) {
			producerService.sendMessage(invoiceGenerator.getNextInvoice());
			Thread.sleep(1000);
		}
	}

}

14. Start confluent server
15. Create topic called "pos-topic"
16. Start appl, once started appl will send 60 invoices to pos-topic
17. Start console consumer to view those invoices


Producing Avro messages
      Previously we created KafkaTemplate to send an invoice object to kafka cluster. We are not sending string but we are sending Java object called Invoice.
      KafkaTemplate will automatically serialize this invoice object as JSON string and send it to kafka topic, since we configured template to use JSON serialization in configuration file. So whatever object we want to serialize as JSON, it must have been created from JSON friendly class. 
     Now we are going to generate POSInvoice object using Avro serialization

1. Create KafkaAvroInvoice with spring for apche kafka, lombok dependency 

2. Now we create 3 model classes but it should be avro friendly class. So we want to use maven plugin to automate avro friendly class 

<dependency>
     <groupId>org.apache.avro</groupId>
     <artifactId>avro</artifactId>
     <version>1.9.2</version>
</dependency>

Now we add avro plugin to create class

<plugin>
    <groupId>org.apache.avro</groupId>
    <artifactId>avro-maven-plugin</artifactId>
    <version>1.8.2</version>
    <executions>
       <execution>
          <phase>generate-sources</phase>
          <goals>
              <goal>schema</goal>
          </goals>
          <configuration>
              <sourceDirectory>src/main/avro</scourceDirectory>
<outputDirectory>${project.build.directory}/generated-sources</outputDirectory>
              <imports>
                  <import>${project.basedir}/src/main/avro/LineItem.avsc</import>
                  <import>${project.basedir}/src/main/avro/DeliveryAddress.avsc</import>  
              </imports>
              <stringType>String</stringType>
          </configuration>
       </execution>
    </executions>
</plugin>


3. Create DeliveryAddress.avsc, LineItem.avsc, PosInvoice.avsc avro files inside src/main/avro
      Each file has some fields which comes with field name and datatype, all datatypes are either string or null
      PosInvoice has a cross reference to DeliveryAddress and LineItem, so we must configure plugin for cross reference. 
      In pom.xml, we configured the plugin to import 2 files DeliveryAddress and LineItem because they are being cross referenced in another file
      We also configure this plugin to automatically generate class defination in generate-sources directory 

4. mvn clean install
       It will generate the source files inside generated-sources folder 

5. Copy dataGenerator, service packages to this project

6. Copy main class info from previous project 

7. Copy configuration file also

spring:
   kafka:
      producer:
         client-id: avro-pos-simulator
         bootstrap-servers: localhost:9092
         key-serializer: org.apache.kafka.common.serialization.StringSerializer
         value-serializer: io.confluent.kafka.serializers.KafkaAvroSerializer
         properties:
            schema.registry.url: http://localhost:8081/

application:
   configs:
      invoice.count: 60
      topic.name: avro-topic

8. This uses confluent avro serializer, so we have to provide additional dependency in pom.xml

<dependency>
     <groupId>io.confluent</groupId>
     <artifactId>kafka-avro-serializer</artifactId>
     <version>6.0.0</version>
</dependency>

This confluent is not available in maven, it comes from confluent repo so we to add

<repositories>
    <repository>
         <id>confluent</id>
         <url>https://packages.confluent.io/maven</url>
    </repository>
</repositories>

9. Start confluent server
10. Create avro-topic
>kafka-topics.bat --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic avro-pos-topic --create

11. Start console consumer 
>kafka-console-consumer.bat --bootstrap-server localhost:9092 --topic avro-pos-topic --from-beginning --property key.deserializer=org.apache.kafka.common.serialization.StringDeserializer --property print.key=true --property key.separator=":"

12. Start the appl, we can see in consumer console all 60 invoices will be displayed

Real time Stream processing requirement
      Consider we are working for retail store, and we have tons of retail stores spread all over the country  The management decided to transform themselves into a real time data driven organization. So they started sending their invoices to a kafka topic in kafka cluster in real time
      Next step, they want to create the following automated services like shipment service, Loyalty management service and Trend analytics.
      Now we are asked to create kafka streams appl to perform following tasks
1. Select invoices where DeliveryType="HOME-DELIVERY" and push them to shipment service
       We have pos-topic where all the invoices are coming, now we create a service that listens to pos-topic, filter out everything but Home-delivery invoice and push to shipmentservices
2. Select invoices where customerType="PRIME" and create a notification event for the Loyalty Management service
    We need another listener to listen to same pos-topic and filter for prime customers, then we want to transform invoice object to notification object, finally we want to send notification object to loyaltytopic
3. Select all invoices, mask the personal info and create records for Trend analytics. 
      We need another listener that listen to pos-topic, then we transform invoice object to masked invoice object, then we need to flatten the invoice object and extract individual items from the invoice to create a Hadoop friendly record format, finally we want to send to hadoop-sink-topic

Now we combine all 3 requirements, here we are creating 3 listeners because we have 3 output requirements. All of these 3 listeners are reading the same input from same pos-topic, so every incoming invoice will be processed by all 3 listeners. If output is supposed to go to single topic or output format is same even if it was going to 3 different topics, we could have managed with single listener. But in this case we want to send 3 different types of messages to 3 different topics, so we need 3 listeners
    In previous session we created 2 versions of invoice generator first produce json formatted invoice and second produce avro formatted invoice. Here we want to process those invoice and produce output in json or avro format. So here we read JSON invoice and produce avro output, similarly we read avro invoice and produce JSON output 

1. Create JSONAvroInvoice spring boot project with cloud stream, spring for apache kafka, lombok, spring for apache kafka streams dependency and also avro, confluent avro serde dependency 

<dependency>
     <groupId>org.apache.avro</groupId>
     <artifactId>avro</artifactId>
     <version>1.9.2</version>
</dependency>
<dependency>
     <groupId>io.confluent</groupId>
     <artifactId>kafka-streams-avro-serde</artifactId>
     <version>5.4.3</version>
</dependency>

Previous project we were creating a Kafka producer to send avro messages, kafka producer are designed to use a serializer so we added dependency for confleunt kafka-avro-serializer. However in this project we use kafka streams API, we wont be using producer API. The kafka streams API does not need a serializer, instead the streams will be using Serde so we are adding kafka-streams-avro-serde

We also need a maven avro plugin to create avro friendly data model 

<plugin>
    <groupId>org.apache.avro</groupId>
    <artifactId>avro-maven-plugin</artifactId>
    <version>1.8.2</version>
    <executions>
       <execution>
          <phase>generate-sources</phase>
          <goals>
              <goal>schema</goal>
          </goals>
          <configuration>
              <sourceDirectory>src/main/avro</scourceDirectory>
<outputDirectory>${project.build.directory}/generated-sources</outputDirectory>
              <stringType>String</stringType>
          </configuration>
       </execution>
    </executions>
</plugin>

<repositories>
		<repository>
			<id>confluent</id>
			<url>https://packages.confluent.io/maven</url>
		</repository>
	</repositories>

2. We need to create input and output model, input is coming in JSON format so we need JSON friendly classes for input model
    So copy DeliveryAddress, LineItem, PosInvoice model classes from previous project and paste it

3. We need AVRO based output, so we create avro schema and generate AVRO friendly classes
    Create avro folder inside src/main and keep notification.avsc and Hadoop.avsc 

4. mvn clean install
       - So it will create AVRO friendly class

5. - Now we want to select invoices where customerType="PRIME" and create a notification event for the Loyalty Management service
    So we want to transform invoice to notification record, kafka streams api will not do it for u because it is custom business requirement 
    So we create Notification getNotification(PosInvoice invoice) method which takes an invoice, applies necessary logic and returns Notification object 
    - For the next requirement, select all invoices, mask the personal info and create records for Trend analytics.
     This requirement needs 2 transformation (ie) masking the invoice and flattening the invoice, so we need 2 more business transformation functions  
     First method is PosInvoice getMaskedInvoice(PosInvoice invoice) which takes an invoice and return an invoice which should be masked 
     Similarly next function List<HadoopRecord> getHadoopRecords(PosInvoice invoice), which takes the invoice then flatten the invoice and return list of HadoopRecords  

Create RecordBuilder class with @Service annotation, 
   First getNotification() creates a blank notification, once we have notification object fill in the required details from the input invoice and return it 
   Next getMaskedInvoice() will take input invoice, set the customer cardnumber and address details to null and returns it 
   Finally getHadoopRecords() will create a list of hadoop records by loop through the line items in the given invoice and creating one new HadoopRecord for each line item, then add these records to list and return the list 

6. Create input output channel and configure them in application.yml
   The channel definations will come under spring.cloud.stream.bindings. First we provide channel defination for notification service, first is input channel mapped from pos-topic, next is output channel mapped to loyalty-topic. Similarly we create another pair of channels for Hadoopservice, first is input channel mapped from pos-topic,next is output channel mapped to hadoop-sink-topic.
     The next step to configure the binder and also provide some additional binding details. First we have to provide broker connection details, and all our output will go in avro format and we will be using confluent avro serializer package which depends on confluent schema registry, so we need to add schema registry configuration 
     Next the channel is created by spring cloud stream, but setting serialization activity is performed by binder, so we define those details for each channel under the binder. We need to send output as avro so we are setting valueSerde for both output channel 
     We want to send output as string key and avro value, but we are setting only output valueSerde, we not setting inputSerde and not even any serde for my key, because those values will find some defaults to match our requirements. We can set the record serialization format for each incoming and outgoing channel, we are not care about incoming channel serialization because it comes as JSON which is the default

7. Create binding interface for your channels 
       We create 2 input channels so we create 2 input  methods, similarly we have 2 output channels so we create 2 output methods
       The primary purpose of interface is to map the channel with the record type. notification-input-channel and hadoop-input-channel have same record type called KStream of string key and PosInvoice value 
       notification-output-channel is KStream of string and Notification value, similarly hadoop-output-channel is KStream of string and HadoopRecord value 
 
8. Now we will read the records from the input channel and start processing them, so we create 2 stream listeners one for generating notifications and another for HadoopRecord 
     Create NotificationProcessorService with @Service and @Log4j2, then we also apply @EnableBinding and bind it to PosListenerBinding interface
    We create @streamlistener process method and implement our processing logic inside the method. Now spring cloud stream framework will listen to "notification-input-channel", as soon as new invoice record comes to channel topic, the framework will deserialize the message to PosInvoice object and call this process() passing invoice as KStream
   We use RecordBuilder so we autowire it, so we taking invoice input and applying a filter, then we apply mapValues() transformation on the filtered input  to change it to a notification record. The result of these two operation is a new KStream of the notification object. Now we can use forEach() on this notification stream and produce some log entries. We are returning the notification, the spring cloud stream framework will take the return value and send it to the notification-output-channel using @SendTo annotation 
    Similarly create HadoopRecordProcessorService also

9. Start confluent server

10. Start KafkaInvoiceGenerator project, becauce we need invoices, So when we run it will store all 60 invoice in pos-topic

10. Create loyalty-topic, hadoop-sink-topic
>kafka-topics.bat --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic loyalty-topic --create

>kafka-topics.bat --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic hadoop-sink-topic --create

11. Start  the appl, we can see log of invoice messages

Real life Serialization Scenarios
    In previous example, we were reading a stream of JSON formatted invoices, then we processed those invoices in real time and produced AVRO formatted output. Now we will be reading stream of AVRO formatted invoices, then we processed those invoices in real time and produced JSON formatted output.
     Both examples will have exact same processing, the only difference is input serialization and output serialization 

Why serialization is important?
     In real life project we might be getting data from bunch of sources. Consider we have an ecommerce portal, the portal generates some online transactions, we are also sending these transactions to a kafka topic for some real time streaming operations. However it is web based appl, so it is little easy to send the events to Kafka in JSON format 
    On the other side, we also have some enterprise appl generating some transactions that go into a database, we also want to bring those transactions to a kafka topic to perform some real time streaming operations. But these are packaged enterprise appl and we cannot modify them to send data to kafka. But application offers you a kafka connect connector, so we decided to implement kafka connect to bring data from these appl to ur kafka topic 
     Similarly we have many other sources that are generating real time streaming data,and you manage to bring them to kafka topic. We also have many stream processing appl processing these streams and again generating inputs to the kafka topic for other applications.
      So in a large enterprise, we will see a mesh of streaming events coming to kafka and going out of kafka topics. Now if we analyze these events, we minght find 2 broad categories of input formats
1. Input stream from non kafka technologies, these inputs are more often JSON formatted events and they are not likely to use schema registry
2. Input Streams from native kafka technologies, these inputs are often AVRO formatted events and they will be using Confluent Schema Registry

So in previous example, the invoice events are coming in JSON format and they do not use a schema registry, we have read and procee them. The output of a Kafka Streams appl is more likely to be a AVRO and use a schema registry 

Processing AVRO message Stream
      In this example, we will be reading the AVRO formatted invoices from the kafka topic, then we will process them to produce output in loyalty topic and hadoop sink topic 

1. Create AvroJSONInvoice spring boot project with cloud stream, spring for apache kafka, lombok, spring for apache kafka streams dependency and also avro, confluent avro serde dependency 

2. Create data model
      We are going to read invoices from kafka topic and we modeled the invoice using 3 classes like DeliveryAddress, LineItem and POSInvoice. These classes should be avro friendly class
   1. Create avro schema
   2. mvn clean install, it will create AVRO friendly classes 
    
3. The output data model needs 2 classess (ie) Notification and HadoopRecord and those records are to go as JSON messages, so we need to create JSON friendly class using @JSON annotations

4. Next we create data transformation services and for us we need 3 methods getNotification(), getMaskedInvoice and getHadoopRecords
    Copy paste RecordBuilder class from JSONAvroInvoice project 

5. Create binding interface
 Copy paste PosListenerBinding class from JSONAvroInvoice project 

6. Create listener service class
 Copy paste HadoopRecordProcessorService and NotificationProcessorService class from JSONAvroInvoice project 

7. Configure kafka channels and stream binders in application.yml file
   The yml file defines 2 input and 2 output channels, but we need to change input topic name, because avro-topic topic is containing invoices to these project 
   Now we will be reading AVRO messages which were serialized using the confluent AVRO serialization package, to deserialize those messages, we must specify the AVRO deserialization package 
    Now we added Avro serialization for notification-input-channel, similarly hadoop-input-channel also used Avro serialization. Now notification-output-channel is configured to used confluent JSON serialization package, similarly hadoop-output-channel also uses JSON serialization package 

8. Start KafkaAvroInvoice to generate invoices for this project

9. Start the appl and check notication and hadoop messages as json format  


Understanding Record Serialization
     Stream processing is all about data processing in real time, these data events will come to a kafka topic as a continuous stream of messages. These messages will flow over the wire to kafka and sit into the kafka topic. When you create a listener to read these messages from kafka topic, they will again flow over the wire and come to ur listener and these messages will travel as byte array
    So when the message is in transit over the network, it must be in a byte array format, however we want to see those messages as a Java object. 
    In previous project, the listener function is expecting the incoming message value to come as a POSInvoice object. However, this POSInvoice message travelled from the kafka topic as a byte array, but we cannot process byte array, we want to look at these messages as a Java object to apply methods to them and transform them as per ur requirements 
   The framework takes the byte array and converts it to a POSInvoice object before passing it to this listener function and this is known as deserialization, so converting byte array to a Java object is known as deserialization. Now we will process these incoming objects and produce another Java object. We will return the Java object which again goes to Kafka topic, now the framework will take the outgoing object, convert it to a byte array and send it to the kafka topic, this work is known as Serialization. So the framework converting an object to byte array is Serialization process 

How does serialization and deserialization happens?
    We have 2 frameworks Spring cloud stream and kafka streams which is available as Kafka Streams binder. Both of these frameworks are capable of doing the serialization and deserialization work. It is up to we decide who you want this work to be done (ie) spring cloud streams or kafka streams. We can configure ur choice using following properties

Deserialization-spring.cloud.stream.bindings.your-input-channel-name.nativeDecoding: true

Serialization- spring.cloud.stream.bindings.your-output-channel-name.nativeEncoding: true

Native Decoding is for input channels, so this one will configure the deserialization process. Native Encoding is for output channels, so that one configures the serialization process. The term native is meant for Kafka Streams, so if we enable native encoding/decoding then the serialization work is taken care by Kafka Streams framework. If we disable it, then the serialization work is performed by Spring cloud streams. By default it is true, so serialization work is performed by the Kafka Streams framework 
    Kafka Streams framework comes with some built-in classes to convert byte array to the native Java objects and types, these classes are known as Serde which stands for Serialization and Deserialization. So we have built-in Serde classes like StringSerde, IntegerSerde, LongSerde, ShortSerde, BytesSerde, DoubleSerde, FloatSerde and ByteArraySerde 
    So with these Serde classes, kafka streams already knows how to translate simple values like String, Integer or Double to wire format. These are standard Java types, so Kafka streams framework comes with built-in support for these classes.
    The problem comes when we are using complex and custom Java objects, Kafka streams does not know how to serialize or deserialize them, because kafka streams framework does not have Serde class for these objects, we can fix this problem by creating a custom Serde class for these objects
     Creating custom Serde class is an option, but Confluent Kafka offers you a straightforward method. They offered some standard Serde classes such as SpecificAvroSerde and KafkaJsonSchemaSerde, we can use these Serde classes to serialize or deserialize any Java object 
    So if we want to use SpecificAvroSerde, then we must create Avro friendly class. If ur class defination is Json friendly, we can use KafkaJsonSchemaSerde 
    We must provide a Serde to Kafka streams for performing the serialization and we have 2 options
   1. Define a custom Serde for ur data model
   2. Create Avro friendly or JSON friendly data model and use one of the standard Serde offered by Confluent Kafka 

Refer HadoopRecordProcessorService.java
       This service listens to kafka topic and brings in the incoming message to the process() method. Now kafka streams binder will look at the input parameter types which is KStream of String key and the value is defined a PosInvoice object. So the binder knows that is should convert the incoming message key to a String, similarly it should convert incoming message value to PosInvoice 
   So the binder will look for the custom Serde in ur project classpath and matches with the signature of this input parameter and look for the standard Serde classes such as StringSerde, IntegerSerde etc and try to match with input parameter with one of these, in this example the message key is a String. So the binder will deserialize the message key using the StringSerde class 
    Now we left with the message value, which does not match any standard Serde classes, so binder will look into ur appl configurations yml file and then the binder will use the configurations and try to deserialize this message value 
    We configured SpecificAvroSerde for this input channel, so the binder will assume that PosInvoice class is an Avro friendly class and deserialize it using SpecificAvroSerde. In case if we do not configure Serde for this channel, then the binder will try to use JsonSerde provided by Spring Kafka 
    Similarly binder will look for the return type, in this example return type goes with String key and HadoopRecord value, so binder will try to find matching serde in classpath and then tries to match it with standard serde classes. The next step is look for application yml and use the configuration, if nothing configured then ise JsonSerde given by Spring framework and this is all about serialization process

KStream methods
    KStream represents a record stream of key/value pairs. All KStream functions are designed to work using functional programming approach, which accepts lambda function as an argument 
    We can take KStream, apply any method of KStream and the result is another KStream 

1. filter and filterNot - used to filter out some records from ur KStream

2. map - used to transform key and value of KStream (one to one transform)
  mapValues - used to transform only KStream value and leaving the key unchanged
   flatMap, flatMapValues - accept one message and return a list of messages (one to many transform)

3. forEach() - loop through all the records in a KStream
  forEach is similar to map() which is also a looping method where we take one input record at a time, transform it and return the transformed message. forEach is also same but does not return anything
    So we cannot use forEach method for transformation purpose, we often use foreach method when we do not want to transform the KStream 
    We cannot chain foreach method in between other KStream methods, because all other methods will return a KStream so we can apply other KStream methods on the return value. However, foreach method returns nothing, so KStream method call chain terminates. We cannot call any other method after forEach 

4.     So alternative is, KStream offers you the peek() method for the same.
      peek() method is an alternative to foreach method because it returns back the same KStream (ie) we can insert a peek method between the chain of transformations to log the input record and still continue the chain of calls 

5. print - to print the KStream to the console, this is used only for debugging purposes, you should avoid using it in prod code

6. branch - used to split one stream into two streams using some conditions 
7. to - used for sending messages to kafka topic
8. toTable - used to convert KStream to KTable 

Kafka Streams - Exactly Once Implementation
        Previously we created an example to read one stream and produce two streams (ie) we created two input channels, two listener services to process those input channels and produced output to two output channels 
       In HadoopRecordProcessorService of AvroJsonInvoice project, here we listens to hadoop-input-channel and writes to hadoop-output-channel. Similarly in NotificationProcessorService, we listens to notification-input-channel and writing to notification-output-channel.
    Here we have two problems in this implementation
   1. The first problem is repeated reading, we are reading the same kafka topic twice (ie) HadoopRecordProcessorService and NotificationProcessorService are reading the same input topic from the Kafka cluster. We created 2 different channels, but both the channels are pointing to same kafka topic which is not efficient (ie) same message travel twice over the network 
    2. Second problem is separate transactions, here we implemented 2 different listener services called HadoopRecordProcessorService and NotificationProcessorService, both of these listeners are independent and the spring framework will start these two listeners in 2 different threads, so they will running in parallel 
    In some scenarios, this is what we want to achieve (ie) we want to implement two independent parallel listeners doing different work. But in this case, both the processors are light weight and both are reading the same input topic, so is makes sense to combine these 2 listener services into one service. But we need to know how to implement two listeners in single spring boot appl and also wanted to understand that each listener needs a separate input and output channel configuration and specify different serializer for each channel. We also discussed @SendTo annotation which allows you to send the return value to kafka topic either to single or multiple topics. But in this example we have send to single topic so we return one value from process method. If we want to send the output to two or multiple topics, then we can return an array from process method.

Limitations of @SendTo
   1. If we want to send the output to multiple topics using @SendTo annotation, all the output records should be of the same type. In the previous example we want to send Notification and HadoopRecord which is two different types of records and @SendTo annotation does not support this requirement, that is another reason for creating two listeners in previous example since we produce two different types of outputs 

Now we want to combine these 2 listeners to achieve 
   1. Read once produce multiple - Read input only once and produce Notification as well as HadoopRecord from the same listener 
   2. Implement Exactly once transaction - In Kafka streams appl, we will read input records, do some processing and produce output to one or more topics 
    Each input record is processed only once, and it produces the output only once against each input, this is called Exactly once processing. That means each input record is processed exactly once, even in case of failures 
    Implementing exactly once is simple for a single threaded non-distributed application, but in case of distributed application it is hard to achieve due to failure scenario
     In this example, we are reading some records from the input topic, these records will not come to you one by one. The kafka listener internally pulls packet of multiple records from the kafka cluster. Consider we pulled packet of 10 records, now the listener will process all of those 10 records one by one. Now we will produce output for those 10 records and send it to these two topics 
    In this example, Notification receives one output for each input,so we send 10 records to notification topic. However hadoop-sink topic receives multiple output records for each input record because we used flatMap in the processing. So consider hadoop-sink-topic receives 40 output records for 10 input records 
     Now these records are not sent to kafka topic one by one, these records are also sent to the kafka topic as a packet of records. So listener pulls a packet of input messages from kafka, process them one by one and send all the output back to the target topics as an output packet. The listener will also wait for the acknowledgement from the output topics that the packet is successfully received and safely stored in the kafka cluster 
     Once we get acknowledgement for the output, the listener will also send back the commit acknowledgment to the input topic that the input messages were received and processed. This cycle is one transaction, so one transaction involves one input and two output topics
      A complex application requirement might extend this scenario to have a single transaction involving multiple input topics and multiple output topics. In this example, we have one input anf two outputs, but we may have 3 or 5 inputs and 5 or 10 output channels and still want to do all of the work as single atomic transaction

What do we mean by single atomic transaction?
    Either all of the activities of a transaction are successfully committed or all of this is rollbacked. In practice, this atomic transaction is difficult to achieve for a distributed system like Apache kafka 
     Consider our appl sends data to the notification topic and sends some data to the hadoop topic. The acknowledgement from Hadoop topic name, however acknowledgement from the notification topic didnt come because of millisecond network outage. The messages were received on the Notification topic, the receiving broker sent an acknowledgement also. But the acknowledgement didnt reach ur appl because you had a network outage for a millisecond 
    Now ur appl will wait for a while and get a time out, in case of timeout the appl will retry sending the last packet once again. The previous message was already received, this retry is also successful but your data is now duplicate in Hadoop topic. So the retry in case of not receiving an acknowledgement is one source of data duplication 
     The second problem is, we received a packet of input, processed it, sent the output to both the topics, also received an acknowledgement. Now the last thing is to send a commit acknowledgement to the input topic. However our appl crashed for some reason, so the commit acknowledgement is not sent back to the input topic. So when we restart the appl, we will read the same packet once again, process it and send the outputs. But we processed it twice (ie) we have already processed the same record before failure but could not store the commit successfully, so we reprocessed the same input packet after the failure and thats going to corrupt all output topics with duplicates 
    So the main problem of creating duplicates is caused in a failure scenario, if everything runs without a failure, we may not see a problem at all 

1. Create kafka-ExactlyOnce project with lombok, cloud stream, kafka binder, spring for apache kafka 

2. Configure all properties in application.yml
       We are creating one input channel and we will be creating a single listener to read the incoming invoices from this channel. Previously we create two input channels because we planned to create two listeners. But in this example we are planning to create only one listener and do everything in same single listener, so we are creating a single input channel 
    Next we have configured broker and schema registry url. Next we define processing.guarantee as exactly_once, the default value of this configuration is at_least_once. at_least_once will ensure that we do not lose any record, and all the records are processed at least once. But we want to implement an exactly once transaction,so we are setting it to exactly once. We must ensure that we do everything in a single listener 
     Two listeners are always two transactions, so if we want to implement some work as a single transaction, then all of those things should be done in a single listener. Next we will be reading an AVRO formatted invoice and producing AVRO formatted output, so we are setting the default Serde to SpecificAvroSerde because everything is AVRO formatted 

3. Paste avro schema defination like DeliveryAddress, LineItem, PosInvoice are to define input data model and HadoopRecord, Notification are to define output data model and  generate avro class files
>mvn clean install 

4. Create business logic, all our business logic was packed into RecordBuilder class

5. Create binding interface, we have only one input channel so we define only one method in this interface

6. Create listener service class with 3 annotations. We will using RecordBuilder class so we will autowire it 
    Now we will create process method which takes KStream of string key and PosInvoice value as argument, but this input will not come until we add @StreamListener annotation. The process() method returns a void, so Spring framework will not send any output to Kafka topic, but we want to send two outputs (ie) Notification and HadoopRecord
       First we start with input stream, then we apply mapValues() to mask the invoice, then we apply flatMapValues() to explode the invoice to HadoopRecords. This chain of transformation will change PosInvoice stream to a HadoopRecordstream. Next we create Notificationstream by applying two transformations like filter() and mapValues(). Now we apply forEach() on both of these streams to send it to the log. Finally we use to() on these two streams which is used to send output to a kafka topic. So we are sending hadooprecordstream to hadoop-sink-topic and the notificationstream to loyalty-topic 
    So in this example, the process method does not return anything and the Spring framework will not send any output to the kafka topic, so we didnt define any output channel in yml file. Instead we are using kafka streams to() method to directly send the output to the kafka topic
    So creating HadoopRecord, creating Notification, sending HadoopRecord to hadoop-sink-topic and sending NotificationRecord to loyalty-topic is performed in a single listener and hence it is performed using a single stream flow. So kafka streams will consider all of these as a single transaction, we also configured the kafka streams to apply exactly_once, so all these 4 operations are now part of a single transaction 
    If ur appl fails inbetween for any reason, kafka streams will rollback the incomplete transaction when you restart ur appl again. Once rollback is complete, your appl will start a new transaction from the point of the last successful commit 
    This transaction is implementing by Kafka for all kafka operations, non kafka operations are not part of the transaction. For example, consider we added some code in this process() method for inserting HadoopRecord to the database table, inserting into a database table is not a kafka operation, so database table insert is not protected against failure, and kafka will not rollback it. 
    Hence we never write to the database from the listener code, instead we send the output to a kafka topic and use kafka connector for databases to move records from the kafka topic to the database. The kafka connector implements transactions across database and will take care of failure scenarios 

7. Start KafkaAvroInvoice appl to get invoice object

8. Start this application  





Use case 2:
    Consider we are working for a global retail organization which has an ecommerce website to take orders from the customers. These orders are sent to kafka topics for planning and processing the delivery of purchased items
   We create Kafka streams appl that reads all the incoming orders and separate them into India delivery orders and International delivery orders.
   So the task is read the incoming order, look for the delivery address in the order, if the delivery address is for india, forward the order to India-orders topic else send it to abroad-orders topic.
    The first requirement here is that the incoming order data is coming as an XML message and outgoing data should be in JSON message. So what if incoming message is in some other format where we dont have serializer 
    Next requirement is every production appl must handle with errors and exceptions. Here we handle 2 types of errors, one is Malformed XML inputs where we must validate the incoming input message, if the input order does not confirm to a predefined schema then send it to error-topic and continue processing the next order. The second error may be due to the missing or incomplete address details, so incoming message does not have address then send it to error-topic and process the next order 
    Next requirement is all of this should be done as an exactly once transaction
    Next requirement is that outgoing messages should have an appropriate message key. For example, valid orders should go to the outgoing topic with order-id as key. The invalid orders must go out with the error code as a key 

So we will create stream processing appl which reads the input XML orders, validate it against two types of error. If u have an error, send the same XML order to error-topic, if the validation is passed convert it to a JSON formatted order, classify to india-orders or abroad-orders and send them to their respective topics. All outgoing orders must have a message key, all valid orders will be keys with order-id and invalid orders must be keyed with error code

1. Create KafkaXMLBranching spring boot project with cloud stream, spring for apache kafka,lombok and with Java8

2. We have requirement to read an XML formatted input message, however for reading an input message we need serializer package. When we were reading an AVRO message, we used serializer package offered by Confluent 
    When we wanted to read JSON messages, we use 2 types of serializer like spring cloud stream framework and confluent provided serializer. However XML does not have any serializer, neither Spring nor Confluent offers a serializer for XML messages
   In this example, we have a requirement to read XML, and we may have requirement to read CSV, BSON, MessagePack, CBOR, NetString, Pickle, Protocol Buffers and many more. 

How do we deal with such requirement?
1. Compatible kafka serializers
2. Library Support

    Look for compatible Kafka Serializers, if y have a supported serializer for the desired format, then it is simple. If you do not find a serializer, look for the library support for parsing and converting them to Java objects  
       For this example, we can parse XML messages and convert them to Java objects using JAXB library. JAXB allows you to map Java classes to XML representations
      We can also find jaxb2-maven-plugin for generating your data model from XML Schema defination. In case if we other formats we have to find related supporting library, however JSON, Avro and ProtocolBuffer are commonly used message formats in realtime  
     Consider we have sample XML Order, so we have order identified by order-id, we also have other fields such as order-by, ship-to-address and the items
    So we create XML schema for order xml, inside resources/schema/order.xsd

3. We also added some sample XML message for practising purpose inside "practiceXml" folder 
    We will be use this input messages to observe the behaviour of our appl

4. Create data model,  our input is XML, so we need XML friendly class 
    We have XML schema, also we have jaxb2-maven-plugin for generating java classes using this XML schema, so we need to configure this plugin in pom.xml which access schema from source location and generate class file is specified package
   - We also need 4 JAXB dependencies too 

5. mvn clean install
      - Expand, target - generated-sources - jaxb and we can see generated class file "Order", "ObjectFactory" class will be used by JAXB 

6. First we need to create datamodel for tagged order called OrderEnvelop with four feilds
   xmlOrderKey, xmlOrderValue are original xml order key and order value 
   orderTag can have either ValidOrder or ParseError or AddressError
   validOrder is Java object for XML input, if XML order is valid we will map the xml message to our input data model and set the value to this field 

7. Configure input and output channels in application.yml file 
     We create one input channel and 2 output channels, we are setting broker address, schema registry, processing.guarantee, finally we are setting KafkaJsonSchemaSerde for both of output channels

8. Define binding interface called OrderListenerBinding

9. Create business logic - Pseudocode of business logic

@StreamListener("xml-input-channel")
@SendTo({"india-orders-channel","abroad-orders-channel"})
public KStream<String,Order>[] process(KStream<String,String> input) {
    validOrderStream=input.mapValues(v-> validate-and-return-only-valid-orders);
    return validOrderStream.branch(isIndiaOrder,isAbroadOrder);
}
       We created listener to read xml order stream from "xml-input-channel", then we expect to validate the input order and take only valid orders, so we use mapValues() for doing the validation, finally split the valid order streams into 2 output streams, w ecan use KStream.branch() for splitting the streams. Finally @SendTo annotation will send both output streams to their respective kafka topics
    
@StreamListener("xml-input-channel")
@SendTo({"india-orders-channel","abroad-orders-channel"})
public KStream<String,Order>[] process(KStream<String,String> input) {
    taggedOrderStream=input.mapValues(v-> validate-and-return-only-tagged-orders);
    taggedOrderStream.filter((k,v)->!isValidOrder).to("error-topic");
    return taggedOrderStream.filter((k,v)->isValidOrder).branch(isIndiaOrder,isAbroadOrder);
}

   However we also need to send invalid orders to error-topic. We again start with input stream and use mapValues() for doing validation, but instead of returning input values only, we return all the orders, no matter whether it is valid or invalid, but it will add valid or invalid tag to order. 
   Next we can filter invalid orders and send it to "error-topic", similarly we can filter valid order and branch them to indiaorders and abroadorders 

OrderListenerService.java
     First 2 lines are reading error topic name from application.yml file.
     Next we have @StreamListener annotation which defines the input channel for this listener, we also have @SendTo annotation which defines 2 output channels for this annotation. So the listener is expected to return 2 output streams, first stream will go to india-orders and second stream will go to abroad-orders. But how to return two streams from one function, we can return as array of streams thats what we are doing here 
    The process() returns an array of KStream and the spring framework will send the first elt of the array to india-orders and second element goes to abroad-orders 
     First we are taking input stream and applying map() method, which takes key value lambda so we get the key and value of incoming xml order. We are creating new OrderEnvelop and setting xmlorderkey and xmlordervalue.
   Next step is to parse the xml input value, so we 
create JAXBContext for parsing Order class, again we use JAXBContext to create an unmarshaller which takes an XML string and converts it to Orders object. If xml order is valid, unmarshaller will parse it correctly and return an Order object, then we set that object inside OrderEnvelop and tag the order as "VALID_ORDER" 
     If parsing is successful we will be checking the address field for having a valid address, and here we are validating only city field. If address is invalid we again retag the order for "ADDRESS_ERROR"
     If the unmarshaller fails to parse xml order, we will get an exception which we handle in the catch block, the catch block will simply tag the order with PARSE_ERROR tag. Finally we return KeyValue pair from the lambda, orderTag becomes the key and orderEnvelop becomes the value, so outcome of map() transformation is KStream of OrderEnvelop 
    Next we will take orderEnvelopStream and apply filter operation and filtered for invalid orders, even we can use filterNot() instead of filter(), and we use to() to send the invalid orders to error_topic.
    to() will send stream to a kafka topic, so whenever we try to send something to kafka, the framework will try to serialize the message and then send it to kafka topic, the serialization requires a Serde. 
    Kafka Stream sframework will try to find suitable Serde for the OrderEnvelop, but we didnt define any Serde for this object, because we learned to define Serde for the input/output channels. We created one input channel for receiving XML orders and 2 output channels for sending india orders and abroad orders.       In application.yml file, output channels are using KafkaJsonSchemaSerde and the input channel is receiving XML string so it can be deserialized using String serde. The framework will automatically apply String Serde and we do not need to specify it.
     But for error-topic and orderEnvelop we do not have Serde configuration. We are Produced.with() to specify 2 Serdes, first Serde is for key which is String so we use String serde and second Serde is for value which is OrderEnvelop object so we are usinf custom Serde for OrderEnvelop which we define in AppSerdes class 
    We created AppSerdes class which extends Serdes class from Kafka Serialization package, then we create static method which returns custom Serde for OrderEnvelop. In fact we are using KafkaJsonSchemaSerde for OrderEnvelop, and we create an instance and configure it. The configuration has only 2 things, the first argument setting schema registry and second argument is telling that serde will be used for value and not for key, finally returning configured serde  
    So AppSerdes.OrderEnvelop() method will return custom serde and to() will send OrderEnvelop stream of invalid orders to error-topic. These messages will be serialized using KafkaJsonSchemaSerde, because custom Serde is defined to use KafkaJsonSchemaSerde 
    For valid orders we branch them or split them into streams (ie) india-orders and abroad-orders, we can do using branch() which takes comma separated list of predicates. So we create 2 predicate to check for india and other for abroad, finally pass this predicate to branch(), but if we want to create 3 or 5 branches then we can pass 3 or 5 predicates 

10. Start confluent server

11. Create topics
kafka-topics --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic xml-order-topic

kafka-topics --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic error-topic

kafka-topics --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic india-orders

kafka-topics --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic abroad-orders

12. Start application
13. Start kafka producer 
>kafka-console-producer --topic xml-order-topic --broker-list localhost:9092
    - Give input file as xml

14. Check kafka console consumer based on xml input
>kafka-console-consumer --bootstrap-server localhost:9092 --topic india-orders --from-beginning --property print.key=true --property key.separator=":"

>kafka-console-consumer --bootstrap-server localhost:9092 --topic abroad-orders --from-beginning --property print.key=true --property key.separator=":"

>kafka-console-consumer --bootstrap-server localhost:9092 --topic error-topic --from-beginning --property print.key=true --property key.separator=":"


Handling Poison pills
     Previously we were reading an XML input as a String, then we parsing the XML and handling the parse exception
   Consider now we are reading a JSON input, so we will use json serializer to parse the input JSON and convert it into a Java object. However the parsing of JSON input is performed by the framework, we do not need to parse it.

What happens if we receive malformed JSON input?
      The framework will try to parse it and deserialize the input JSON to Java object, but we received malformed input so the framework will fail to parse and throw an exception. These malformed input records are known as poison pills 
   Day by day we are sending 100 or 1000 of well formed input message, but once a day someone sends malformed record then appl will throws an exception and crashes 

How do we handle these malformed input deserialization exceptions ?
     Kafka Stream binder gives 3 options
1. logAndFail
      - default - will log the error and stop ur appl
2. logAndContinue
      - log the error record and continue processing the next record
3. sendToDlq
      - send the error record to kafka topic and continue processing the next record, this error record topic is often known as DLQ(Dead Letter Queue)
       Dead Letter Queue is the place to send unprocessed or undelivered messages for later investigation 

Where do we apply?
   We set deserialization exception handler in configuration yml file, DeserializationExceptionHandler is a binder configuration 

spring.cloud.stream.kafka.streams.binder.deserializationExceptionHandler: logToFail

spring.cloud.stream.kafka.streams.binder.deserializationExceptionHandler: logToContinue

spring.cloud.stream.kafka.streams.binder.deserializationExceptionHandler: sendToDlq

How to send error record to DLQ, what is DLQ topic name?
      Assume we are reading from 2 input topics topic-1 and topic-2, so we define 2 input channels like input-channel-1 and input-channel-2. Since we are reading from 2 input channel, we may get error record from any of those channels, so we create one DLQ topic for each input channel 

spring:
  cloud:
    stream:
      bindings:
        input-channel-1:
          destination: topic-1
        input-channel-2:
          destination: topic-2
      kafka:
        streams:
          binder:
            brokers:  localhost:9092  
          bindings:
            input-channel-1:
              consumer:
                valueSerde: io.confluent.kafka.streams.serdes.json.KafkaJsonSchemaSerde
                dlqName: topic-1-dlq
            input-channel-2:
              consumer:
                valueSerde: io.confluent.kafka.streams.serdes.json.KafkaJsonSchemaSerde
                dlqName: topic-2-dlq

DLQ is bindings configuration that goes to the internal consumer, so we define them under consumer configuration

KTABLE
    - Ktable is like a database table with primary key (ie) Ktable record must have a key because it is similar to a table with primary key
    - Ktable cannot have a null key, this is different than KStream, we have created many examples with key as null 
    - KTable implements UPSERT operation (ie) every record flowing to KTable will try to update an existing record of the same key, if the record for the same key does not exist yet, then KTable will insert it, so the record for the same key is overwritten by new record 
    - Null values in a Ktable are used to implement a DELETE operation (ie)  new record with a null value will DELETE the record for the given key 

How do we create KTable?
    We create a listener method and asked the framework to give us a KTable, the listener will internally create a Kafka consumer and read all the incoming messages from Kafka topic and give you KTable of records 

KTable Usecase
    We want to create simple Kafka streams appl that subscribes to a kafka topic using KTable API. Kafka topic with produce some stock-ticks like
     HDFCBANK:2120
     HDFCBANK:2150
     TCS:2920
The message key is stock symbol and value is current price for the given symbol, so Kafka Streams appl should read these messages and create a KTable out of them, then we want to apply filter on original KTable to result in a new KTable. Finally we want to convert last KTable to KStream and print the contents to the console

1. Create KTableDemo with cloud stream, Spring for apache kafka, spring for apache kafka streams, lombok dependency 
    Here we read string message key and string message value, so we do not need any additional serializer, schema registry and plugins etc 

2. Define appl configuration in application.yml
        We are creating one input channel,then we create binder configuration where we are giving broker coordinates 
        We know that input channel will internally create a kafka consumer to read messages from kafka topic and we are planning to read those messages using KTable API. 
       KTable must be saved locally by your appl, Kafka stream framework is designed to use Rocks DB local database to store the local copies of the KTable. The incoming records to the KTable should be materialized in the following KTable, so "stock-input-store" is Ktable name in local Rocks DB database 
      This is different from KStream, Kstream does not require local storage because the framework will read a packet of messages and immediately start sending them to ur listener, we dont want to store it anywhere. In case of KTable, the framework will pull some records from Kafka topic and see if we have duplicate records for the same key 
      For example, we pull 3 records 
     HDFCBANK:2120
     TCS:2920
     HDFCBANK:2150
Now the framework will look at these records by the key, we have 2 records for HDFCBANK, HDFCBANK:2120 came first and HDFCBANK:2150 came last

How do we know HDFCBANK:2120 came first?
      Every kafka record has got an offset number, if offset number is lower, it came first and higher offset number came next. We know HDFCBANK:2120 came first, so the framework will insert HDFCBANK:2120 into a local database, next it will insert TCS:2920 into local database, finally the framework will store HDFCBANK:2150 in local database 
     But we already have HDFCBANK:2120 in database, the primary key for the record is same, hence the framework will update the older record. So final state of local KTable will have only 2 records, the other record HDFCBANK:2120 is discarded by update operation
      HDFCBANK:2150
           TCS:2920

      Now the framework will wait for "commit.interval.ms" which is configured in yml file. The default value for this configuration 30000ms and we configure 10000ms. So the framework will wait for 10000ms and again pull some messages from Kafka topic.
    Consider we do have some more records like 
          HDFCBANK:2153
          HDFCBANK:2165
          HDFCBANK:2180
So they will also go to local KTable store and update the existing HDFCBANK record, so we have only 2 records in final KTable state store
           HDFCBANK:2180
           TCS:2920

       Once time for commit.interval.ms is complete, then framework will start sending these 2 records to your listener 
        state.dir is the location for Rocks DB database, so framework will create a local state store dir in ur current directory 

Why do we need a KTable to buffer some records, update them with latest record and send us only most recent record?
     We may have usecase where you care only about most recent records and KTable helps us to discard older records and optimize ur appl to process only the most recent records 

3. Create binding interface
       We have only one channel so we create only one method and we will be reading input as KTable 

4. Create listener service class 
       We receive input KTable with key as string and value also string 
       Next we apply filter() to remove everything else and keep only HDFCBANK records, then converting KTable to Kstream because we cant apply some methods on KTable like forEach(), to() (ie) we cannot send KTable back to kafka topic, we must convert KTable to KStream if u want to send it to Kafka topic

5. Start confluent

6. Create topic 
>kafka-topics --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic stock-tick-topic

7. Start kafka console producer and give input
>kafka-console-producer --topic stock-tick-topic --broker-list localhost:9092 --property parse.key=true --property key.separator=":"
HDFCBANK:2120
HDFCBANK:2150
HDFCBANK:2180

8. Start the application, we can see one HDFCBank output after 10000ms 
     Because the framework waited for 10sec and then only we got the incoming records 

9. Next we send TCS:2920 record in producer console, we got the output as
   key = TCS Value = null

The code will filter out the TCS record and accept only HDFCBANK record and return another KTable
    And filtering TCS record implies a delete operation because we have written code to remove TCS record from stream flow, so we are trying to delete TCS record which came in this flow. Hence filter operation will produce one output for null value. The key remain same (ie) TCS but value becomes null, we learned that KTable implements a delete operation using null value record, and that is why we are seeing null value record in output 

10. Refresh ur project, show state.dir options where location of local db

Computing Streaming Aggregates - Use case - Word count 
     We will create a Kafka topic and use command line producer to push some text to the topic. Consider we created a topic and send two messages, on other side we create kafka streams appl to consume data from the topic, it should break the text message into words and count the frequency of unique words and print outcome to the console. However it is a stream processing appl, we must continuously monitor the topic for new incoming messages and revise the outcome  
 
1. Create KTableAggregate with cloud stream, Spring for apache kafka, spring for apache kafka streams, lombok dependency   

2. Define the appl configuration in application.yml
    We created one input channel "streaming-words-topic", we provided binder configuration like broker coordinates, commit.interval,ms, state.dir. We will reading and writing string messages so setting default String Serde

3. Create binder interface for this input channel
         We have only one input channel, so we are creating one method. We are reading the topic as KStream so return type of binding is also KStream
 
4. Create listener service class
      We are reading a KStream and comes to input variable, so the first action is to extract words from the line of text by using flatMapValues(), this method will take the value which is line of text, then converting it to lowercase and splitting it using space character, now all those words are separated, and we will form list of words. The flatMapValues() will take the list and create a KStream of words 
     Kafka Streams also take aggregation as two step process
   1. Before we compute an aggregate, we must group our streams, however there is one limitation (ie) kafka streams can be grouped only on a key, so we must create an appropriate key for your grouping requirement
   However in our example, the message is null and message value is words, we want to group our data on words, so we need to copy the message value into the key using groupBy() which takes key value lambda and returns the grouping key, but here we want to groupBy by the value so we are returning only the value 
     Grouping is always performed by the key, so if ur streams already comes with the desired key, you can simply group ur KStream using groupByKey() which does not accept any argument and performs the grouping by the current key. Alternatively, we can use groupBy() when your stream does not have a key or you want to change the key for the grouping operation. Since our messages in this example are without the key so we use groupBy() method
    groupBy() method creates KGroupedStream, so the return type of groupBy() is KGroupedStream. You cant do anything on KGroupedStream except applying aggregation formula. In our example, formula is simply count(), so the count will compute the aggregates and result is always a KTable 
     Now we want to print it or we want to send the aggregated results to a Kafka topic, in both cases we must convert KTable to Kstream using toStream(), then using peek() we are printing the result 

5. Start confluent server
6. Create kafka topic
>kafka-topics --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic streaming-words-topic

7. Start kafka console producer and send some lines   
>kafka-console-producer --topic streaming-words-topic --broker-list localhost:9092
>Hello world
>Hello Kafka


8. Start the appl, and prints the count of words 
When we provide the some more input, we get output in 10sec because we provide commit interval 
    The count value will be revised, you will get latest aggregate value and ur appl keeps monitoring the input topics. As soon as we get new values, we will recompute the aggregation and revise the result 


Aggregation Concepts
     Computing aggregate is 2 step process (ie) group ur data by key and then apply aggregation formula
1. Group
2. Aggregate

Facts on Aggregation
1. Aggregation key
       The record key is the most critical element for an aggregation 
2. Aggregation applies to KStream or KTable
       The primary source of aggregation is either KStream or KTable, however the outcome of aggregation is always KTable. Aggregating a KStream is less complicated and straight, whereas aggregating KTable is often complicated and takes extra care because KTable is an update stream

3. Aggregation is only possible after grouping 
         Before you start applying aggregation formula, we must determine ur aggregation key and group ur data.

1. KStream -> groupBy(), groupByKey()->KGroupedStream
         We can group ur KStream using groupBy() or groupByKey() and returns KGroupedStream  

2. KTable -> groupBy() -> KGroupedTable
         We can group ur KTable using groupBy(), it does not support groupByKey() and returns KGroupedTable 

     Once we have ur data arranged in groups, we can apply aggregation formula on KGroupedStream as well as KGroupedTable. 3 method
    1. count() - count the number of records by the grouped key, records on KStream with a null key are ignored 
    2. reduce()
    3. aggregate()


Use Case - reduce()
     In this example, we are reading invoices from kafka topic, after filtering prime customers, we computed reward points earned by the customer on given invoice and sent a notification to real time loyalty topic. The output format of loyalty topic is 
     {
       "Invoicenumber":"xx",
       "CustomerCardNo":"xx",
       "TotalAmount":"xx",
       "EarnedLoyaltyPoints":"xx"
     }
We took one invoice, extracted invoice number, customer cardnumber, total invoice amount and computed loyalty points for total amount. Calculating and notifying customers with the loyalty points that he earned on most recent purchase is a good thing to do, still it would make more sense for the customers if we can include the past rewards and notify them of the total points earned so far, including the current rewards. So we want to inform the customer of his total reward points  including the new shopping
     So the notification record should include one more field for total loyalty points, this new field is the cumulative sum of the loyalty points earned so far, including all previous invoices.
     We can implement sum() operation for this field, however Kafka streams api does not offer sum() function, instead kafka streams api offers you only 2 generic formulas (ie) reduce and aggregate

1. Create KafkaReduce with lombok, cloud stream, spring for apache kafka, spring for apache kafka binder, confluent dependency

2. This appl will be reading invoices sent by the Avro invoice generator, so we need 3 input data model classes, we will be processing input invoice and generating notification messages to loyalty topic, so we need one output data model for notification 
    We read avro messages and also produce avro messages, so we add avro schema defination inside src/main/avro folder 
    We added 4 avro schema defination files because we need 4 data model classes 

3. >mvn clean install
        - It will generate the schema class files inside target - generated-sources folder 

4. Create business logic
       - We read PosInvoice object and convert it to Notification object. So we create RecordBuilder service where we create a method to implement business logic 
      The method will takes an invoice and returns a Notification, so we create new Notification object and setting all 5 fields for Notification. This method computes the earned loyalty points as 20% of invoice amount and also setting the same amount for totalLoyaltyPoints
    So in the beginning, the earned loyalty points and the total remains the same. As we get more invoices, we will aggregate the total and update this field

5. Define input output channels in application.yml file
      We are creating one input channel and one output channel, we also define broker configuration like broker address, schema registry url, commit interval, state store directory 
      The result of aggregation is always KTable, so every kafka streams aggregation project will internally create one or more KTables using local Rocks DB database. So we must configure commit.interval.ms, state.dir configuration for ur aggregation projects, if we dont then framework will take some default values 
       The aggregation is a complex internal process, so ur appl will be writing intermediate data to one or more KTable state stores. It might reread it from internal KTable, you may also need to use groupBy() and change the grouping key. In that case, your data will go back to an internally created kafka topic and come back again after repartitioning
       In fact, every local KTable is also backed up in some internally created kafka topics. This is necessary to achieve fault tolerance. If ur machine fails and you are restarting ur appl on a different machine, then the framework will recreate the local KTable from backup copy. Hence every local KTable also creates a backup copy in the Kafka cluster 
      Your appl will read/write data to kafka cluster and local Rocks DB storage. And all of these operations will go through the serialization and deserialization process, so we must specify Serde for each of those operations. So as best practice we must define default Serde for ur appl, if you are not defining a default Serde, then you will be forced to define a Serde at each step which leads to code problem and multiple serialization related defects

6. Creating binding interface 
         We will be reading KStream input and writing to KStream output 

7. Create Listener service class with 3 annotations, we also need RecordBuilder class so we autowire it 
        Next we create process() which receives KStream of invoice and returns a KStream of notification. We also need to define @StreamListener which brings input stream and @SendTo annotations which will push the Notification to Kafka topic
     First we call filter() which takes boolean key value lambda and returns true if we have a PRIME customer. 
      Next step is to apply map() transformation to convert PosInvoice to a Notification object. The map() method will take incoming key value and return a new key value and we are returning customerId as key and notification as value. Even we can use mapValues() instead of map() method, because we wanted to change the key. The mapValues() method is a key preseving equivalent of map() method, we should prefer key preseving api if we do not require changing the key. However in this example, my incoming key is store id, but we dont want to aggregate the values on the store id, we want to calculate sum of loyalty points by customerid, so we must change the key to customerid
    Next we do aggregation which is 2 step process, groupBy() and reduce(). Now we group based on customerid and since we already changed the message key to customerid, so we can use groupByKey() method  which is also key preserving api. This method will group the records based on existing one, so we avoid expensive repartitioning operation 
     Now last step is to use reduce() method and aggregate the grouped stream, which takes lambda of 2 arguments, first arg is aggregated value and 2nd arg is new value.
    So the framework will pass the aggregated value in the first argument, but for the first time we do not have any aggregated value 9ie) we are starting the appl for the first time, we recieved first invoice and transformed it into a Notification. It is new notification so it goes into second argument. The aggregated value for the first time is a blank notification object. So for the first time we will receive a blank aggregated value and a new notification.
    Now we will take new Notification value and set the total loyalty points 

How do we set?
   We take a sum of earnedLoyaltyPoints in this Notification amd totalLoyaltyPoints in the aggregated Notification, the aggregated totalLoyaltyPoints will be null for first time and finally return the newValue
     Now comes second iteration when we receive another input, this time reduce() will again receive the aggregated value and new value. newValue is new record and aggValue is the same record that we returned in previous iteration. We will again sum up the loyalty points from the earlier aggregated value and the newly received value and this keeps running
     Everytime we receive new input, we will create a new notification that comes out with the sum of the totalLoyaltyPoints. Then we convert it to stream, which you will have in this notificationStream
    The last step is to take this notificationKStream and use forEach() to log the output, finally we return the notificationKStream

8. Start confluent server

9. Start KafkaAvroInvoice and generate 60 invoices

10. Start the application and check the console log for data 


Use case - aggregate() method
       We want to create kafka topic as employee and send some messages to the topic. The messages are in JSON format but we want them to go as AVRO messages, we can do it using kafka-avro-console-producer tool. These 5 records are for employees with their name, department and salary. We want to create a streaming appl and compute the average salary for each department 
    We can compute the average salary by department using sql,  but how would we do it on an ever increasing stream in realtime 

1. Create KafkaAggregate project with lombok, cloud stream, spring for apache kafka and kafka binder dependency 

2. Create data model. 
       This appl will be reading employee records and producing department aggregates, so we need 2 classes for our data model. We will send input as AVRO, so we will create an AVRO schema for our data model 
    So we create 2 schema files like departmentaggregate.avsc which has total_salary, employee_count, avg_salary and employee.avsc which has id, name, department, salary 
    So we will create sum of the salary and keep it in total_salary, similarly we will count the number of employees and keep it in employee_count. Once we have total_salary and employee_count, we calculate average salary by dividing both fields

3. >mvn clean install 
       - We create model class 

4. Create input and output channels in application.yml file
     Here we create one input channel only, we do not have an output channel because we do not want to send the output to a kafka topic, we want to print the output to the console, so an output channel is not required and finally we are setting default serde of confluent 

5. Create binding interface, we create one method for reading the input stream from the input channel

6. Create listener class with 3 annotation like @Service, @Log4j2, @EnableBinding. We also need RecordBuilder so we autowire it 
    Now we create process() method which takes KStream of Employee as input and returns nothing, we also need to annotate this method with @StreamListener.
    We want to see the input key and values before we start doing anything so we use peek() method. The peek() method will apply the lambda and return the same stream without any changes.
    Now we want to use map() method for converting employee record to a DepartmentAggregate record, then we can use groupBy() and group the stream by department id, finally we can reduce it to compute the aggregation which we did in previous example
     However in this example we want to use aggregate() method which is similar to reduce() method but it is combination of a map() and reduce(). Normally we can do aggregation in 3 steps as we did in previous example using map(), groupBy() and reduce()
     But now we do in 2 steps (ie) groupBy() and aggregate(), because aggregate() method is combination of map() and reduce(), so if we use aggregate() we can avoid using map() method. 
   Now we chain groupBy() method which takes a key value lambda and returns the new grouping key, we want to group this stream on the department id, so we are returning the department id. Next step is to use aggregate() method which takes 2 arguments initializer lambda and aggregator lambda
    The initializer lambda does not take any argument, and it should return the initial state of the aggregated method (ie) we want to return DepartmentAggregate record with initial values set to 0. 
   So we define initializer method in RecordBuilder class, where we create new DepartmentAggregate, setting all the values to zero and returning it. So initializer lambda will create a blank DepartmentAggregate. Now the aggregator lambda takes 3 argument (ie) key, value and the aggregated value. Here key is department id which we changed it using groupBy() method 
    So when new record comes for the first time, the framework will call the aggregator lambda and pass the department id and employee record to the first 2 arguments (ie) key is department id and value is an employee record. Now the last argument is DepartmentAggregate 
    Now we create aggregate() inside RecordBuilder class, where we create new DepartmentAggregate, incrementing employee count and then we are adding the current employee salary to the total_salary, then calculating the average and finally return DepartmentAggregate.
    In aggregate() method, first we create an initial blank aggregate, in the next step we will get old aggregate and the new record, then we add the new record values to the old aggregate and return the newly updated aggregates. So in next iteration, we will again get the updated aggregate and the new value, so we will repeat the step and update the aggregate once again, and this goes in a loop for each new record. Finally we change to toStream() and apply the foreach method to print it 

7. Create topic

kafka-topics --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic employees-topic

8. Start the appl

9.Start kafka console producer and send messages 
{"id": "101", "name": "Prashant", "department": "engineering", "salary": 5000}
{"id": "102", "name": "John", "department": "accounts", "salary": 8000}
{"id": "103", "name": "Abdul", "department": "engineering", "salary": 3000}
{"id": "104", "name": "Melinda", "department": "support", "salary": 7000}
{"id": "105", "name": "Jimmy", "department": "support", "salary": 6000}
{"id": "101", "name": "Prashant", "department": "support", "salary": 5000}
{"id": "104", "name": "Melinda", "department": "engineering", "salary": 7000}


4. Create business logic in RecordBuilder class with @Service annotation
    Now we want to transform employee record to a DepartmentAggregate record, so we create a method that takes an employee and returns the DepartmentAggregate 

Kafka Time Semantics
     Previously we compute aggregates on KStream and KTable. The methods we learned are good to compute aggregates on infinite stream of data. However we often need to create smaller chunks or buckets of data and calculate aggregates on the bucket only, such buckets are more often termed as window

Windowing Aggregates
     For example, we want to compute sales over the past hour, in this case we have to bucket ur data into an hourly window and calculate aggregates on each window. Time is most commonly used attribute for windowing 
     For example, we might want to compute sales by a minute, page views per second, or ad clicks per day. However kafka offers the following two types of stream windows 
      1. Time window
      2. Session window

First we understand how Kafka understands time?
         Apache kafka allows you to work with the following timestamps
    1. Event time - It is the exact time when the event was generated at the source system 
         For example, an invoice is made at POS terminal, in this case the event time can be stamped by the POS system and the time becomes integral part of the invoice. Most of the time, we represent the event time using a field in the event object itself. For example our POS simulator generates an invoices with field createdTime in the schema
    However in many cases ur event generating system may not already have a timestamp field and not feasible to modify the appl to add timestamp field. In some of those cases, we can use timestamp metadata in message record. If you are using kafka producer to send data to the kafka cluster, the API always includes a timestamp in the record metadata, we can safely use the timestamp from the metadata as ur event time

     2. Ingestion time
               You many have a business case to ignore event time, but consider the time when the event reached your kafka broker. Ingestion time is the time when the event reaches ur system 
         Obviuosly we cant explicitly set ingestion time, it must be assigned by the kafk abroker automatically. You can achieve ingestion timestamp by setting 
   message.timestamp.type=LogAppendTime configuration at topic level
     When the topic is configured to use LogAppendTime, the timestamp in the message record metadata will be overwritten by the broker using the broker system time (ie) we can either create a timestamp metadata using the producer time or broker time, but we cant have both

3. Processing time
      It is when you are processing the event, and it is simplest thing to implement. We can get the processing time as the current wall clock timestamp from ur local machine and use it for ur time based windowing 

So when you are creating an appl, we must configure ur appl to use one of the 2 timestamps, so we can do it by creating a TimestampExtractor and configure it for your input channel. Kafka provides some builtin timestampextractors like
   1. If we want to use processing time we can configure WallclockTimestampExtractor for ur input channel 
    If we want to use Ingestion time we can configure remanining 3 timestamp extractors 
   2. FailOnInvalidTimestamp
   3. LogAndSkipOnInvalidTimestamp
   4. UsePreviousTimeOnInvalidTimestamp

However if we want to use Event time we must create a custom timestamp extractor and use it 
      
Windowing Aggregates - Use case
     We want to compute the number of invoices generated by each store in a 5min window. 
     First create a kafka topic, send the following 3 invoices to kafka topic. Each invoice comes as key value pair with storeID being a key and invoice itself as a JSON serialized messages. The event time of the invoice is recorded in the createdTime field of JSON message 
     Event time is normalized to UTC Epoch milliseconds, this format of representing time is not human readable . However most real time appl will follow a time format normalized to a standard time zone 
     We want to develop a stream processing appl that uses event time semantics to compute the number of invoices by storeID in a 5min window. 
    For 3 invoices sent earlier, you can expect the output as, we have 2 stores so you will get 2 records in the output. The first store created 2 invoices in the first 5 min, so we will see 2 records for the first store, the second store has got only 1 record 
     Now we can send some more messages, after sending the new messages the outcome should update to the following like STR1534 produced one more invoice, but this invoice was produced in the next 5min slot, so this new window will count one record 
    Your appl should group the record by the storeid, then for reach store you will again subgroup the records in 1 5min window and count the number of invoices for each window 

1. Create KafkaWindow with lombok, cloud stream, spring for apache kafka, kafka binder dependency 

2. Create data model class with 4 fields and all are annotated to become a JSON friendly class 

3. Define configuration in application.yml file
         We define one input channel and binder configuration 
         We defined a consumer timestampExtractor bean name for our input channel, so we already discussed that we must create and configure a timestamp extractor for each channel, if we are doing a time based operation. So the timestamp extractor will extract the time from the message record and define the time window

3. Create timestamp extractor class which implements TimestampExtractor interface and override extract()
    Everytime when u receive a message record from kafka topic, the framework will call this extract() and give you the message record. Now we can extract the timestamp from this message record and return it 
     So we are reading value from consumerRecord which is simpleinvoice, then we get createdtime from simpleinvoice and return it. The extract() also provides us the time of the previous record, so if this records created time is not valid, we will return the time of previous record 
    So the purpose of the timestampextractor is to provide the time for each record, we can take time from record itself like we are taking createdTime or we can take system time or we can take it from record metadata using ConsumerRecord methods   
        We also need to create spring bean so the framework can create an instance of this class at runtime so we annotate with @Configuration. So we can create @Bean method called invoiceTimeExtractor which just creates an instance of TimestampExtractor 

4. Now we copy bean method name and paste in application.yml file 
    The spring framework will trigger this bean method to create a timestamp extractor and use it to extract a timestamp for each event 

5. Create binding interface, we have one input channel so we define one method

6. Create Listener service class 
      We are starting with input stream and applying the peek() method, because we want to print the input message key and the created time to the log
    Createedtime is not human readable format so we are converting it to a formatted string, next we are grouping input records by the storeid since records are already coming with storeid key so we use groupByKey() since it is key preserving api. We also want to subgroup our record by 5min window using windowBy() method, we saw kafka offers 2 types of window like Timewindow and Session window. Now we want to create 5min time window, so we are configuring window as TimeWindows of 5min
     Next we count the record using count(), since all aggregation result in KTable now we are converting KTable to KStream using toStream() and printing the output. Now my records were grouped twice, first grouping key is storeid and the second grouping key is the timewindow, we can access storeid using key() and time window using window()

7. Start kafka server

8. Create topic
>kafka-topics --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic simple-invoice-topic

9. Start the appl

10. Start console producer and give input
>kafka-console-producer --broker-list localhost:9092 --topic simple-invoice-topic --property parse.key=true --property key.separator=":"

STR1534:{"InvoiceNumber": 101,"CreatedTime": "1549360860000","StoreID": "STR1534", "TotalAmount": 1920}
STR1535:{"InvoiceNumber": 102,"CreatedTime": "1549360900000","StoreID": "STR1535", "TotalAmount": 1860}
STR1534:{"InvoiceNumber": 103,"CreatedTime": "1549360999000","StoreID": "STR1534", "TotalAmount": 2400}

STR1536:{"InvoiceNumber": 104,"CreatedTime": "1549361160000","StoreID": "STR1536", "TotalAmount": 8936}
STR1534:{"InvoiceNumber": 105,"CreatedTime": "1549361270000","StoreID": "STR1534", "TotalAmount": 6375}
STR1536:{"InvoiceNumber": 106,"CreatedTime": "1549361370000","StoreID": "STR1536", "TotalAmount": 9365}

Tumbling window vs Hopping time window 
     Previous project is an example of tumbling window. Tumbling windows are fixed size, non overlapping and gap less window
     You can define a tumbling window using the window size as we did in the TimeWindows.of() method. Since tumbling window never overlap, a data record will belong to one and only one window 
      The color represents the key, so we received 3 green records, 1 blue records and 2 orange records. The first 2 green records arrived between 10 and 10:05, we wanted 5min window so the framework will create one window from 10 to 10:05 and count the green records as 2. Similarly we received one blue record in the same window, so the framework will show u the blue record count as 1
     The next green record arrived after 10:05, so the thrid green record will only impact the count for the next window. The next window time starts at 10:05 and ends at 10:10. Since Tumbling windows are fixed size, non overlapping and gap less window, so every record exactly falls in a single time window and gets aggregated

You may ask What is the first window time? 
      Why it is 10am in this example, why do we have windows starting from 10:00 and ending at 10:05?. Cant we configure it to start the window from 10:01 and end at 10:06?
     No we cant configure the window start  time, the framework will automatically determine the window start time based on ur first event time. In this example, my first message had an event time as of Feb 5 2019, 10:00AM, so framework started the window at 10:00AM. The framework does not allow you to set the window start time, the framework will always make a guess and set the window start time using ur streams first event. Once the first window start time is set,then all other windows will follow the fixed interval from that first window 

Hopping window/Slidding window
    Hopping windows are also fixed size but overlapping windows and no gaps.
    Hopping windows are defined by 2 properties (ie) window size and advance interval

Sample code for defining Hopping window:
     input.groupByKey()
          .windowedBy(TimeWindows.of(Duration.ofMinutes(5))
          .advanceBy(Duration.ofMinutes(1)))
          .count()

The advance interval specifies how much a window moves forward relative to the previous one. So here we have configured hopping window of 5min and advance interval of 1min
    In this figure, we received 3 messages, the first window starts at 9:57 and ends at 10:02. If we created a tumbling window then we should have only 3 windows, the count for each window will be one event. However hopping window will generate too many windows for the same duration as shown in figure
     The first window starts at 9:57 and ends at 10:02, the next window starts after 1min, so the next window start time is 9:58 and ends in 5min.  Similarly we will see series of windows each starting at a 1min interval, since we define 1min of advance interval
    Hopping windows are used to compute moving aggregates for a fixed size window. For example, we want to use hopping windows to calculate page views in the most recent 5min reported event 1 minute 
    Kafka offers 2 types of time based windowing, the method for creating both type of windowing is same, only difference is in tumbling window we wont set advance interval


Session based Windowing 

What is Session?
    Session are a period of activity followed by a defined gap of inactivity, for that reasons, session windows are also represented by a period of activity rather than being a fixed size window 

Use case
    We have an website and we added a feature to ur website that generates user click events and sends them back to ur server, ur click events are JSON message. We planned to stream those events to ur kafka cluster 
     We wanted all the events for the same user to flow to a designated partition, and hence we implement userID as a message key. Such a continuous flow of click events is known as clickstream 
     All the events are for Feb 05,2019, which we have normalized to minutes for simplicity. For the given sample clickstream, we want to count the number of clicks per user session

1. Create KafkaSessionWindow with lombok, cloud stream, spring for apache kafka, kafka binder dependency

2. Create data model class which models user click events

3. Created a timestamp extractor and configured it for input channel 
     We are reading event time from the click event itself, so we are implementing event time concept 

4. Configure properties in application.yml

5. Create Binding interface

6. Create listener service class with necessary annotation and process()
      The method reads all the user clicks and we process them. So we want to group the click events by the user id, then we will create a session window of 5min and count the records 
      We are starting with peek() method which will the incoming records, then we will group it by the current key. Next step is to apply windowing method using SessionWindow of 5min. Next we will count it, convert it to a stream and print the results 
    Earlier we created a time window but now we are using session window, however that small change makes a big difference. Time windows are fixed in size, however the session window is variable in length and the size of the window is dependent on user activity 
     Assume you have 2 active users, the first user is active and she keeps frequently clicking for 30min, so the session for first user is 30 min. We have another user who leaves ur website in 10mins, so his session is 10min session 

7. Start kafka server

8. Create topic
>kafka-topics --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic user-clicks-topic

9. Start the appl
     First we can see User101 clicked at 10:01. So kafka will start session window for user101, the session start time is equal to event time, so start time is 10:01 and end time is also 10:01.

why?
   Because kafka assumes that the session is over,if the user clicks again in less than 5min we will extend the session, if not it is already ended 

   Now we send next record in producer for user102, so new user session started at 10:02 and ended at the same time 
   Now we send next record for again user101, user101 clicked once again in less than 5min, this time user clicked at 10:04, so the framework will take the earlier session and extend the session end time to 10:04. So we can see session start time is 10:01 but end time is 10:04
   Now user101 can again click before 10:09 to keep session alive and same session is extended because we created 5min session window, 5min is idle time and not session time. So when u define 5min session, that means u defined a 5min of idle time 
    You may see some null value records, when kafka extends an existing session, it will delete the old record and create a new record with an extended time

10. Start the producer and send the first event 
>kafka-console-producer --broker-list localhost:9092 --topic user-clicks-topic --property parse.key=true --property key.separator=":"       
USR101:{"UserID": "USR101","CreatedTime": "1549360860000","CurrentLink": "NULL", "NextLink": "Home"}
USR102:{"UserID": "USR102","CreatedTime": "1549360920000","CurrentLink": "NULL", "NextLink": "Home"}
USR101:{"UserID": "USR101","CreatedTime": "1549361040000","CurrentLink": "Home", "NextLink": "Books"}
USR101:{"UserID": "USR101","CreatedTime": "1549361400000","CurrentLink": "Books", "NextLink": "Kafka"}

Kafka Stream Joins
      Joins are one of the most common and desirable features of any data system. We might be working with a batch system or a real time system, in both cases at some point we would need to join two data sets 
      In real time stream processing framework offered by Apache Kafka, ur datasets are abstracted into 2 categories like KStream, KTable or GlobalKTable. Hence we would expect the capability to join a combination of these 2 types of datasets 

Join op       Result     Join type           Feature
1. kstream-   Kstream    Inner,left,outer   windowed,
   kstream               swapping for right  key based

2. ktable-    KTable     Inner,left,outer,  non-window
   ktable                swapping from right  key base

3. KStream-   KStream    Inner,left,outer   non-window
   KTable                                   key based

4. KStream-   KStream    Inner,left         non-window
   GlobalKTabke                         key, non-key                                              based

Precondition for joining two datasets in Kafka
1. Joins in Kafka are performed over record keys, hence KStream or KTable must have a valid key 
2. The join input topics must have the same number of partitions 
3. Data in topics must be co-partitioned (ie) all appl that write to the input topics must have the same partitioning strategy, so that records with the same key are delivered to the same partition number 
    If the inputs of a join are not co-partitioned yet, you must ensure this manually by rewriting the data into a new topic with same number of topics and uses the same partitioner
4. Co-partitioning is not mandatory for KStream-GlobalKtable joins, because all partitions of the GlobalKTable's are made available to each instance (ie) each instance has a full copy of the GlobalKTable stream
5. Non key based joins are allowed with KStream-GlobalKTable joins, all other joins are only key based joins 
6. If there is a stream in the join, such as KStream-KTable and KStream-GlobalKTable, the stream must be on the joins left side
    In other words, swapping the orders as KTable-KStream and GlobalKTable-KStream is not allowed 
7. The join operation on two records willproduce zero, one or two new records depending upon the type of join operation 

Kafka streams offer you the following join types
1. Inner join - will produce one record when there is a matching record on both sides. This type of join doe not produce a record when the keys are not matching. Inner joins are supported for all 4 types of join operations 
 
2. Left outer join - will produce a record from the left table and matching records from the right table. This type of join always produce a record from the left side even if there is no matching record on the right side. It is also supported for all 4 join operations

3. Right outer join - It is opposite to left outer join, that will always produce a record from the right side even if there is no matching record on left side 
    Kafka does not have a method for implementing right outer join, however we can perform right outer join by swapping left and right side datasets. This swapping is only allowed with kstream-kstream and ktable-ktable joins

4. Full outer join results in a single record on the matching key. If there is no matching record, a full outer join will produce 2 records (ie) one from both sides. Outer joins are not available with GlobalKTable 

In summary, inner joins and left outer joins are fully supported. Right outer joins are available through swapping objects when both sides are of the same type. Full outer joins are fully supported for all exceptt GlobalKTable 

KStream to KStream joins
      KStream is an infinte, unbounded stream of records, so if u try to join 2 endless streams, sooner or later, we will end up consuming all ur resources and crash your system. Hence KStream-KStream joins are always windowed joins and non windowed joins are not permitted 
     When joining two streams, you must specify a boundary condition using a JoinWindow 

Use Case
   We want to pay bills for your internet provider called inet which gives you an option topay using your mobile wallet called mWallet. We log into iNet appl and select the mWallet option to initiate the bill payment. The iNet would ask for your linked mobile number, once you supply ur cell number, the iNet would call the mWallet payment API 
      The mWallet payment API implement Apache Kafka based system and for each payment request, it performs the following options
   - On receiving requests from iNet, the mWallet initiates a transaction and sends the transaction event to the Kafka cluster. The mWallet will also send the transaction OTP to you, this OTP is valid for 5min. Finally, the mWallet would respond back to iNet with a transactio ID
     Now, the iNet would wait for you to enter the OTP. When you supply the OTP, the iNet again calls the mWallet with the transactionID and OTP.
      The second call would generate a new event and comes to mWallet and is pushed to Kafka. Now we need to write a streams appl that joins these two streams and validates the OTP. For the sake of simplicity, we can print the validation outcome on the console. Since the OTP is valid only for 5min, this reqquirement makes an excellent use case for stream stream join using 5min join window 

1. Create Kafka-KStream-KStreamJoin with lombok,spring for apache kafka, kafak binder dependency

2. Configure properties in application.yml

3. We need 2 data model classes, PaymentConfirmation, PaymentRequest and TransactionStatus
      TransactionStatus have 2 fields like transactionId and status. The status will be success or failure for the given transactionId 

4. Both of input messages should go into 2 different topics, we got 2 input channels in yaml file 
    Stream to Stream join is always windowed and windowing requires you to have event time. So we need timestamp extractor for both the input channels 

Create PaymentRequestTimeExtractor which implements TimestampExtractor and override extract()   

4. Create business logic, so we create RecordBuilder class and add one method for the business logic 
     We create getTransactionStatus() which takes PaymentRequest and PaymentConfirmation, then compare OTP and return success it they match 

5. Create listener service class with @Service, @Log4j2, @EnableBinding and autowire RecordBuilder class and also create process() annotated with @StreamListener annotation
       process() should receive 2 inputs, so we do not define the input channel using stream listener annotation, instead we will define @Input annotations to the process() arguments, the first argument will read from payment-request-channel and second argument will be reading from payment-confirmation-channel 
     First we read and print the transactionId and the createdTime, similarly print the confirmation record also. These 2 log entries will help us to read the input values and understand the application behavior.
     We created both the topics with the same number of partitions, both types of messages come with transactionID as a key, and they use the same default partitioner. Hence we already satisfy the co-partitioning requirment for the join, and there is no need to repartition the data
    Now we perform the join, we will satrt the request and call join() which takes 3 mandatory arguments. The first argument is obviously the other stream that we want to join, the second argument is a ValueJoiner lambda of 2 arguments, the first argument is a record from the left side (ie) PaymentRequest, the second lambda argument is a record from right side (ie) the PaymentConfirmation. The lambda method is triggered by framework only if the matching record is found. The body  of the lambda method is the place where you would create an output record 
    The join logic in straight, and it is implemented in RecordBuilder getTransactionStatus() which we will call. The getTransactionStatus will return a TransactionStatus record and we will return it back 
    The 3rd argument is the JoinWindow that we use to define the time constraint, we are setting 5min window for the join operation (ie) if the payment request and payment confirmation records arrive within a 5min window, the ValueJoiner is triggered and we will get a transaction status record with success or failure. The success and failure depend upon the OTP matching and the record is produced only if the ValueJoiner is triggered (ie) if we receive both the records within a 5min window we will get a transaction status record. 
    If anyone side of the record is missing or they are not falling within 5min window, ValueJoiner is not triggered and we wont see a transaction status record 
    The last argument is to define the required Serdes, we have defined default Serdes in application yml file. However the join operation fails to infer the correct Serdes, so we defined it explicitly. The first Serde is for the common key among the two events, second and third Serdes are for the two events. Finally we will print the transaction status record 

6. Start kafka server
7. Create topics
>kafka-topics --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic payment_request

>kafka-topics --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic payment_confirmation

8. Start both kafka producer
>kafka-console-producer --broker-list localhost:9092 --topic payment_request \
--property parse.key=true --property key.separator=":"

>kafka-console-producer --broker-list localhost:9092 --topic payment_confirmation --property parse.key=true --property key.separator=":"

9. Start the appl

10. Now send some payment request in payment-request producer
100001:{"TransactionID": "100001", "CreatedTime": 1550149860000, "SourceAccountID": "131100", "TargetAccountID": "151837", "Amount": 3000, "OTP": 852960}
100002:{"TransactionID": "100002", "CreatedTime": 1550149920000, "SourceAccountID": "131200", "TargetAccountID": "151837", "Amount": 2000, "OTP": 931749}
100003:{"TransactionID": "100003", "CreatedTime": 1550149980000, "SourceAccountID": "131300", "TargetAccountID": "151837", "Amount": 5000, "OTP": 591296}
100004:{"TransactionID": "100004", "CreatedTime": 1550150100000, "SourceAccountID": "131400", "TargetAccountID": "151837", "Amount": 1000, "OTP": 283084}

Now in the appl we can see all the messages are time stamped for Feb 14,2019 with event time as 13:11, 13:12, 13:13 and 13:15. After sending these messages the appl does not show any outcome in the beginning because there are no matching confirmation records on the other topic yet.

11. Now send some confirmation message in payment-confirmation topic
100001:{"TransactionID": "100001", "CreatedTime": 1550150100000, "OTP": 852960}
100002:{"TransactionID": "100002", "CreatedTime": 1550150280000, "OTP": 931749}
100004:{"TransactionID": "100004", "CreatedTime": 1550150040000, "OTP": 283086}

All the confirmation messages are also timestamped for Feb 14,2019 with event timestamp as 13:15, 13:18 and 13:14
    We received 4 payment request, but on;y 2 of them appeared in the outcome. The request 100001 was sent at 13:11 and confirmed at 13:15 with a matching OTP, hence it comes as success. The request 10002 was sent at 13:12 but confirmed outside 5min window at 13:18, hence ValueJoiner for that pair didnt trigger, and status does not appear in the outcome
    The request 10003 was not confirmed, so that one also dosent appear in the outcome. Finally the request 10004 was generated at 13:15, but somehow it was confirmed earlier at 13:14. This record appears in the result because both the records fall in the 5min window with a matching key, however the status is failed because their OTP didnt match 

KTable to KTable join
     These joins are non windowed joins and they offer the same result as in the case of standard database table joins. The join result is another KTable. It is most straightforward join and simple to implement

Use case
     We are running a bank which has vast user database. We have streamed all your customer records to ur kafka cluster. All the user data is now available for any real time operation. The sample records are 
  100001:{"UserName": "Prashant", "LoginID": "100001", "LastLogin": 1550150109302}
    Every time a new user registers, her details are also streamed to the system. Similarly whenever a user successfully logs in to your application, you send an event to your kafka cluster 
    We want to create a simple appl that updates the last login timestamp, whenever the user logs into the system 

1. Create Kafka-KTable-KTableJoin with lombok,spring for apache kafka, kafak binder dependency

2. Define all configuration in application.yml

3. Create 2 model classes 
     UserDetails which has 3 fields and annotated with JSON properties and UserLogin with 2 fields loginid and createdTime

4. Create Binding interface, we are reading 2 input topics and we are reading them as KTable, both inputs are coming with a valid key so we can directly read them as KTable

5. Create listener service class with 3 annotations and process()
    We are reading both input channels, first arg reads user details and second one reads the login events. Both of them coming as KTable and we need to join these 2 KTables and update the users last login details 
    The first 2 lines are to print the input values. Next we start with left KTable, apply  join on the right KTable, logins KTable is left KTable and users are the right KTable 
     Second argument of join() is ValueJoiner lambda, here the first lambda argument is left record and second lambda argument is the right record, the lambda body implements join operation. The result of join() is KTable, so we are converting it to KStream and printing the results
    The data in my users topic is keyed by login id, on other side data in userlogin topic is also keyed by login id. Having the same key for both the topics will ensure that all the data from both the topics for a specific user will flow to same stream thread  
    So with this join operation we are updating users last login time with the current login time. 

6. Start confluent server

7. Create topics
>kafka-topics --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic user-master

>kafka-topics --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic user-login

8. Start the appl

9. Start kafka producer and give some sample input
>kafka-console-producer --broker-list localhost:9092 --topic user-master --property parse.key=true --property key.separator=":"
100001:{"UserName": "Prashant", "LoginID": "100001", "LastLogin": 1550150109302}
100009:{"UserName": "Alisha", "LoginID": "100009", "LastLogin": 1550150280409}
100087:{"UserName": "Abdul", "LoginID": "100087", "LastLogin": 1550150290305}

10. Start kafka producer for user logins with sample output
>kafka-console-producer --broker-list localhost:9092 --topic user-login --property parse.key=true --property key.separator=":"

100001:{"LoginID": "100001", "CreatedTime": 1550150291000}
100087:{"LoginID": "100087", "CreatedTime": 1550150580000}

KStream to KTable join
     This join is also non windowed. A typical use case of such join is to implement lookups and stream enrichment 
     For example, if you have a KStream of user activities and you are looking to enrich this stream with the latest user profile information from a KTable. So we want to implement KStream to KTable join, a similar use case is often implemented using KStream to GlobalKTable join 

GlobalKTable
    We learned about KTables, however KTables are local (ie) each streams task would have its own local copy of the KTable and a local copy of state store where the ktable data is persisted. These local tables are great because they allow you to work on a portion of data in parallel. 
    In this scenario each stream task can work independently without relying on other tasks. However in some cases, we need a global table
    Global table is something available to all stream threads, it is a common dataset which anyone can read. Any changes to this global table should be available to all stream tasks, even run on different machines. Kafka streams offers this capability as GlobalKTable 
    Like KTable, a GlobalKTable is also an update stream, where each data record represents an update or insert. However there is one fundamental difference between these 2 structures (ie) a standard KTable is local in nature, whereas GlobalKTable is global in nature 

      Assume we have a topic T5 with 5 partitions and we want to read this topic into a table. We already know that partitions are the main idea behind the parallel processing in Kafka. We can run 5 instances of your application on topic T5 to achieve a maximum degree of parallelism.
      Assume we started 5 instances of the same application, all of them subscribed to T5 and read the data into a table. In this scenario, each instance of the appl will be assigned one partition. The local KTable will be able to process data only from one assigned partition. Each instance is processing one partition, and all 5 instances together process all the data 
      Suppose we read the topic into GlobalKTable, in that case each instance of the application will be assigned all 5 partitions. The globalKTable at each instance will read data from all the partitions and hence all of them will possess all the data. This scenario is problematic for parallel processing because it causes duplicate processing, however GlobalKTable makes perfect sense for broadcast and lookup tables
     GlobalKTable ismainly used in star-joinsm foreign key lookups and broadcast information, but we should be careful as GlobalKTable require local storage on each application instance. They also increase the network traffic and broker workload because all instances read the entire data. GlobalKtable is excellent for a small set of info that we want to be available to all ur instances

Use case
    Since usecase for KStream to KTable join and KStream to GlobalKTable joins are same, now we implement for GlobalKtable, implementing KStream to KTable join is same as we are going to do it for KStream to GlobalKTable joins  
    We are popular news website and we directly sell ad campaigns to ur customers. We have created predefined locations across ur websites where we can place ads at runtime. These advert placement locations are often known as advert inventories 
     Advert inventories are tagged with the bunch  of information like 
     1. Age - how old is the news around the inventory
     2. Type of content - whether the story around them is business news or sports news 
     3. Placement location - left side, right, top, bottom and in between the content 

It is common requirement to measure the performance of such inventories in terms of impressions and clicks. These mesaurements are critical to adjust the campaign in real time/ We want to create akafka stream appl to compute the ad clicks by news type  (ie) we want to know the number of ads clicked for sports news, business news, political news etc 
    So we model the inventory using a JSON message as shown and publish them to ur Kafka cluster. Next we bring all ur ad clicks as a real time streaming event into ur kafka cluster. Once we have these two topics and data starts flowing into them, we want to create a real time appl, to count the ad clicks by the news type. Based on sample data given we can except the outcome 


1. Create Kafka-GlobalKTableJoin with lombok,spring for apache kafka, kafak binder dependency

2. Define all configuration in application.yml

3. Create binder interface
        We are reading inventorychannel into a GlobalKTable and clicks-channel is a standard KStream

4. Create data model classes like AdInventories with inventoryId and newsType, similarly we have AdClick with one field inventoryId 
       So when inventory is clicked, we pass inventoryId, now we join these 2 events and count the clicks

5. Create listener service class with 3 annotations and process() method
    We are reading all advert inventories to GlobalKTable, so all the inventories would be available to all the stream threads, because we are reading it into a GlobalKTable. We also reading all advert clicks into a standard KStream. If u r running multiple threads of the same appl, we will have a full list of imventories at all the threads and a partial list of click events and join them easily at each stream thread, because all the records for the other side of join are available with each stream thread 
    We are starting with click stream and apply join() with the global table of inventories. The purpose of join is to enrich the click event with news type information. The click stream has got inventoryId, but we want to pull the news type information based on the id, so we join the clicks with the inventories, which is join method first argument
     In all cases, second argument is ValueJoiner lambda but GlobalKTable has got one additional capability. A join with GlobalKTable takes a key value mapper lambda and returns a new join key which could be different key than the current KStream key. This feature is to join GlobalKTable with a foreign key, but we dont need this feature in our requirement so we return the same key 
     Next argument is ValueJoiner lambda, the first argument for valueJoiner lambda is ad click event and second argument is inventory record. The valueJoiner is triggered if and only if both the record keys are matching. So we know inventory is clicked, so we have to return inventory record 
     The outcome of join is new KStream where key is inventoryid string and value is inventory record and this new joined KStream represents the clicked inventories 
    Now we have stream of inventories that are clicked, now we want to group them on the news type and count using groupBy() which takes key value mapper lambda and returns a grouping key, in our case we want to group it on the news type with Serdes setting
    Finally we count the grouped stream and print the final stream so we can see the output 

6. Start confluent server

7. Create topic
>kafka-topics --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic active-inventories

>kafka-topics --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic ad-clicks

8. Start kafka producer with sample inventories
>kafka-console-producer --broker-list localhost:9092 --topic active-inventories --property parse.key=true --property key.separator=":"
>1001:{"InventoryID": "1001", "NewsType": "Sports"}
>1002:{"InventoryID": "1002", "NewsType": "Politics"}
>1003:{"InventoryID": "1003", "NewsType": "LocalNews"}
>1004:{"InventoryID": "1004", "NewsType": "WorldNews"}
>1005:{"InventoryID": "1005", "NewsType": "Health"}
>1006:{"InventoryID": "1006", "NewsType": "Lifestyle"}
>1007:{"InventoryID": "1007", "NewsType": "Literature"}
>1008:{"InventoryID": "1008", "NewsType": "Education"}
>1009:{"InventoryID": "1009", "NewsType": "Social"}
>1010:{"InventoryID": "1010", "NewsType": "Business"}

9. Start kafka producer with sample click events
C:\Softwares\confluent-windows-5.0.1\bin\windows>kafka-console-producer --broker-list localhost:9092 --topic ad-clicks --property parse.key=true --property key.separator=":"
>1001:{"InventoryID": "1001"}
>1002:{"InventoryID": "1002"}
>1003:{"InventoryID": "1003"}
>1004:{"InventoryID": "1004"}
>1004:{"InventoryID": "1004"}
>1005:{"InventoryID": "1005"}
>1006:{"InventoryID": "1006"}
>1007:{"InventoryID": "1007"}
>1008:{"InventoryID": "1008"}
>1009:{"InventoryID": "1009"}
>1010:{"InventoryID": "1010"}

10. Start the application

Complex Aggregation
    Previously we created an example to implement join and simplest form of aggregation using count(). 
    In previous example we computed the clicks by news type, now we want to extend the appl further to see the top 3 news types receiving the maximun number of clicks 

1. Create Kafka-ComplexAggregation with lombok,spring for apache kafka, kafak binder dependency

2. Copy application,yml, data model and binding interface from above project, because its going to be same

3. Previous listener service example, we are computing the count, converting it to a Kstream and printing it. In this example we have removed the code for converting the count KTable to Kstream and foreach loop to print the records 
     We created intermediate KTable variable called "clicksByNewsTypeKTable", this intermediate table is starting point of our current example, everything is going to be same.
    Now we got Ktable for clicks by newstype and now we want to determine the top 3 from this KTable. So first sort the KTable by the number of clicks and take the top 3 from the sorted KTable.
     KTable does not provide any api for sorting, because of 2 reasons
      1. KTable is local for the stream thread, so ur data is distributed across the stream threads so for sorting we must bring it to a single thread 
      2. Sorting on the record key does not make any sense in many use cases. For example, we do not want to sort the record on the news type in our case instead we want to sort on the number of clicks and find the top 3. So most of use cases would be sorting on some other field and not on the record key 

What is the solution?
    We have a KTable with data for newstype and its number of clicks, and this data is not available in one place, it is distributed across different stream threads. Because KTable is local to the thread and the data is distributed across threads 
    Each stream thread would be maintaining its local share of the data, now we want to sort it by value. So the first thing that we want to do is to bring it to one place, otherwise it is impossible to sort it
    So we are going to groupBy this data using a fixed key, so we are implementing a group on this KTable and changing the key to "ABC". The result will be a KGroupedTable and all these records will come to a single partition because they all have the same key. However we are lost with news type information and left with click count only 
    In the groupBy call, while we change the key to "ABC", we also change the value to preserve the newstype and the count. So the KGroupedTable should be look like this given in ppt

3. Create new data model called "ClicksByNewsType" to hold news type and the clicks with 2 fields called newsType and clicks 

4. Now we go to listener service and implement the groupBy
    Now we start with final KTable called clicksByNewsTypeKTable with groupBy() which takes key value mapper lambda, inside the lambda body we create a new instance for ClicksByNewsType and then we set newsType and clicks, finally we return the key value pair, the key is fixed string called "top3NewsType" and value is new data structure which we created. The grouping also requires Serde using Grouped.with() method and we set String serde for key and JSON Serde for ClicksByNewsType 
     Now we can easily sort it and take 3 records from the top, however this KGroupedTable is not static database table, it is a real time grouped table where new records and updates will keep coming and we need to keep things sorted as they arrive 

How can we do that?
   Consider we create a custom data structure for this purpose called "SortedNewsTypes". Assume we got first count and we simply add this to the SortedNewsTypes. Now we got second click and we again add it to the SortedNewsTypes. The SortedNewsTypes data structure is smart enough to keep things sorted. So with these 2 records we are expecting the data structure to sort it, Politics at the topi and Sports in the second position, because clicks are same for both records so we sorted them based on string name
    We got 3rd event, and simply add it ur data structure again in sorted format. Similarly we got ur 4th event and we added to our data structure. Now we have 4 items on the list, however we need only top 3. So we make SortedNewsTypes which will remove one record from the lower end and maintains only 3 records from the top
    Now we are expecting the next record again for WorldNews and hence the count is updated to 2. Now we added to SortedNewsTypes and the data structure will sort it, and finally remove the lowest member to keep only 3.
    Now we got another record, and this time it is an update for the politics. Now we added to SortedNewsTypes, but Polictics record is already present in the list and it must be removed before we add a newly updated record for the politics 
    So we need a class that supports adding and removing records and it should automatically  maintain a list of top3 records in sorted order 
     
5. Create Top3SortedNewsTypes class 
       First we need a private member to hold the sorted list of records, so we create TreeSet of ClicksByNewsType and name it as top3Sorted, which will automatically compare 2 elements and keep them sorted. But we do not need numeric comparison we need a custom comparison 
    So we create custom comparator, where we create a new instance of Treeset and initializing it with the comparator lambda. The comparator lambda takes 2 values and we compare these 2 values and return the result. So we compare them by clicks, and if the click count is same then we compare them by newsType string. Now TreeSet will now automatically maintain an ordered set of the ClicksByNewsType object 
    Next we need to create an add() method that takes a new value and adds it to TreeSet, but after adding it to TreeSet, we must check for the size, whether it became more than 3 
    Next we create remove() which remove given item from the set 
    All ur data structures must be serializable, because Kafka api will materialize them in the state store and transmit them over the network, so we make the class serializable. We are using JSON serialization, so we make the class JSON Serializable by creating getter and setter methods for each attribute and provide JSON annotations   
    We will need a Jackson Databind Object mapper, we had one attribute in this class which is TreeSet, we create a getter method, in that we will be using ObjectMapper's writeValueAsString method to convert TreeSet to JSON string and return it 
    Similarly we will be using ObjectMapper's readValue method to read JSON string into an array, then we loop through the array to use add() and populate our treeset 
    Next annotate the class to include only non-nulls and ignore the null values on serializing using @JsonInclude, then we annotate the class with list of properties using @JsonPropertyOrder 

6. Now goto listener class and implement the top3 values 
    We start with KGroupedTable and apply aggregate() with 3 mandatory arguments initilizer, adder and subtractor
    The initializer is creating an empty data structure, added method takes 3 arguments (ie) key, new event value and the previous aggregated value. We need to add the new value to the previoulsy aggregated value and return the new aggregated value. Sorting and maintaining only 3 records is taken care by the data structure. Next is subtractor which removes the given element from the data structure 
    The aggregate() will materialize some intermediate data to a local KTable, so we have to provide materialization details. We use default Serdes and our stream pipeline should infer the serialization mechanism. 
    top3-clicks is the name of the intermediate state store table, KeyValueStore is the type of state store, the other 2 arguments are the key and value serdes 
    Finally we can convert to the stream and print the results 

7. Start confluent server

8. Create topics
>kafka-topics --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic active-inventories

>kafka-topics --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic ad-clicks 

9. Start kafka producer with sample inputs

10. Start another kafka producer with sample inputs

11. Start the application


Unit testing of Kafka
    Testing a Kafka streams application requires you to have a Kafka cluster. If we do not have a kafka cluster, we cannot run ur kafka streams appl and we cannot test it. However the Spring framework offers an Embedded kafka cluster for implementing automated test cases 

1. Create KafkaTest with lombok,spring for apache kafka, kafak binder dependency

2. Define properties in application.yml
        We have 2 kafka channels, first channel is process-in-0 and this channel points to Kafka topic named input-topic. We have another channel process-out-0 and we will binding this channel for writing the output of my streaming application
       We are using a particular naming conventions here for these channels, however we can use any name for these channels. We will see later why we use particular format. All other configurations are same as usual

3. Create binding interface 
      Here we will read a KStream from process-in-0 channel and write out KStream to process-out-0 channel. We will be using String key and String value for both input and output 

4. Create listener service class with 3 annotations and create process() which reads an input KStream and returns an output KStream. We also need to annotate this method with @StreamListener and @SendTo annotations 
      We created a channel name using listener method name which is process(), so we created an input channel for this process method and named it as process-in and suffix the channel name with 0, because this is first input channel for this method. In case if we use 2 input channels for this process(), we can name the second channel as process-in-1 etc. 
      So we can give whatever name to ur channels, however the convention is to use method name for ur channel name, then suffix ur channel name with -in or -out depending on the purpose of the channel, then again suffix the channel name with an -indexnumber 
     Now we are printing the input stream, converting it to upper case and returning it back. So this appl will read an input string from the input channel, convert it to uppercase and write it back to an output channel, very simple appl because we want to focus on testing approach 

5. How do we test ur appl?
         We have 2 types of approach
1. Manual testing
       Here is a list of typical manual testing approach 
   1. Start Kafka cluster
   2. Create input/output topics
>kafka-topics --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic input-topic

>kafka-topics --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic output-topic
   3. Send some input messages to the input topic
>kafka-console-producer --broker-list localhost:9092 --topic input-topic
>test string 1
>test string 2
   4. Start ur appl
   5. Consume messages from the output topic
>kafka-console-consumer --topic output-topic --from-beginning --bootstrap-server localhost:9092
>TEST STRING 1
>TEST STRING 2
   6. Compare the output results with the expected result. If we got the same results as expected (ie) we wanted to see input string converted to uppercase so ur testing is passed otherwise test is failed

2. Automated testing 
       Now we want to automate the testing and we can repeat it without spending manual time for the above project 

1. For testing kafka appl, we need some additional dependency in pom.xml

<dependency>
    <groupId>org.springframework.cloud</groupId>
    <artifactId>spring-cloud-stream</artiffactId>
    <scope>test</scope>
    <classifier>test-binder</classifier>
    <type>test-jar</type>
</dependency>

So our spring starter project might come with the spring-cloud-stream test binder, we dont want this one (ie) test binder is also an alternative for testing spring-cloud-stream applications.
    However we want to test Kafka specific project, so we remove the dependency and we add 3 dependencies for testing kafka streams like junit, spring-kafka-test, kafka-streams-test-utils and spring-boot-starter-test which is already present as part of default project setup 

<dependency>
            <groupId>junit</groupId>
            <artifactId>junit</artifactId>
            <scope>test</scope>
        </dependency>
		<dependency>
			<groupId>org.springframework.kafka</groupId>
			<artifactId>spring-kafka-test</artifactId>
			<scope>test</scope>
		</dependency>
		<dependency>
			<groupId>org.apache.kafka</groupId>
			<artifactId>kafka-streams-test-utils</artifactId>
			<version>${kafka.version}</version>
			<scope>test</scope>
		</dependency>

2. Now we write testing coding in src/test/java, we have one class which is automatically included by Spring Initializer, we can use the same class for implementing ur test cases or we can delete this and recreate a new similar test class 
    The test class should be annotated with @SpringBootTest. However a typical Spring boot appl is expected to start an embedded web server, but we dont want that. Now we want to test Kafka Stream functionality, so we do not need a web server, so we define that using @SpringBootTest parameters which disables the web environment 
     Next we define @Log4j2 annotation to define log entries. We also need to have @RunWith annotation and use SpringRunner class. The test class must be public so we define class to public, next we delete the empty test and start freshly 

- First steps are to start  kafka cluster and create input and output topics
     But for automated testing we cannot start local kafka cluster. So we will be using embedded kafka cluster which will start within the same JVM. Configuring and starting an embedded kafka cluster requires u two lines of code 

@ClassRule
    public static EmbeddedKafkaRule embeddedKafkaRule = new EmbeddedKafkaRule(1, true, 1,
            "input-topic", "output-topic");
    private static EmbeddedKafkaBroker embeddedKafka = embeddedKafkaRule.getEmbeddedKafka();

We are using Junit @ClassRule annotation to create a static EmbeddedKafkaRule, this rule set some initial configuration parameters for the embedded kafka cluster, the first argument tells the number of brokers, we provide 1 since we are setting for single broker because it is sufficient for testing. The second arg tells that we want the broker to follow a controlled shutdown as true, the 3rd argument is the default number of partitions for each topic, 4th argument is a comma separated list of topics, we want to create 2 topics, so we provide list of 2 topics. These rules will be applied when the framework starts an embedded kafka cluster to run our tests.
     The next line is to create a private static variable for the EmbeddedKafkaBroker. With these 2 lines, the SpringBootTest framework will work together with JUnit framework and start an embedded kafka cluster for us
   
- Now we setup consumer to consume and validate the output. 
   So we define private variable to hold a consumer and setup the consumer in @BeforeClass method, so this method will be executed only once. Setting up consumer goes in 3 steps
   1.  Map<String, Object> consumerProps = consumerProps("group", "false", embeddedKafka); 
      The first step is to set up some consumer configurations, so we created a Map of consumer properties and initilizing it using ConsumerProps() which is to create initial set of consumer configrations, this method is provided by KafkaTestUtils which will gives us a bunch of utility methods. This method also takes an embedded kafka cluster to infer some default configuration from the embedded cluster, we are setting consumer group name as group and auto commit as false, all other default configuration values will be inferred from embedded cluster
    consumerProps.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");
      Once we have default consumer configs, we can set some additional configs or override the defaults, so we are settibg auto-offset-reset to the earliest, so the consumer starts reading from the beginning  

2. Next step for setting up ur consumer is to create a ConsumerFactory instance and use it to create a Consumer instance
  DefaultKafkaConsumerFactory<String, String> cf = new DefaultKafkaConsumerFactory<>(consumerProps);
        consumer = cf.createConsumer();

The ConsumerFactory is part of the Spring for Apache Kafka project 

3. Finally we will associate the consumer with the embedded kafka cluster and configure the consumption topic 
 embeddedKafka.consumeFromAnEmbeddedTopic(consumer, "output-topic");

This setUp() method will be triggered at the beginning of the test and you can use this consumer in ur test case 

- Now we have to do cleanup in @AfterClass, so we create tearDown() which is used to close the consumer 

Step 1 and Step 2 is already done, step 3 and step 5 are for producing messages and consuming messages, the last step is to compare the results. So step 3,5 and 6 will go in ur test case 

But how do we start the Kafka systems?
 @Autowired
    StreamsBuilderFactoryBean streamsBuilderFactoryBean;
      We can autowire StreamsBuilderFactoryBean and ur spring framework will start ur application.
    So our Spring boot application will start, and it will try to connect to the kafka cluster which we defined in application.yml. In yml file we defined the broker configuration as my localhost, but we do not want our appl to connect to the localhost when we are running our test case, instead we want it to connect to the embedded kafka cluster and also we dont want to disturb our yml file

- So we copy resources folder and paste inside test folder, so we make changes to yaml file and it only impacts the test case execution, my actual yml file remains unchanged. 
   So we can change this yml file to connect to the embedded kafka cluster using system variable 
  brokers:${spring.embedded.kafka.brokers}
The embedded kafka broker starts on a random port

- Next we create test case with @Test annotation
      We are creating a Kafka producer, associating the producer to the input topic and sending 2 records. Then we are reading the output topic and collecting the output into actualResultSet, we are reading in while loop until we get all the expected results. Finally we are comparing actualresultSet with the expectedResultSet

3. Run test class and check whether test is passed 

Stream Listener to Functional Style 
     Spring cloud streams framework is evolving to support the functional style of coding a stream processing application. Kafka streams binder supports this style as well. Previously our projects are using imperative programming style, however ur future projects might choose to implement solutions using functional style

1. Create starter project, to convert streaming text to upper case using functional style 
2. Configure all properties in yml file
      We followed naming conventions and created 2 channels, one for input and other for output
      This naming convention is used by Spring cloud stream functional API (ie) we can remove these channel definations from yaml file, because the functional API will automatically create these channels, we do not create these channels manually in ur yml file 
      The input and output channels are automatically created by the framework, since we do not define input output channel we also do not define the binding interface. The binding interface is not required for the functional approach because input, output channels and the required binding is automatically created by the frameworks 

3. Create listener service class where we will be using the functional approach 
    The imperative approach requires us to create a service class, whereas functional approach requires us to create a configuration class
    Create a class with @Configuration and @EnableAutoConfiguration class, so this class is a configuration class and the configuration is automatically enabled 
    In Spring project, the configuration class is used to supply spring beans, so we create a process() method with @Bean annotation and Spring will create this bean and place it in the classpath for the required usage
    This method should return a Java Function, the first arg of Function is an input and the second argument is an output, we want to read a KStream from a Kafka topic and also want to write a KStream to a kafka topic, so input and output are KStream. The process() method returns a Function, so the method starts with return stmt and a lambda expr 
    The lambda takes one input argument which is ur input KStream, the return of lambda expr is also a KStream, so we define function signature with 2 arguments. The first arg is input type which defines the type of the lambda input, second arg is output type which defines lambda return type
     The framework will automatically define the input channels and the binding interface for this bean. The input KStream will be mapped to process-in-0, the output KStream is mapped with process-out-0. The framework will automatically will create 2 channels and name them process-in-0 and process-out-0,but framework still dosent know the kafka topic name 

How do we supply topic name? - 2 options
  1. Create topic name using channel name - for this example, we should create input topic and name it as process-in-0 and create output topic as process-out-0, but it is not good idea, so we go for next approach
  2. Define channel destination in yml file (ie) define topic mapping in ur yml file 
     So now we undo the changes made in yml file. So my appl will automatically create the input output channels and binding interface,but those channels will be reading topic names from yml file


Offset Commit
    Consider we have a topic with 4 partitions and as consumer group with 4 consumers consuming the messages from the topic using polling mechanism. 
    So we are having a topic with 4 partitions (ie) if we are consuming the message within the consumer group we should be having 4 consumers because at a time 1 consumer can consume the messages from a particular partition, it is not possible that multiple consumers are consuming message from same partition, so if we are having consumer group with 6 consumers which are more than 4 partitions then extra consumers will be sitting idle
     So Kafka consumer follows the polling mechanism (ie) consumer request kafka server for messages. So kafka server itself will not pull messages to the consumer side 
     So here we are having topic with 4 partition, in partition 0 we have total 9 messages and 0,1,2 etc indicates message offset, partition 1 have 12 message, partition 2 have 6 messages and partition3 have 4 messages. Next we have consumer group with 4 consumers where individual consumers are consuming messages from individual partitions 
     So we have consumer 1 is consuming the message from partition0, it is currently at offset 4 (ie) the message corresponding to offset 4 has consumed by consumer1 and processed. The consumer2 is in offset8 from partition1 (ie) all message from 0 to 8 has been consumed. Consumer3 is at offset2 for partition2 and consumer4 is at offset2 for partition3 
     Suppose due to some reason consumer4 or any consumer goes down. So one of kafka characteristics is that it does not track acknowledgement from consumers (ie) the cluster dont have any idea about how much messages is processed by a consumer within a particular partition or what offset the consumer is currently consuming the messages from a particular partition. However consumer api itself can take the responsibility of saving this kind of info, that upto what offset it has consumed the messages 
     So our consumer4 which is consuming the message from partition3 is went down, so now we have 4 partition with 3 consumers, so obviously one consumer will start consuming messages from two partitions after kafka balancing. Suppose after kafka rebalancing happens consumer3 got the responsibility to consume the message from partition2 as well as partition3
     Now when consumer3 try to consume the messages from partition3, it need to know from where it should start consuming. If it start consuming message from offset0 then reprocessing of same message will happen, because already consumer4 has consumed the messages upto offset2, actually consumer3 should start consuming the messages from partition3 from offset3. But now consumer3 dont have any info because kafka itself dont keep track upto which offset the consumer has consumed 
     So how consumer3 will know from where it should start the consumption to avoid reprocessing of same message and thats where comes the concept of "offset commit"
     Offset commit helps whenever kafka rebalancing happens and one particular consumer get the responsibility of message consumption from a particular partition then that consumer will get the information from where it should start consuming the message (ie) from which offset it should start 
      Suppose we have a partition with 9 messages and offset value start from 0 to 8,we have consumer1 which is consuming the message. In order to keep it track upto where it has consumed the messages, what the consumer does is, it store the offset till which it has consumed the messages in a kafka internal topic called _consumer_offset. So consumer1 will always commit the info upto particular offset which it has consumed the messages. If next time consumer1 is down and new consumer comes it should start consuming the messages just from the next offset. So consumer1 will commit the info in __consumer_offset which is created and managed by kafka, suppose this particular consumer has consumed the messages upto offset4 so it will commit that information in _consumer_offset topic. So in the back end our consumer api, whether we are using java api or python api, that have to take the responsibility of this info commit, kafka will not take the responsibility 
     After some time if consumer1 goes down and new consumer consumer2 comes up, now it need to know from where it should start consuming the messages from this topic (ie) actually we have to start from offset5. So consumer2 first it will go to _consumer_offset and it will come to know that till offset4 the message is already processed so the consumer2 will start consuming the message from offset5, so no reprocessing of message will happen

How to know initial offset?
    Consumer2 is starting the consumption of messages from last committed offset (ie) offset4, so it is starting consuming message from offset5. Now what about the initial situation when there is no committed offset, suppose we created group and within that group we started some consumer, so overall that group is starting the consumption of messages for the first time (ie) what happens initially from where the consumer will know from where it should start consumption 

How to determine initial offset?
    We have a property called auto.offset.reset and that controls the initial message consumption 

1. Create kafka topic
>kafka-topics.bat --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic demo-test --create

2. Create producer appl

public class SimpleProducer {

	public static void main(String[] args) {
		Properties prop=new Properties();
		prop.put("bootstrap.servers","localhost:9092,localhost:9093,localhost:9094");
		prop.put("key.serializer","org.apache.kafka.common.serialization.StringSerializer");
		prop.put("value.serializer","org.apache.kafka.common.serialization.StringSerializer");
		
		Producer<String,String> producer=new KafkaProducer<>(prop);
		String data="";
		ProducerRecord<String,String> record=null;
		for(int i=0;i<=1000;i++) {
		     data="number "+i;
		record=new ProducerRecord<>("demo-test",data);
		producer.send(record);
		System.out.println(data);
		try {
		Thread.sleep(1000);
		}
		catch(Exception e) {
			e.printStackTrace();
		}
		}
		producer.close();
	
	}

}

3. Start producer appl 

4. Create console consumer to check 
>kafka-console-consumer.bat --bootstrap-server localhost:9092 --topic demo-test --from-beginning
    Now we can see the messages which has been consumed from the beginning since we used --from-beginning 

5. Create console consumer to check 
>kafka-console-consumer.bat --bootstrap-server localhost:9092 --topic demo-test 
     Now we can see only the latest messages (ie) before the consumer whatever has published that this particular consumer has ignored because we didnt write from-beginning
     So we can start kafka consumer basically in 2 ways
    We will start consuming the messages, once the consumers spin up, after that whatever messages are published only those it can consume or we can start consumption of messages from beginning and that can we control by auto.offset.reset property 
   1. earliest - reset offset to the earliest offset, consume from the beginnign of the topic partition. If we started for first time when there is no committed offset
   2. latest (default) - reset offset to the latest offset Consume from the end of the topic partition
       Once we get any committed offset in our _consumer_offset topic, then suppose we are stoping the consumer and starting the consumer, whatever the property we are configuring in auto.offset.reset property, it does not matter, it will start consumption of message from the committed offset onwards
     Once a consumer group has offset written then this configuration parameter no longer applies. If the consumers in the consumer group are stopped and then restarted, they would pick up consuming from the last offset 

6. Create consumer application
public class SimpleConsumer {

	public static void main(String[] args) {
		Properties prop=new Properties();
		prop.put("bootstrap.servers","localhost:9092,localhost:9093,localhost:9094");
		prop.put("key.deserializer","org.apache.kafka.common.serialization.StringDeserializer");
		prop.put("value.deserializer","org.apache.kafka.common.serialization.StringDeserializer");
		prop.put("group.id", "group"); //create consumer group
		prop.put("auto.offset.reset", "latest");
		KafkaConsumer<String,String> consumer=new KafkaConsumer<>(prop);
		consumer.subscribe(Arrays.asList("demo-test"));
		
		while(true) {
			ConsumerRecords<String,String> records=consumer.poll(100);
			for(ConsumerRecord<String,String> record:records) {
				System.out.println(record);
			}
		}
		

	}

}

7. create another topic
>kafka-topics.bat --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic hello_world1 --create

8. Change hello_world1 topic in producer and consumer appl

9. Start producer application

10. Start consumer appl with new group "group123", this group has no committed offset because this is first time we are launching any consumer within this particular group 
     So what will happen, how the messages will be consumed when there is no initial offset, then that will be controlled by auto.offset.reset property (ie) latest, so we have latest messages only that means within this particular group, when first time this consumer will start then after that whatever messages will be published in this particular topic those alone will be consumed 
    We can see some number from 26, so all previously published messages will be gone, because there was no committed offset so message consumption begins based on the configuration (ie) latest

10. Now stop the consumer appl, and start it once again
    Now it will start consuming the messages where it stopped because in back end automatically kafka is committing the offset in _consumer_offset 

11. Stop both producer and consumer appl
12. Create another topic
>kafka-topics.bat --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic hello_world2 --create

13. Change topic name,group name and auto.offset.reset property as "earliest"

14. Start producer appl, now it will produce messages for first time in that particular group 

15. Start consumer appl, it will start consuming the messages from the beginning because there is no committed offset currently for this particular group 
    So when we stop and run, in back end it will create committed offset, so the property have no impact and it will consume the message from the left over message 
